{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":113485,"databundleVersionId":13542141,"sourceType":"competition"},{"sourceId":13023540,"sourceType":"datasetVersion","datasetId":8245752},{"sourceId":13050305,"sourceType":"datasetVersion","datasetId":8264037}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-09-13T15:42:13.170697Z","iopub.execute_input":"2025-09-13T15:42:13.171042Z","iopub.status.idle":"2025-09-13T15:42:13.607032Z","shell.execute_reply.started":"2025-09-13T15:42:13.171014Z","shell.execute_reply":"2025-09-13T15:42:13.605849Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/hackathon-qualification/Starter Notebook.ipynb\n/kaggle/input/hackathon-qualification/archive/sample_submission.csv\n/kaggle/input/hackathon-qualification/archive/train.csv\n/kaggle/input/hackathon-qualification/archive/test.csv\n/kaggle/input/cleaned-dsn-data/train_clean.csv\n/kaggle/input/cleaned-dsn-data/car_data_cleaning_log.json\n/kaggle/input/cleaned-dsn-data/test_clean.csv\n/kaggle/input/cleaned-dsn-data/my_custom_cleaning_log.json\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"!pip install lightgbm\n!pip install catboost","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-13T15:42:13.608459Z","iopub.execute_input":"2025-09-13T15:42:13.608910Z","iopub.status.idle":"2025-09-13T15:42:21.505029Z","shell.execute_reply.started":"2025-09-13T15:42:13.608885Z","shell.execute_reply":"2025-09-13T15:42:21.503806Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: lightgbm in /usr/local/lib/python3.11/dist-packages (4.5.0)\nRequirement already satisfied: numpy>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from lightgbm) (1.26.4)\nRequirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from lightgbm) (1.15.3)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17.0->lightgbm) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17.0->lightgbm) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17.0->lightgbm) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17.0->lightgbm) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17.0->lightgbm) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17.0->lightgbm) (2.4.1)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17.0->lightgbm) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17.0->lightgbm) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17.0->lightgbm) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17.0->lightgbm) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17.0->lightgbm) (2024.2.0)\nRequirement already satisfied: catboost in /usr/local/lib/python3.11/dist-packages (1.2.8)\nRequirement already satisfied: graphviz in /usr/local/lib/python3.11/dist-packages (from catboost) (0.21)\nRequirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from catboost) (3.7.2)\nRequirement already satisfied: numpy<3.0,>=1.16.0 in /usr/local/lib/python3.11/dist-packages (from catboost) (1.26.4)\nRequirement already satisfied: pandas>=0.24 in /usr/local/lib/python3.11/dist-packages (from catboost) (2.2.3)\nRequirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from catboost) (1.15.3)\nRequirement already satisfied: plotly in /usr/local/lib/python3.11/dist-packages (from catboost) (5.24.1)\nRequirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from catboost) (1.17.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy<3.0,>=1.16.0->catboost) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy<3.0,>=1.16.0->catboost) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy<3.0,>=1.16.0->catboost) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy<3.0,>=1.16.0->catboost) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy<3.0,>=1.16.0->catboost) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy<3.0,>=1.16.0->catboost) (2.4.1)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.24->catboost) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.24->catboost) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.24->catboost) (2025.2)\nRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (1.3.2)\nRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (4.58.4)\nRequirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (1.4.8)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (25.0)\nRequirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (11.2.1)\nRequirement already satisfied: pyparsing<3.1,>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (3.0.9)\nRequirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from plotly->catboost) (8.5.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<3.0,>=1.16.0->catboost) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<3.0,>=1.16.0->catboost) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy<3.0,>=1.16.0->catboost) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy<3.0,>=1.16.0->catboost) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy<3.0,>=1.16.0->catboost) (2024.2.0)\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# Core Python\nfrom datetime import datetime\n\n# Visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Warnings\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\n\n# Improve display formatting in pandas\npd.set_option('display.float_format', lambda x: '%.2f' % x)  # Avoid scientific notation\npd.set_option('display.max_rows', None)                      # Show all DataFrame rows\npd.set_option('display.max_columns', None)                   # Show all DataFrame columns\npd.set_option('display.width', None)                         # Disable line wrapping\npd.set_option('display.max_colwidth', None)                  # Show full content in cells\n\n\nfrom scipy.optimize import minimize_scalar\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.linear_model import Ridge\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import StandardScaler\nimport lightgbm as lgb\nfrom catboost import CatBoostRegressor","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-13T15:42:21.506429Z","iopub.execute_input":"2025-09-13T15:42:21.506727Z","iopub.status.idle":"2025-09-13T15:42:24.117428Z","shell.execute_reply.started":"2025-09-13T15:42:21.506700Z","shell.execute_reply":"2025-09-13T15:42:24.116435Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"def load_and_describe_data(file_paths):\n    \"\"\"\n    Load and describe multiple CSV datasets.\n\n    Parameters:\n    file_paths (dict): Dictionary where keys are dataset names and values are file paths.\n\n    Returns:\n    dict: Dictionary of loaded DataFrames {name: DataFrame}\n    \"\"\"\n\n    datasets = {}\n\n    for name, path in file_paths.items():\n        try:\n            df = pd.read_csv(path)\n            datasets[name] = df\n\n            print(f\"\\n{'='*70}\")\n            print(f\"📂 Dataset: {name}\")\n            print(f\"File: {path}\")\n            print(f\"Shape: {df.shape[0]} rows × {df.shape[1]} columns\")\n            print(f\"{'='*70}\\n\")\n\n            print(\"🔍 First 5 rows:\")\n            display(df.head())\n\n            print(\"\\nℹ️ Info:\")\n            df.info()  # already prints, no need for display()\n\n            print(\"\\n📊 Missing Values:\")\n            missing = df.isnull().sum()\n            missing = missing[missing > 0].sort_values(ascending=False)\n            if not missing.empty:\n                print(missing)\n            else:\n                print(\"No missing values detected.\")\n\n            print(\"\\n✅ Data Types:\")\n            print(df.dtypes.value_counts())\n            print(\"\\n\" + \"-\"*70 + \"\\n\")\n\n        except Exception as e:\n            print(f\"❌ Error loading {name} ({path}): {e}\")\n\n    return datasets","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-13T15:42:24.119636Z","iopub.execute_input":"2025-09-13T15:42:24.120324Z","iopub.status.idle":"2025-09-13T15:42:24.129185Z","shell.execute_reply.started":"2025-09-13T15:42:24.120295Z","shell.execute_reply":"2025-09-13T15:42:24.128102Z"}},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"## Dataset Overview\nThe project uses three datasets: **Train**, **Test**, and a **Submission Template**.\n\n## Train Dataset\n* **File:** `train_clean.csv`\n* **Shape:** 188,533 rows × 34 columns\n* Contains vehicle information including brand, model, year, mileage, fuel type, engine details, transmission, colors, accident history, title status, and price (target variable).\n* **Missing values:** Most notable in `aspiration` (173,891), `drivetrain` (172,735), `trim` (140,743), `config` (104,907), and `num_speeds` (84,286).\n* **Target variable:** `price`.\n\n## Test Dataset\n* **File:** `test_clean.csv`\n* **Shape:** 125,690 rows × 33 columns\n* Has the same features as the train set except `price`, which needs to be predicted.\n* **Missing values:** Similar trends as the train set, with high gaps in `aspiration` (115,897), `drivetrain` (115,000), and `trim` (94,074).\n\n### Submission Template\n* **File:** `sample_submission.csv`\n* **Shape:** 125,690 rows × 2 columns (`id`, `price`)\n* Serves as the required format for competition submission.\n\n**Summary:**\nThe datasets provide a wide range of structured attributes describing used cars, but many fields (engine configuration, drivetrain, trim, aspiration) are sparsely populated. Handling these missing values effectively will be crucial in building a strong predictive model.","metadata":{}},{"cell_type":"code","source":"file_paths = {\n    \"Train\": \"/kaggle/input/cleaned-dsn-data/train_clean.csv\",\n    \"Test\": \"/kaggle/input/cleaned-dsn-data/test_clean.csv\",\n    \"Submission_Template\": \"/kaggle/input/hackathon-qualification/archive/sample_submission.csv\"\n}\n\ndatasets = load_and_describe_data(file_paths)\n\n# Access a dataset:\ntrain = datasets[\"Train\"]\ntest = datasets[\"Test\"]\nsub = datasets[\"Submission_Template\"]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-13T15:42:24.130188Z","iopub.execute_input":"2025-09-13T15:42:24.130490Z","iopub.status.idle":"2025-09-13T15:42:27.237409Z","shell.execute_reply.started":"2025-09-13T15:42:24.130460Z","shell.execute_reply":"2025-09-13T15:42:27.236475Z"}},"outputs":[{"name":"stdout","text":"\n======================================================================\n📂 Dataset: Train\nFile: /kaggle/input/cleaned-dsn-data/train_clean.csv\nShape: 188533 rows × 34 columns\n======================================================================\n\n🔍 First 5 rows:\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"   id          brand              model  model_year  milage      fuel_type  \\\n0   0           MINI      Cooper S Base        2007  213000       Gasoline   \n1   1        Lincoln              LS V8        2002  143250       Gasoline   \n2   2      Chevrolet  Silverado 2500 LT        2002  136731  E85 Flex Fuel   \n3   3        Genesis   G90 5.0 Ultimate        2017   19500       Gasoline   \n4   4  Mercedes-Benz        Metris Base        2021    7388       Gasoline   \n\n                                                engine  \\\n0         172.0HP 1.6L 4 Cylinder Engine Gasoline Fuel   \n1         252.0HP 3.9L 8 Cylinder Engine Gasoline Fuel   \n2  320.0HP 5.3L 8 Cylinder Engine Flex Fuel Capability   \n3         420.0HP 5.0L 8 Cylinder Engine Gasoline Fuel   \n4         208.0HP 2.0L 4 Cylinder Engine Gasoline Fuel   \n\n                     transmission ext_col int_col  \\\n0                             A/T  Yellow    Gray   \n1                             A/T  Silver   Beige   \n2                             A/T    Blue    Gray   \n3  Transmission w/Dual Shift Mode   Black   Black   \n4                     7-Speed A/T   Black   Beige   \n\n                                 accident clean_title  horsepower  \\\n0                           None reported         Yes      172.00   \n1  At least 1 accident or damage reported         Yes      252.00   \n2                           None reported         Yes      320.00   \n3                           None reported         Yes      420.00   \n4                           None reported         Yes      208.00   \n\n   engine_liters  cylinders config aspiration  extra_descriptors base_model  \\\n0           1.60       4.00    NaN        NaN    Cylinder Engine     Cooper   \n1           3.90       8.00    NaN        NaN    Cylinder Engine      LS V8   \n2           5.30       8.00    NaN        NaN  Engine Capability  Silverado   \n3           5.00       8.00    NaN        NaN    Cylinder Engine        G90   \n4           2.00       4.00    NaN        NaN    Cylinder Engine     Metris   \n\n  trim body_style drivetrain base_color paint_finish  two_tone raw_ext_col  \\\n0  NaN  Hatchback        NaN     Yellow        Solid     False      yellow   \n1   LS      Sedan        NaN     Silver        Solid     False      silver   \n2   LT      Truck        NaN       Blue        Solid     False        blue   \n3  NaN        NaN        NaN      Black        Solid     False       black   \n4  NaN        Van        NaN      Black        Solid     False       black   \n\n  base_int_color material raw_int_col transmission_type  num_speeds  \\\n0           Gray  Unknown        gray         Automatic         NaN   \n1          Beige  Unknown       beige         Automatic         NaN   \n2           Gray  Unknown        gray         Automatic         NaN   \n3          Black  Unknown       black               NaN         NaN   \n4          Beige  Unknown       beige         Automatic        7.00   \n\n   has_auto_shift                   special_notes  price  \n0               0                             A/T   4200  \n1               0                             A/T   4999  \n2               0                             A/T  13900  \n3               0  Transmission w/Dual Shift Mode  45000  \n4               0                     7-Speed A/T  97500  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>brand</th>\n      <th>model</th>\n      <th>model_year</th>\n      <th>milage</th>\n      <th>fuel_type</th>\n      <th>engine</th>\n      <th>transmission</th>\n      <th>ext_col</th>\n      <th>int_col</th>\n      <th>accident</th>\n      <th>clean_title</th>\n      <th>horsepower</th>\n      <th>engine_liters</th>\n      <th>cylinders</th>\n      <th>config</th>\n      <th>aspiration</th>\n      <th>extra_descriptors</th>\n      <th>base_model</th>\n      <th>trim</th>\n      <th>body_style</th>\n      <th>drivetrain</th>\n      <th>base_color</th>\n      <th>paint_finish</th>\n      <th>two_tone</th>\n      <th>raw_ext_col</th>\n      <th>base_int_color</th>\n      <th>material</th>\n      <th>raw_int_col</th>\n      <th>transmission_type</th>\n      <th>num_speeds</th>\n      <th>has_auto_shift</th>\n      <th>special_notes</th>\n      <th>price</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>MINI</td>\n      <td>Cooper S Base</td>\n      <td>2007</td>\n      <td>213000</td>\n      <td>Gasoline</td>\n      <td>172.0HP 1.6L 4 Cylinder Engine Gasoline Fuel</td>\n      <td>A/T</td>\n      <td>Yellow</td>\n      <td>Gray</td>\n      <td>None reported</td>\n      <td>Yes</td>\n      <td>172.00</td>\n      <td>1.60</td>\n      <td>4.00</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Cylinder Engine</td>\n      <td>Cooper</td>\n      <td>NaN</td>\n      <td>Hatchback</td>\n      <td>NaN</td>\n      <td>Yellow</td>\n      <td>Solid</td>\n      <td>False</td>\n      <td>yellow</td>\n      <td>Gray</td>\n      <td>Unknown</td>\n      <td>gray</td>\n      <td>Automatic</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>A/T</td>\n      <td>4200</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>Lincoln</td>\n      <td>LS V8</td>\n      <td>2002</td>\n      <td>143250</td>\n      <td>Gasoline</td>\n      <td>252.0HP 3.9L 8 Cylinder Engine Gasoline Fuel</td>\n      <td>A/T</td>\n      <td>Silver</td>\n      <td>Beige</td>\n      <td>At least 1 accident or damage reported</td>\n      <td>Yes</td>\n      <td>252.00</td>\n      <td>3.90</td>\n      <td>8.00</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Cylinder Engine</td>\n      <td>LS V8</td>\n      <td>LS</td>\n      <td>Sedan</td>\n      <td>NaN</td>\n      <td>Silver</td>\n      <td>Solid</td>\n      <td>False</td>\n      <td>silver</td>\n      <td>Beige</td>\n      <td>Unknown</td>\n      <td>beige</td>\n      <td>Automatic</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>A/T</td>\n      <td>4999</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>Chevrolet</td>\n      <td>Silverado 2500 LT</td>\n      <td>2002</td>\n      <td>136731</td>\n      <td>E85 Flex Fuel</td>\n      <td>320.0HP 5.3L 8 Cylinder Engine Flex Fuel Capability</td>\n      <td>A/T</td>\n      <td>Blue</td>\n      <td>Gray</td>\n      <td>None reported</td>\n      <td>Yes</td>\n      <td>320.00</td>\n      <td>5.30</td>\n      <td>8.00</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Engine Capability</td>\n      <td>Silverado</td>\n      <td>LT</td>\n      <td>Truck</td>\n      <td>NaN</td>\n      <td>Blue</td>\n      <td>Solid</td>\n      <td>False</td>\n      <td>blue</td>\n      <td>Gray</td>\n      <td>Unknown</td>\n      <td>gray</td>\n      <td>Automatic</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>A/T</td>\n      <td>13900</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>Genesis</td>\n      <td>G90 5.0 Ultimate</td>\n      <td>2017</td>\n      <td>19500</td>\n      <td>Gasoline</td>\n      <td>420.0HP 5.0L 8 Cylinder Engine Gasoline Fuel</td>\n      <td>Transmission w/Dual Shift Mode</td>\n      <td>Black</td>\n      <td>Black</td>\n      <td>None reported</td>\n      <td>Yes</td>\n      <td>420.00</td>\n      <td>5.00</td>\n      <td>8.00</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Cylinder Engine</td>\n      <td>G90</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Black</td>\n      <td>Solid</td>\n      <td>False</td>\n      <td>black</td>\n      <td>Black</td>\n      <td>Unknown</td>\n      <td>black</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>Transmission w/Dual Shift Mode</td>\n      <td>45000</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>Mercedes-Benz</td>\n      <td>Metris Base</td>\n      <td>2021</td>\n      <td>7388</td>\n      <td>Gasoline</td>\n      <td>208.0HP 2.0L 4 Cylinder Engine Gasoline Fuel</td>\n      <td>7-Speed A/T</td>\n      <td>Black</td>\n      <td>Beige</td>\n      <td>None reported</td>\n      <td>Yes</td>\n      <td>208.00</td>\n      <td>2.00</td>\n      <td>4.00</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Cylinder Engine</td>\n      <td>Metris</td>\n      <td>NaN</td>\n      <td>Van</td>\n      <td>NaN</td>\n      <td>Black</td>\n      <td>Solid</td>\n      <td>False</td>\n      <td>black</td>\n      <td>Beige</td>\n      <td>Unknown</td>\n      <td>beige</td>\n      <td>Automatic</td>\n      <td>7.00</td>\n      <td>0</td>\n      <td>7-Speed A/T</td>\n      <td>97500</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}},{"name":"stdout","text":"\nℹ️ Info:\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 188533 entries, 0 to 188532\nData columns (total 34 columns):\n #   Column             Non-Null Count   Dtype  \n---  ------             --------------   -----  \n 0   id                 188533 non-null  int64  \n 1   brand              188533 non-null  object \n 2   model              188533 non-null  object \n 3   model_year         188533 non-null  int64  \n 4   milage             188533 non-null  int64  \n 5   fuel_type          183450 non-null  object \n 6   engine             188533 non-null  object \n 7   transmission       188533 non-null  object \n 8   ext_col            188533 non-null  object \n 9   int_col            188533 non-null  object \n 10  accident           186081 non-null  object \n 11  clean_title        167114 non-null  object \n 12  horsepower         155274 non-null  float64\n 13  engine_liters      181835 non-null  float64\n 14  cylinders          173347 non-null  float64\n 15  config             83626 non-null   object \n 16  aspiration         14642 non-null   object \n 17  extra_descriptors  187727 non-null  object \n 18  base_model         188533 non-null  object \n 19  trim               47790 non-null   object \n 20  body_style         145389 non-null  object \n 21  drivetrain         15798 non-null   object \n 22  base_color         188533 non-null  object \n 23  paint_finish       188533 non-null  object \n 24  two_tone           188533 non-null  bool   \n 25  raw_ext_col        188533 non-null  object \n 26  base_int_color     188533 non-null  object \n 27  material           188533 non-null  object \n 28  raw_int_col        188533 non-null  object \n 29  transmission_type  168751 non-null  object \n 30  num_speeds         104247 non-null  float64\n 31  has_auto_shift     188533 non-null  int64  \n 32  special_notes      188533 non-null  object \n 33  price              188533 non-null  int64  \ndtypes: bool(1), float64(4), int64(5), object(24)\nmemory usage: 47.6+ MB\n\n📊 Missing Values:\naspiration           173891\ndrivetrain           172735\ntrim                 140743\nconfig               104907\nnum_speeds            84286\nbody_style            43144\nhorsepower            33259\nclean_title           21419\ntransmission_type     19782\ncylinders             15186\nengine_liters          6698\nfuel_type              5083\naccident               2452\nextra_descriptors       806\ndtype: int64\n\n✅ Data Types:\nobject     24\nint64       5\nfloat64     4\nbool        1\nName: count, dtype: int64\n\n----------------------------------------------------------------------\n\n\n======================================================================\n📂 Dataset: Test\nFile: /kaggle/input/cleaned-dsn-data/test_clean.csv\nShape: 125690 rows × 33 columns\n======================================================================\n\n🔍 First 5 rows:\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"       id brand                 model  model_year  milage fuel_type  \\\n0  188533  Land        Rover LR2 Base        2015   98000  Gasoline   \n1  188534  Land     Rover Defender SE        2020    9142    Hybrid   \n2  188535  Ford    Expedition Limited        2022   28121  Gasoline   \n3  188536  Audi         A6 2.0T Sport        2016   61258  Gasoline   \n4  188537  Audi  A6 2.0T Premium Plus        2018   59000  Gasoline   \n\n                                                                  engine  \\\n0                           240.0HP 2.0L 4 Cylinder Engine Gasoline Fuel   \n1  395.0HP 3.0L Straight 6 Cylinder Engine Gasoline/Mild Electric Hybrid   \n2                                        3.5L V6 24V PDI DOHC Twin Turbo   \n3                                                         2.0 Liter TFSI   \n4                           252.0HP 2.0L 4 Cylinder Engine Gasoline Fuel   \n\n         transmission          ext_col int_col       accident clean_title  \\\n0         6-Speed A/T            White   Beige  None reported         Yes   \n1         8-Speed A/T           Silver   Black  None reported         Yes   \n2  10-Speed Automatic            White   Ebony  None reported         NaN   \n3           Automatic  Silician Yellow   Black  None reported         NaN   \n4                 A/T             Gray   Black  None reported         Yes   \n\n   horsepower  engine_liters  cylinders      config aspiration  \\\n0      240.00           2.00       4.00         NaN        NaN   \n1      395.00           3.00       6.00  Straight 6        NaN   \n2         NaN           3.50       6.00          V6      Turbo   \n3         NaN           2.00        NaN         NaN        NaN   \n4      252.00           2.00       4.00         NaN        NaN   \n\n                        extra_descriptors  base_model trim body_style  \\\n0                         Cylinder Engine       Rover  NaN      Sedan   \n1  Cylinder Engine Gasoline Mild Electric       Rover   SE      Sedan   \n2                       24V PDI DOHC Twin  Expedition  NaN        SUV   \n3                          2.0 Liter TFSI          A6  NaN      Sedan   \n4                         Cylinder Engine          A6  NaN      Sedan   \n\n  drivetrain base_color paint_finish  two_tone      raw_ext_col  \\\n0        NaN      White        Solid     False            white   \n1        NaN     Silver        Solid     False           silver   \n2        NaN      White        Solid     False            white   \n3        NaN     Yellow        Solid     False  silician yellow   \n4        NaN       Gray        Solid     False             gray   \n\n  base_int_color material raw_int_col transmission_type  num_speeds  \\\n0          Beige  Unknown       beige         Automatic        6.00   \n1          Black  Unknown       black         Automatic        8.00   \n2          Black  Unknown       ebony         Automatic       10.00   \n3          Black  Unknown       black         Automatic         NaN   \n4          Black  Unknown       black         Automatic         NaN   \n\n   has_auto_shift       special_notes  \n0               0         6-Speed A/T  \n1               0         8-Speed A/T  \n2               0  10-Speed Automatic  \n3               0           Automatic  \n4               0                 A/T  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>brand</th>\n      <th>model</th>\n      <th>model_year</th>\n      <th>milage</th>\n      <th>fuel_type</th>\n      <th>engine</th>\n      <th>transmission</th>\n      <th>ext_col</th>\n      <th>int_col</th>\n      <th>accident</th>\n      <th>clean_title</th>\n      <th>horsepower</th>\n      <th>engine_liters</th>\n      <th>cylinders</th>\n      <th>config</th>\n      <th>aspiration</th>\n      <th>extra_descriptors</th>\n      <th>base_model</th>\n      <th>trim</th>\n      <th>body_style</th>\n      <th>drivetrain</th>\n      <th>base_color</th>\n      <th>paint_finish</th>\n      <th>two_tone</th>\n      <th>raw_ext_col</th>\n      <th>base_int_color</th>\n      <th>material</th>\n      <th>raw_int_col</th>\n      <th>transmission_type</th>\n      <th>num_speeds</th>\n      <th>has_auto_shift</th>\n      <th>special_notes</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>188533</td>\n      <td>Land</td>\n      <td>Rover LR2 Base</td>\n      <td>2015</td>\n      <td>98000</td>\n      <td>Gasoline</td>\n      <td>240.0HP 2.0L 4 Cylinder Engine Gasoline Fuel</td>\n      <td>6-Speed A/T</td>\n      <td>White</td>\n      <td>Beige</td>\n      <td>None reported</td>\n      <td>Yes</td>\n      <td>240.00</td>\n      <td>2.00</td>\n      <td>4.00</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Cylinder Engine</td>\n      <td>Rover</td>\n      <td>NaN</td>\n      <td>Sedan</td>\n      <td>NaN</td>\n      <td>White</td>\n      <td>Solid</td>\n      <td>False</td>\n      <td>white</td>\n      <td>Beige</td>\n      <td>Unknown</td>\n      <td>beige</td>\n      <td>Automatic</td>\n      <td>6.00</td>\n      <td>0</td>\n      <td>6-Speed A/T</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>188534</td>\n      <td>Land</td>\n      <td>Rover Defender SE</td>\n      <td>2020</td>\n      <td>9142</td>\n      <td>Hybrid</td>\n      <td>395.0HP 3.0L Straight 6 Cylinder Engine Gasoline/Mild Electric Hybrid</td>\n      <td>8-Speed A/T</td>\n      <td>Silver</td>\n      <td>Black</td>\n      <td>None reported</td>\n      <td>Yes</td>\n      <td>395.00</td>\n      <td>3.00</td>\n      <td>6.00</td>\n      <td>Straight 6</td>\n      <td>NaN</td>\n      <td>Cylinder Engine Gasoline Mild Electric</td>\n      <td>Rover</td>\n      <td>SE</td>\n      <td>Sedan</td>\n      <td>NaN</td>\n      <td>Silver</td>\n      <td>Solid</td>\n      <td>False</td>\n      <td>silver</td>\n      <td>Black</td>\n      <td>Unknown</td>\n      <td>black</td>\n      <td>Automatic</td>\n      <td>8.00</td>\n      <td>0</td>\n      <td>8-Speed A/T</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>188535</td>\n      <td>Ford</td>\n      <td>Expedition Limited</td>\n      <td>2022</td>\n      <td>28121</td>\n      <td>Gasoline</td>\n      <td>3.5L V6 24V PDI DOHC Twin Turbo</td>\n      <td>10-Speed Automatic</td>\n      <td>White</td>\n      <td>Ebony</td>\n      <td>None reported</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>3.50</td>\n      <td>6.00</td>\n      <td>V6</td>\n      <td>Turbo</td>\n      <td>24V PDI DOHC Twin</td>\n      <td>Expedition</td>\n      <td>NaN</td>\n      <td>SUV</td>\n      <td>NaN</td>\n      <td>White</td>\n      <td>Solid</td>\n      <td>False</td>\n      <td>white</td>\n      <td>Black</td>\n      <td>Unknown</td>\n      <td>ebony</td>\n      <td>Automatic</td>\n      <td>10.00</td>\n      <td>0</td>\n      <td>10-Speed Automatic</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>188536</td>\n      <td>Audi</td>\n      <td>A6 2.0T Sport</td>\n      <td>2016</td>\n      <td>61258</td>\n      <td>Gasoline</td>\n      <td>2.0 Liter TFSI</td>\n      <td>Automatic</td>\n      <td>Silician Yellow</td>\n      <td>Black</td>\n      <td>None reported</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>2.00</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>2.0 Liter TFSI</td>\n      <td>A6</td>\n      <td>NaN</td>\n      <td>Sedan</td>\n      <td>NaN</td>\n      <td>Yellow</td>\n      <td>Solid</td>\n      <td>False</td>\n      <td>silician yellow</td>\n      <td>Black</td>\n      <td>Unknown</td>\n      <td>black</td>\n      <td>Automatic</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>Automatic</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>188537</td>\n      <td>Audi</td>\n      <td>A6 2.0T Premium Plus</td>\n      <td>2018</td>\n      <td>59000</td>\n      <td>Gasoline</td>\n      <td>252.0HP 2.0L 4 Cylinder Engine Gasoline Fuel</td>\n      <td>A/T</td>\n      <td>Gray</td>\n      <td>Black</td>\n      <td>None reported</td>\n      <td>Yes</td>\n      <td>252.00</td>\n      <td>2.00</td>\n      <td>4.00</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Cylinder Engine</td>\n      <td>A6</td>\n      <td>NaN</td>\n      <td>Sedan</td>\n      <td>NaN</td>\n      <td>Gray</td>\n      <td>Solid</td>\n      <td>False</td>\n      <td>gray</td>\n      <td>Black</td>\n      <td>Unknown</td>\n      <td>black</td>\n      <td>Automatic</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>A/T</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}},{"name":"stdout","text":"\nℹ️ Info:\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 125690 entries, 0 to 125689\nData columns (total 33 columns):\n #   Column             Non-Null Count   Dtype  \n---  ------             --------------   -----  \n 0   id                 125690 non-null  int64  \n 1   brand              125690 non-null  object \n 2   model              125690 non-null  object \n 3   model_year         125690 non-null  int64  \n 4   milage             125690 non-null  int64  \n 5   fuel_type          122307 non-null  object \n 6   engine             125690 non-null  object \n 7   transmission       125690 non-null  object \n 8   ext_col            125690 non-null  object \n 9   int_col            125690 non-null  object \n 10  accident           124058 non-null  object \n 11  clean_title        111451 non-null  object \n 12  horsepower         103509 non-null  float64\n 13  engine_liters      121310 non-null  float64\n 14  cylinders          115635 non-null  float64\n 15  config             55803 non-null   object \n 16  aspiration         9793 non-null    object \n 17  extra_descriptors  125179 non-null  object \n 18  base_model         125690 non-null  object \n 19  trim               31616 non-null   object \n 20  body_style         96827 non-null   object \n 21  drivetrain         10690 non-null   object \n 22  base_color         125690 non-null  object \n 23  paint_finish       125690 non-null  object \n 24  two_tone           125690 non-null  bool   \n 25  raw_ext_col        125690 non-null  object \n 26  base_int_color     125690 non-null  object \n 27  material           125690 non-null  object \n 28  raw_int_col        125690 non-null  object \n 29  transmission_type  112494 non-null  object \n 30  num_speeds         69625 non-null   float64\n 31  has_auto_shift     125690 non-null  int64  \n 32  special_notes      125690 non-null  object \ndtypes: bool(1), float64(4), int64(4), object(24)\nmemory usage: 30.8+ MB\n\n📊 Missing Values:\naspiration           115897\ndrivetrain           115000\ntrim                  94074\nconfig                69887\nnum_speeds            56065\nbody_style            28863\nhorsepower            22181\nclean_title           14239\ntransmission_type     13196\ncylinders             10055\nengine_liters          4380\nfuel_type              3383\naccident               1632\nextra_descriptors       511\ndtype: int64\n\n✅ Data Types:\nobject     24\nint64       4\nfloat64     4\nbool        1\nName: count, dtype: int64\n\n----------------------------------------------------------------------\n\n\n======================================================================\n📂 Dataset: Submission_Template\nFile: /kaggle/input/hackathon-qualification/archive/sample_submission.csv\nShape: 125690 rows × 2 columns\n======================================================================\n\n🔍 First 5 rows:\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"       id    price\n0  188533 43878.02\n1  188534 43878.02\n2  188535 43878.02\n3  188536 43878.02\n4  188537 43878.02","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>price</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>188533</td>\n      <td>43878.02</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>188534</td>\n      <td>43878.02</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>188535</td>\n      <td>43878.02</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>188536</td>\n      <td>43878.02</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>188537</td>\n      <td>43878.02</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}},{"name":"stdout","text":"\nℹ️ Info:\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 125690 entries, 0 to 125689\nData columns (total 2 columns):\n #   Column  Non-Null Count   Dtype  \n---  ------  --------------   -----  \n 0   id      125690 non-null  int64  \n 1   price   125690 non-null  float64\ndtypes: float64(1), int64(1)\nmemory usage: 1.9 MB\n\n📊 Missing Values:\nNo missing values detected.\n\n✅ Data Types:\nint64      1\nfloat64    1\nName: count, dtype: int64\n\n----------------------------------------------------------------------\n\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"## Target Variable: Price\nThe target variable for this project is **`price`**, representing the resale value of each vehicle. Understanding its distribution is key for building a predictive model.\n\n### Descriptive Statistics\n* **Count:** 188,533 entries, confirming no missing values in the target.\n* **Mean:** ≈ 43,878, but the **standard deviation** (≈ 78,819) is almost twice the mean, indicating very high variability in car prices.\n* **Range:** Values span from **\\$2,000** to **\\$2,954,083**, a huge gap that reflects both everyday used cars and luxury/exotic vehicles.\n* **Quartiles:**\n\n  * 25% of cars are priced below **\\$17,000**.\n  * The median is **\\$30,825**, which is much lower than the mean, highlighting the effect of extreme high prices pulling the average up.\n  * 75% are below **\\$49,900**, meaning the majority of vehicles fall into a more “affordable” range, while only a small fraction sit at very high values.\n\n### Distribution & Skewness\nVisual exploration confirmed a **heavily right-skewed distribution**. Most vehicles are clustered in the lower price ranges, with a long tail stretching out to rare, high-value outliers.\n\n* **Boxplots** showed significant outliers at the top end.\n* Applying a **log transformation** to `price` produced a distribution that was far closer to normal, reducing skew and compressing extreme values.\n\n### Why Log Transformation?\nUsing the **log of `price`** as the training target improves model stability:\n* It reduces the influence of extreme outliers that would otherwise dominate the loss function.\n* It brings the distribution closer to Gaussian, which aligns better with many model assumptions.\n* It makes the model more sensitive to relative differences (e.g., the difference between \\$10k and \\$20k is more meaningful than between \\$200k and \\$210k).\n\nWhen reverting predictions back to the original scale, **`np.expm1()`** is used.\n\n* `np.expm1(x)` computes `exp(x) - 1`, the inverse of `log1p(y)` (i.e., `log(1 + y)`).\n* This is often chosen over a plain `exp()` because `log1p` is numerically stable for small values, avoiding floating-point precision issues.\n* In practice, this ensures more accurate recovery of the original price values, especially for vehicles at the lower end of the distribution.\n\n### Implication for Modeling\nBy training on `log(price)` and converting predictions back, the model:\n* Handles the wide range of car values more effectively.\n* Reduces bias introduced by extreme luxury outliers.\n* Provides predictions that better reflect typical car pricing patterns while still accommodating rare high-value vehicles.\n\n**In short:** The descriptive stats confirm that car prices are highly variable and skewed. Transforming the target was a necessary step to build a model that learns robust patterns without being distorted by extreme values.","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(16, 5))\n\nplt.subplot(1, 3, 1)\nsns.histplot(train['price'], bins=50, kde=False)\nplt.title('Price Distribution (Raw)')\nplt.xlabel('Price')\n\nplt.subplot(1, 3, 2)\nsns.histplot(np.log1p(train['price']), bins=50, kde=False)\nplt.title('Log(Price) Distribution')\nplt.xlabel('Log(Price)')\n\nplt.subplot(1, 3, 3)\nsns.boxplot(x=train['price'])\nplt.title('Price Boxplot (Raw)')\n\nplt.tight_layout()\nplt.show()\n\nprint(\"Price Stats:\")\nprint(train['price'].describe())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-13T15:42:27.238430Z","iopub.execute_input":"2025-09-13T15:42:27.238731Z","iopub.status.idle":"2025-09-13T15:42:28.170668Z","shell.execute_reply.started":"2025-09-13T15:42:27.238709Z","shell.execute_reply":"2025-09-13T15:42:28.169900Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 1600x500 with 3 Axes>","image/png":"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\n"},"metadata":{}},{"name":"stdout","text":"Price Stats:\ncount    188533.00\nmean      43878.02\nstd       78819.52\nmin        2000.00\n25%       17000.00\n50%       30825.00\n75%       49900.00\nmax     2954083.00\nName: price, dtype: float64\n","output_type":"stream"}],"execution_count":6},{"cell_type":"markdown","source":"## Investigating the Impact of Missing Values on Price\nBefore deciding how to handle missing values, I ran a quick test to see whether missingness itself carried information about car price. The logic was simple: if cars with missing values in certain features consistently show higher or lower prices compared to those with complete data, then the missingness might not be random — and could actually be a useful signal for the model.\n\n### Interpretation\n* Across all three features (`horsepower`, `cylinders`, `engine_liters`), the **average price was significantly higher when values were missing**.\n* For example, cars with missing horsepower values averaged around **\\$64k**, compared to **\\$39k** for those with recorded horsepower — a gap of nearly **\\$25k**.\n* Similar patterns held for cylinders and engine liters, though with slightly smaller differences.\n\nThis suggests that missing values are **not random**. They may correspond to higher-end vehicles where certain specs are either not disclosed in listings or structured differently (e.g., premium trims, hybrids, or electric cars that don’t fit traditional “cylinder” or “engine liter” categories).\n\n### Implications for Modeling\n* Simply filling in missing values with averages or medians could erase this signal.\n* A better approach is to create **missingness indicator features** (e.g., `is_horsepower_missing = 1/0`) so the model can learn whether the absence of a value itself correlates with higher prices.\n* This allows imputation to handle the numeric side, while the missing flag preserves potentially important information.\n\n**In short:** missingness itself carries predictive power in this dataset, and rather than treating it as noise, it is to be engineered into features for the model.","metadata":{}},{"cell_type":"code","source":"# Compare average price when missing vs not missing\nprint(\"== Average Price When Feature is MISSING vs PRESENT ==\")\nfor col in ['horsepower', 'cylinders', 'engine_liters']:\n    missing_flag = train[col].isnull().astype(int)\n    missing_mean = train.loc[missing_flag == 1, 'price'].mean()\n    present_mean = train.loc[missing_flag == 0, 'price'].mean()\n    print(f\"{col}: Missing=${missing_mean:,.0f} | Present=${present_mean:,.0f} | Diff=${present_mean - missing_mean:,.0f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-13T15:42:28.171672Z","iopub.execute_input":"2025-09-13T15:42:28.172015Z","iopub.status.idle":"2025-09-13T15:42:28.194889Z","shell.execute_reply.started":"2025-09-13T15:42:28.171986Z","shell.execute_reply":"2025-09-13T15:42:28.193782Z"}},"outputs":[{"name":"stdout","text":"== Average Price When Feature is MISSING vs PRESENT ==\nhorsepower: Missing=$64,243 | Present=$39,516 | Diff=$-24,727\ncylinders: Missing=$62,225 | Present=$42,271 | Diff=$-19,955\nengine_liters: Missing=$57,507 | Present=$43,376 | Diff=$-14,131\n","output_type":"stream"}],"execution_count":7},{"cell_type":"markdown","source":"## Exploring Model and Brand Cardinality\nTo better understand the structure of the dataset, I examined two categorical dimensions that are likely to strongly influence car prices: **base\\_model** and **brand**. The goal was to check their cardinality (how many unique values exist) and explore which entries dominate by frequency or price.\n\n### What was discovered\n* **Base Models:**\n  * There are **542 unique base models**, showing high cardinality.\n  * The most common models in the dataset are mainstream, high-volume vehicles such as the **Range Rover (7.6k)**, **Ford F-150 (6.1k)**, and **Porsche 911 (5.3k)**.\n  * This long tail distribution implies that many models have relatively few observations, which could make them harder for the model to learn from directly. Encoding strategies will need to account for this imbalance.\n\n* **Brands:**\n  * There are **57 unique brands**, which is more manageable.\n  * The ranking by **median price** shows a clear hierarchy: ultra-luxury brands (**Bugatti, Lamborghini, McLaren**) dominate the top, with median prices far above the dataset average.\n  * Interestingly, newer electric and luxury startups like **Rivian** and **Lucid** also appear in the top 10, reflecting their higher-end positioning in the market.\n\n### Why This Matters for Modeling\n\n* High cardinality in `base_model` suggests the need for techniques like **frequency encoding, target encoding, or grouping rare categories** so that the model can generalize without overfitting.\n* For `brand`, the smaller set of categories means it can likely be encoded directly, but the clear stratification by price highlights its strong predictive power.\n* Together, these features provide both **volume-driven signals** (e.g., popular mainstream models) and **luxury-driven signals** (e.g., brand prestige), which are crucial for accurate price prediction.","metadata":{}},{"cell_type":"code","source":"print(\"== Base Model Cardinality ==\")\nprint(f\"Unique base_model: {train['base_model'].nunique()}\")\nprint(f\"Top 10 base_model by count:\")\nprint(train['base_model'].value_counts().head(10))\n\nprint(\"\\n== Brand Cardinality ==\")\nprint(f\"Unique brand: {train['brand'].nunique()}\")\nprint(f\"Top 10 brand by median price:\")\nprint(train.groupby('brand')['price'].median().sort_values(ascending=False).head(10))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-13T15:42:28.195845Z","iopub.execute_input":"2025-09-13T15:42:28.196137Z","iopub.status.idle":"2025-09-13T15:42:28.263095Z","shell.execute_reply.started":"2025-09-13T15:42:28.196103Z","shell.execute_reply":"2025-09-13T15:42:28.261933Z"}},"outputs":[{"name":"stdout","text":"== Base Model Cardinality ==\nUnique base_model: 542\nTop 10 base_model by count:\nbase_model\nRange Rover    7647\nF-150          6113\n911            5303\nAMG            5167\nCorvette       4027\nMustang        3738\nCamaro         3551\nE-Class        3288\n1500           3178\nWrangler       2903\nName: count, dtype: int64\n\n== Brand Cardinality ==\nUnique brand: 57\nTop 10 brand by median price:\nbrand\nBugatti       220000.00\nLamborghini   102000.00\nMcLaren        87500.00\nRivian         78000.00\nRolls-Royce    77500.00\nFerrari        75500.00\nBentley        73600.00\nAston          67650.00\nLucid          56000.00\nMaybach        54999.00\nName: price, dtype: float64\n","output_type":"stream"}],"execution_count":8},{"cell_type":"markdown","source":"## Feature Engineering and Data Preparation\n\n### 1. Creating Engineered Features\nTo make the dataset more useful for modeling, I created a set of engineered features. These included binary flags such as whether a car had an accident report, whether it carried a clean title, and whether its fuel type was missing. I also added missingness indicators for numeric features like horsepower, cylinders, and engine liters to preserve any signal in the null values.\nUsage-based features such as car age, mileage per year, and log-transformed mileage were introduced to capture wear-and-tear patterns. A special flag was also added to identify whether a car belonged to the luxury brand category. These features gave the model richer signals beyond the raw dataset.","metadata":{}},{"cell_type":"code","source":"def create_engineered_features(train, test, luxury_brands=None):\n    \"\"\"\n    Create engineered features from raw columns.\n    Safe for train and test.\n    Must be run BEFORE imputation.\n    \"\"\"\n    if luxury_brands is None:\n        luxury_brands = [\n            'Bugatti', 'Lamborghini', 'McLaren', 'Rivian', \n            'Rolls-Royce', 'Ferrari', 'Bentley', 'Aston', \n            'Lucid', 'Maybach'\n        ]\n    \n    # =============================\n    # STEP 1: BINARY FLAGS\n    # =============================\n    # Accident\n    train['has_accident_reported'] = (\n        train['accident'] == 'At least 1 accident or damage reported'\n    ).astype(int)\n    test['has_accident_reported'] = (\n        test['accident'] == 'At least 1 accident or damage reported'\n    ).astype(int)\n    \n    # Clean title\n    train['is_clean_title'] = (train['clean_title'] == 'Yes').astype(int)\n    test['is_clean_title'] = (test['clean_title'] == 'Yes').astype(int)\n    \n    # Fuel unknown (based on missingness)\n    train['is_fuel_unknown'] = train['fuel_type'].isnull().astype(int)\n    test['is_fuel_unknown'] = test['fuel_type'].isnull().astype(int)\n    \n    # =============================\n    # STEP 2: MISSINGNESS FLAGS (for numeric features)\n    # =============================\n    for col in ['horsepower', 'cylinders', 'engine_liters']:\n        train[f'is_{col}_missing'] = train[col].isnull().astype(int)\n        test[f'is_{col}_missing'] = test[col].isnull().astype(int)\n    \n    # =============================\n    # STEP 3: AGE & USAGE FEATURES\n    # =============================\n    current_year = 2025\n    \n    train['car_age'] = current_year - train['model_year']\n    test['car_age'] = current_year - test['model_year']\n    \n    train['mileage_per_year'] = train['milage'] / (train['car_age'] + 1)\n    test['mileage_per_year'] = test['milage'] / (test['car_age'] + 1)\n    \n    train['log_mileage'] = np.log1p(train['milage'])\n    test['log_mileage'] = np.log1p(test['milage'])\n    \n    # Create log_price in train (for target encoding later)\n    if 'price' in train.columns:\n        train['log_price'] = np.log1p(train['price'])\n    \n    # =============================\n    # STEP 5: LUXURY BRAND FLAG\n    # =============================\n    train['is_luxury_brand'] = train['brand'].isin(luxury_brands).astype(int)\n    test['is_luxury_brand'] = test['brand'].isin(luxury_brands).astype(int)\n    \n    return train, test","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-13T15:42:28.264362Z","iopub.execute_input":"2025-09-13T15:42:28.264947Z","iopub.status.idle":"2025-09-13T15:42:28.278147Z","shell.execute_reply.started":"2025-09-13T15:42:28.264923Z","shell.execute_reply":"2025-09-13T15:42:28.277242Z"}},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":"### 2. Dropping Sparse Columns\nSome columns had more than 70% missing values, making them uninformative for modeling. These were dropped to reduce noise and simplify the dataset. In this case, three ultra-sparse columns—**aspiration, trim, and drivetrain**—were removed. This step ensured that the model wasn’t distracted by poorly populated fields.","metadata":{}},{"cell_type":"code","source":"def drop_sparse_columns(train, test, threshold=0.7):\n    \"\"\"\n    Drop columns where more than threshold% of values are missing.\n    Default: drop if >70% missing.\n    \"\"\"\n    dropped_cols = []\n    \n    for col in train.columns:\n        if train[col].isnull().mean() > threshold:\n            dropped_cols.append(col)\n    \n    train = train.drop(columns=dropped_cols)\n    test = test.drop(columns=dropped_cols)\n    \n    print(f\"🗑️ Dropped {len(dropped_cols)} sparse columns: {dropped_cols}\")\n    return train, test","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-13T15:42:28.281894Z","iopub.execute_input":"2025-09-13T15:42:28.282284Z","iopub.status.idle":"2025-09-13T15:42:28.305424Z","shell.execute_reply.started":"2025-09-13T15:42:28.282256Z","shell.execute_reply":"2025-09-13T15:42:28.303931Z"}},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":"### 3. Imputing Missing Values\nNext, I handled missing values systematically. For categorical features like `clean_title`, `accident`, and `fuel_type`, missing entries were filled with “Unknown.” For numeric fields, I used hierarchical strategies:\n\n* **Horsepower** was imputed using brand and model medians.\n* **Cylinders** were filled based on engine size bins, then brand, and finally overall median.\n* **Engine liters** followed a similar layered median approach.\n* **Number of speeds** was inferred from transmission type, with a fallback default.\n\nA before-and-after report confirmed that all key columns had their missing values reduced to zero in both train and test sets.","metadata":{}},{"cell_type":"code","source":"def impute_missing_values_with_comparison(train, test):\n    \"\"\"\n    Impute missing values with NO FutureWarnings.\n    Returns detailed before/after stats.\n    \"\"\"\n    impute_stats = {}\n    \n    # Track initial missing counts\n    initial_missing = {\n        'train': train.isnull().sum(),\n        'test': test.isnull().sum()\n    }\n    \n    # 1. Categoricals → \"Unknown\" + normalization\n    for df in [train, test]:\n        df.loc[:, 'clean_title'] = df['clean_title'].fillna('Unknown')\n        df.loc[:, 'accident'] = df['accident'].fillna('Unknown')\n        df.loc[:, 'fuel_type'] = (\n            df['fuel_type']\n            .fillna('Unknown')\n            .replace({\n                '–': 'Unknown',\n                'not supported': 'Unknown',\n                '': 'Unknown',   # empty strings\n                None: 'Unknown'\n            })\n        )\n    \n    # 2. Horsepower\n    hp_med_map = train.groupby(['brand', 'base_model'])['horsepower'].transform('median')\n    train.loc[:, 'horsepower'] = train['horsepower'].fillna(hp_med_map)\n    hp_med_brand = train.groupby('brand')['horsepower'].transform('median')\n    train.loc[:, 'horsepower'] = train['horsepower'].fillna(hp_med_brand)\n    train.loc[:, 'horsepower'] = train['horsepower'].fillna(train['horsepower'].median())\n    \n    test.loc[:, 'horsepower'] = test.groupby(['brand', 'base_model'])['horsepower'].transform('median')\n    test.loc[:, 'horsepower'] = test.groupby('brand')['horsepower'].transform('median')\n    test.loc[:, 'horsepower'] = test['horsepower'].fillna(train['horsepower'].median())  # ← Use TRAIN median!\n    \n    # 3. Cylinders\n    train.loc[:, 'engine_liters_bin'] = pd.cut(train['engine_liters'], bins=10, labels=False)\n    cyl_mode_map = train.groupby(['brand', 'engine_liters_bin'])['cylinders'].transform(\n        lambda x: x.mode().iloc[0] if not x.mode().empty else np.nan\n    )\n    train.loc[:, 'cylinders'] = train['cylinders'].fillna(cyl_mode_map)\n    train.loc[:, 'cylinders'] = train['cylinders'].fillna(train.groupby('brand')['cylinders'].transform('median'))\n    train.loc[:, 'cylinders'] = train['cylinders'].fillna(train['cylinders'].median())\n    \n    test.loc[:, 'engine_liters_bin'] = pd.cut(test['engine_liters'], bins=10, labels=False)\n    test.loc[:, 'cylinders'] = test.groupby(['brand', 'engine_liters_bin'])['cylinders'].transform(\n        lambda x: x.mode().iloc[0] if not x.mode().empty else np.nan\n    )\n    test.loc[:, 'cylinders'] = test['cylinders'].fillna(test.groupby('brand')['cylinders'].transform('median'))\n    test.loc[:, 'cylinders'] = test['cylinders'].fillna(train['cylinders'].median())\n    \n    # 4. Engine Liters\n    train.loc[:, 'engine_liters'] = train.groupby(['brand', 'cylinders'])['engine_liters'].transform('median')\n    train.loc[:, 'engine_liters'] = train['engine_liters'].fillna(train.groupby('brand')['engine_liters'].transform('median'))\n    train.loc[:, 'engine_liters'] = train['engine_liters'].fillna(train['engine_liters'].median())\n    \n    test.loc[:, 'engine_liters'] = test.groupby(['brand', 'cylinders'])['engine_liters'].transform('median')\n    test.loc[:, 'engine_liters'] = test['engine_liters'].fillna(test.groupby('brand')['engine_liters'].transform('median'))\n    test.loc[:, 'engine_liters'] = test['engine_liters'].fillna(train['engine_liters'].median())\n    \n    # 5. Num Speeds\n    train.loc[:, 'num_speeds'] = train.groupby('transmission_type')['num_speeds'].transform('median')\n    train.loc[:, 'num_speeds'] = train['num_speeds'].fillna(8)\n    \n    test.loc[:, 'num_speeds'] = test.groupby('transmission_type')['num_speeds'].transform('median')\n    test.loc[:, 'num_speeds'] = test['num_speeds'].fillna(8)\n    \n    # Final missing counts\n    final_missing = {\n        'train': train.isnull().sum(),\n        'test': test.isnull().sum()\n    }\n    \n    # Generate comparison report\n    print(\"📊 === MISSING VALUE IMPUTATION REPORT ===\")\n    print(f\"{'Column':<20} {'Train Before → After':<25} {'Test Before → After':<25}\")\n    print(\"-\" * 75)\n    \n    key_cols = ['horsepower', 'cylinders', 'engine_liters', 'num_speeds', 'clean_title', 'accident', 'fuel_type']\n    for col in key_cols:\n        train_before = initial_missing['train'][col]\n        train_after = final_missing['train'][col]\n        test_before = initial_missing['test'][col]\n        test_after = final_missing['test'][col]\n        print(f\"{col:<20} {train_before:>6} → {train_after:<6} {'':<5} {test_before:>6} → {test_after:<6}\")\n    \n    # Return stats\n    impute_stats = {\n        'initial_missing': initial_missing,\n        'final_missing': final_missing\n    }\n    \n    return train, test, impute_stats","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-13T15:42:28.306802Z","iopub.execute_input":"2025-09-13T15:42:28.307164Z","iopub.status.idle":"2025-09-13T15:42:28.331931Z","shell.execute_reply.started":"2025-09-13T15:42:28.307137Z","shell.execute_reply":"2025-09-13T15:42:28.330808Z"}},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":"### 4. Target Encoding\nTo help the model capture relationships between categorical variables and price, I applied target encoding. Using the training data only (to prevent leakage), I encoded `base_model` and `brand` with the mean of the log-transformed price. This encoding replaced high-cardinality categories with smoother, numerical representations that carried useful pricing information.","metadata":{}},{"cell_type":"code","source":"train['log_price'] = np.log1p(train['price'])  # for target encoding & modeling\n\ndef create_target_encodings(train, test, cat_cols, target_col):\n    \"\"\"\n    Create target encodings for categorical columns using TRAIN only.\n    Applies to test using train's mappings — NO LEAKAGE.\n    \"\"\"\n    for col in cat_cols:\n        # Create mapping from train\n        mapping = train.groupby(col)[target_col].mean()\n        global_mean = train[target_col].mean()\n        \n        # Apply to train\n        train[f'{col}_target_enc'] = train[col].map(mapping).fillna(global_mean)\n        \n        # Apply to test (using train's mapping!)\n        test[f'{col}_target_enc'] = test[col].map(mapping).fillna(global_mean)\n    \n    return train, test","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-13T15:42:28.333350Z","iopub.execute_input":"2025-09-13T15:42:28.333678Z","iopub.status.idle":"2025-09-13T15:42:28.357399Z","shell.execute_reply.started":"2025-09-13T15:42:28.333643Z","shell.execute_reply":"2025-09-13T15:42:28.356194Z"}},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":"### 5. Final Feature Set\nAfter all transformations, the dataset was considerably richer and cleaner. The training set ended up with **45 features**, while the test set had **43**, with most of the engineered features shared between them. At this point, the data was ready for modeling with a strong foundation of well-structured inputs.","metadata":{}},{"cell_type":"code","source":"# Define luxury brands\nLUXURY_BRANDS = [\n    'Bugatti', 'Lamborghini', 'McLaren', 'Rivian', \n    'Rolls-Royce', 'Ferrari', 'Bentley', 'Aston', \n    'Lucid', 'Maybach'\n]\n\n# Step 1: Feature Engineering\nprint(\"🔧 Step 1: Creating engineered features...\")\ntrain, test = create_engineered_features(train, test, luxury_brands=LUXURY_BRANDS)\n\n# Step 2: Drop sparse columns (trim, body_style, etc.)\nprint(\"\\n🗑️ Step 2: Dropping ultra-sparse columns...\")\ntrain, test = drop_sparse_columns(train, test, threshold=0.7)\n\n# Step 3: Imputation with full report\nprint(\"\\n🧪 Step 3: Imputing missing values...\")\ntrain, test, impute_stats = impute_missing_values_with_comparison(train, test)\n\n# Step 4: Target Encoding\nprint(\"\\n🎯 Step 4: Applying target encoding...\")\ntrain, test = create_target_encodings(\n    train, \n    test, \n    cat_cols=['base_model', 'brand'], \n    target_col='log_price'\n)\n\n# Step 5: Show final feature count\nprint(f\"\\n✅ FINAL FEATURE COUNT:\")\nprint(f\"Train columns: {len(train.columns)}\")\nprint(f\"Test columns: {len(test.columns)}\")\nprint(f\"New features added: {len(train.columns) - len(train.columns.difference(test.columns))} (shared)\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-13T15:42:28.358521Z","iopub.execute_input":"2025-09-13T15:42:28.358839Z","iopub.status.idle":"2025-09-13T15:42:30.290862Z","shell.execute_reply.started":"2025-09-13T15:42:28.358804Z","shell.execute_reply":"2025-09-13T15:42:30.289986Z"}},"outputs":[{"name":"stdout","text":"🔧 Step 1: Creating engineered features...\n\n🗑️ Step 2: Dropping ultra-sparse columns...\n🗑️ Dropped 3 sparse columns: ['aspiration', 'trim', 'drivetrain']\n\n🧪 Step 3: Imputing missing values...\n📊 === MISSING VALUE IMPUTATION REPORT ===\nColumn               Train Before → After      Test Before → After      \n---------------------------------------------------------------------------\nhorsepower            33259 → 0             22181 → 0     \ncylinders             15186 → 0             10055 → 0     \nengine_liters          6698 → 0              4380 → 0     \nnum_speeds            84286 → 0             56065 → 0     \nclean_title           21419 → 0             14239 → 0     \naccident               2452 → 0              1632 → 0     \nfuel_type              5083 → 0              3383 → 0     \n\n🎯 Step 4: Applying target encoding...\n\n✅ FINAL FEATURE COUNT:\nTrain columns: 45\nTest columns: 43\nNew features added: 43 (shared)\n","output_type":"stream"}],"execution_count":13},{"cell_type":"markdown","source":"### 6. Leakage Checks\nBefore moving further, I added some guardrails to confirm that no information from the training set was leaking into the test set. Two key checks were performed:\n\n1. **Price column check** – I confirmed that the test data had no `price` column, since the target should never appear in test. This ensured that the model wouldn’t accidentally “cheat” by seeing the answers.\n\n2. **Target encoding validation** – To verify that encodings in the test set were derived strictly from the training set, I compared the target-encoded value for a sample brand across both datasets. The values matched perfectly, which confirmed there was no leakage in how encodings were applied.\n\nThese checks gave me confidence that the preprocessing pipeline was clean and safe for modeling.","metadata":{}},{"cell_type":"code","source":"# Check that test has no price column\nassert 'price' not in test.columns, \"Test data has price column — LEAKAGE!\"\n\n# Check that target encodings in test are based on train\nsample_brand = train['brand'].iloc[0]\ntrain_enc = train[train['brand'] == sample_brand]['brand_target_enc'].iloc[0]\ntest_enc = test[test['brand'] == sample_brand]['brand_target_enc'].iloc[0] if (test['brand'] == sample_brand).any() else None\n\nif test_enc is not None:\n    assert abs(train_enc - test_enc) < 1e-6, \"Target encoding mismatch — possible leakage!\"\n    print(f\"✅ Target encoding for brand '{sample_brand}' matches between train and test.\")\nelse:\n    print(f\"⚠️ Brand '{sample_brand}' not found in test — but no leakage detected.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-13T15:42:30.690239Z","iopub.execute_input":"2025-09-13T15:42:30.690504Z","iopub.status.idle":"2025-09-13T15:42:30.741984Z","shell.execute_reply.started":"2025-09-13T15:42:30.690484Z","shell.execute_reply":"2025-09-13T15:42:30.741057Z"}},"outputs":[{"name":"stdout","text":"✅ Target encoding for brand 'MINI' matches between train and test.\n","output_type":"stream"}],"execution_count":17},{"cell_type":"markdown","source":"### 7. Post-Imputation Missing Value Review\nAfter completing the full preprocessing pipeline (feature engineering, dropping sparse columns, imputations, and encodings), I ran a fresh check for missing values. The results showed that while many critical fields (e.g., `horsepower`, `cylinders`, `engine_liters`, `fuel_type`, `clean_title`) had been fully resolved, a few columns still contained missing data:\n\n* **config** (over 100k missing)\n* **extra\\_descriptors** (806 missing)\n* **body\\_style** (43k missing)\n* **transmission\\_type** (19k missing)\n* **engine\\_liters\\_bin** (6.6k missing — created during imputation)\n\nMost of these fields, like `config` and `extra_descriptors`, appeared to add little meaningful information for prediction and were strong candidates for being dropped entirely. However, I paid closer attention to **`body_style`** and **`transmission_type`**, since both could reasonably influence used car pricing.\n\n* *Body style* (e.g., sedan, SUV, coupe) captures market positioning and consumer demand, which often ties directly to resale price.\n* *Transmission type* (automatic, manual, CVT, etc.) may impact desirability and price depending on the region and buyer preference.\n\nBecause of this, the plan was to drop columns with high missingness and low predictive value, while carefully considering whether to impute or otherwise handle `body_style` and `transmission_type` rather than discarding them outright.\n\nThis step ensured that the dataset was both clean and still preserved features most relevant for price modeling.","metadata":{}},{"cell_type":"code","source":"train.isnull().sum()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-13T15:42:30.742820Z","iopub.execute_input":"2025-09-13T15:42:30.743036Z","iopub.status.idle":"2025-09-13T15:42:30.956918Z","shell.execute_reply.started":"2025-09-13T15:42:30.743019Z","shell.execute_reply":"2025-09-13T15:42:30.955981Z"}},"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"id                               0\nbrand                            0\nmodel                            0\nmodel_year                       0\nmilage                           0\nfuel_type                        0\nengine                           0\ntransmission                     0\next_col                          0\nint_col                          0\naccident                         0\nclean_title                      0\nhorsepower                       0\nengine_liters                    0\ncylinders                        0\nconfig                      104907\nextra_descriptors              806\nbase_model                       0\nbody_style                   43144\nbase_color                       0\npaint_finish                     0\ntwo_tone                         0\nraw_ext_col                      0\nbase_int_color                   0\nmaterial                         0\nraw_int_col                      0\ntransmission_type            19782\nnum_speeds                       0\nhas_auto_shift                   0\nspecial_notes                    0\nprice                            0\nlog_price                        0\nhas_accident_reported            0\nis_clean_title                   0\nis_fuel_unknown                  0\nis_horsepower_missing            0\nis_cylinders_missing             0\nis_engine_liters_missing         0\ncar_age                          0\nmileage_per_year                 0\nlog_mileage                      0\nis_luxury_brand                  0\nengine_liters_bin             6698\nbase_model_target_enc            0\nbrand_target_enc                 0\ndtype: int64"},"metadata":{}}],"execution_count":18},{"cell_type":"markdown","source":"### 8. Exploring Body Style and Transmission Type\n\nBefore deciding how to handle missing values in `body_style` and `transmission_type`, I explored their distributions to understand how representative the available data was.\n\n**Body Style**\nThe dataset was heavily concentrated around a few categories:\n\n* **SUVs** (45k) and **Sedans** (37k) dominated the sample, together accounting for nearly half of all entries.\n* **Coupes** (32k) and **Trucks** (18k) also formed significant portions.\n* Niche categories like **Convertibles** (4.8k), **Vans** (3.4k), **Hatchbacks** (2.4k), and **Wagons** (1.2k) were far less common.\n\nThis skew suggests that while most vehicles fall into mainstream body types, rarer categories could still provide useful signals for pricing.","metadata":{}},{"cell_type":"code","source":"train[\"body_style\"].value_counts()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-13T15:42:30.957926Z","iopub.execute_input":"2025-09-13T15:42:30.958266Z","iopub.status.idle":"2025-09-13T15:42:30.979489Z","shell.execute_reply.started":"2025-09-13T15:42:30.958238Z","shell.execute_reply":"2025-09-13T15:42:30.978383Z"}},"outputs":[{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"body_style\nSUV            45415\nSedan          37718\nCoupe          32272\nTruck          18013\nConvertible     4841\nVan             3442\nHatchback       2474\nWagon           1214\nName: count, dtype: int64"},"metadata":{}}],"execution_count":19},{"cell_type":"markdown","source":"**Transmission Type**\nHere the imbalance was even more striking:\n\n* **Automatic** transmissions dominated with \\~150k vehicles, dwarfing the other categories.\n* **Manual** (16k) and **CVT** (2.8k) had moderate presence.\n* **DCT** (18) and **Single-Speed** (16) were essentially negligible in scale.","metadata":{}},{"cell_type":"code","source":"train[\"transmission_type\"].value_counts()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-13T15:42:30.980516Z","iopub.execute_input":"2025-09-13T15:42:30.980826Z","iopub.status.idle":"2025-09-13T15:42:31.009783Z","shell.execute_reply.started":"2025-09-13T15:42:30.980791Z","shell.execute_reply":"2025-09-13T15:42:31.008920Z"}},"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"transmission_type\nAutomatic       149488\nManual           16395\nCVT               2834\nDCT                 18\nSingle-Speed        16\nName: count, dtype: int64"},"metadata":{}}],"execution_count":20},{"cell_type":"markdown","source":"The takeaway was that both `body_style` and `transmission_type` carry meaningful information for price prediction, though they are heavily imbalanced. Missing values here couldn’t just be dropped blindly — they needed thoughtful handling to retain their predictive contribution without introducing noise.","metadata":{}},{"cell_type":"markdown","source":"### 9. Investigating Fill Strategies for Body Style and Transmission Type\n\nAfter identifying `body_style` and `transmission_type` as potentially important predictors, I inspected how reliably these missing values could be imputed using brand, base model, and model year as reference keys.\n\nThe idea was simple: if a particular (brand, model, year) combination always appeared with the same body style or transmission type, then missing values for that group could be filled with high confidence.\n\nThe inspection confirmed that:\n\n* **Body Style**\n  * *Training set*: 43,144 entries were missing (22.9%). Out of these, **7,908 (18.3%)** could be imputed, leaving **35,236 (81.7%)** still missing.\n  * *Test set*: 28,863 entries were missing (23.0%). Out of these, **5,318 (18.4%)** could be filled, with **23,545 (81.6%)** remaining.\n\n* **Transmission Type**\n  * *Training set*: 19,782 entries were missing (10.5%). Here, **8,034 (40.6%)** could be imputed, leaving **11,748 (59.4%)** unresolved.\n  * *Test set*: 13,196 entries were missing (10.5%). Out of these, **5,266 (39.9%)** could be filled, while **7,930 (60.1%)** remained missing.\n\nThis means imputation helped partially recover the missing data — particularly for `transmission_type`, where about 40% could be reconstructed. However, the majority of missing values (especially for `body_style`) still lacked sufficient reference points, leaving a decision between dropping those records or creating an explicit “unknown” category for the model to learn from.","metadata":{}},{"cell_type":"code","source":"# Refactored Inspection & Mapping Function\ndef get_imputation_mapping(dataframe, target_column):\n    \"\"\"\n    Efficiently identifies groups (brand, model, year) with a single consistent\n    non-null value and returns a mapping for imputation.\n\n    Args:\n        dataframe (pd.DataFrame): The DataFrame to inspect.\n        target_column (str): The column with missing values to inspect.\n\n    Returns:\n        pd.Series: A Series mapping (brand, model, year) to the value to impute.\n    \"\"\"\n    # Check if the target column exists in the dataframe\n    if target_column not in dataframe.columns:\n        print(f\"Warning: '{target_column}' not found in the DataFrame. Skipping inspection.\")\n        return None\n\n    # Group by the key columns and get a count of unique non-null values\n    grouped = dataframe.groupby(['brand', 'base_model', 'model_year'])[target_column].nunique(dropna=True)\n    \n    # Filter for groups that have only one unique value\n    confident_groups = grouped[grouped == 1].index\n    \n    # Create the mapping by selecting the first non-null value from these groups\n    mapping = dataframe.set_index(['brand', 'base_model', 'model_year']).loc[confident_groups][target_column].dropna()\n    \n    # Drop duplicates to ensure a unique mapping for each key\n    mapping = mapping[~mapping.index.duplicated(keep='first')]\n    \n    return mapping\n\ndef inspect_with_mapping(dataframe, target_column, mapping, dataset_name):\n    \"\"\"\n    Analyzes a DataFrame's missing values using a pre-computed mapping.\n    \n    Args:\n        dataframe (pd.DataFrame): The DataFrame to inspect.\n        target_column (str): The column with missing values to inspect.\n        mapping (pd.Series): The imputation mapping to use.\n        dataset_name (str): The name of the dataset (e.g., \"TRAINING\", \"TEST\").\n    \"\"\"\n    print(f\"=== Analyzing the {dataset_name} set for '{target_column}' ===\")\n\n    if target_column not in dataframe.columns:\n        print(f\"Column '{target_column}' does not exist in the {dataset_name} set. Skipping analysis.\")\n        print(\"\\n\" + \"=\"*50 + \"\\n\")\n        return\n\n    # Check for missing values\n    missing_count = dataframe[target_column].isnull().sum()\n    total_rows = len(dataframe)\n    missing_percent = (missing_count / total_rows) * 100\n    print(f\"Total rows with missing '{target_column}': {missing_count} ({missing_percent:.2f}%)\")\n\n    # Use the mapping to identify how many of the missing values can be filled\n    dataframe_with_index = dataframe.set_index(['brand', 'base_model', 'model_year'])\n    potential_fills = dataframe_with_index.loc[dataframe_with_index.index.isin(mapping.index)]\n    \n    fillable_missing_count = potential_fills[target_column].isnull().sum()\n\n    print(f\"Number of missing values that can be filled by the mapping: {fillable_missing_count}\")\n    print(f\"Number of remaining missing values: {missing_count - fillable_missing_count}\")\n    print(\"\\n\" + \"=\"*50 + \"\\n\")\n\n\n# --- 3. Run the Inspection for Both Columns and Datasets ---\nif __name__ == '__main__':\n    # First, get the imputation mappings from the TRAINING data only.\n    # This is critical to prevent data leakage from the test set.\n    body_style_mapping = get_imputation_mapping(train, 'body_style')\n    transmission_mapping = get_imputation_mapping(train, 'transmission_type')\n\n    # Now, use these mappings to analyze the train and test sets.\n    inspect_with_mapping(train, 'body_style', body_style_mapping, 'TRAINING')\n    inspect_with_mapping(train, 'transmission_type', transmission_mapping, 'TRAINING')\n\n    # Note: It appears your test set has one fewer column than your training set.\n    # This refactored code gracefully handles the case where a target column\n    # is missing from one of the dataframes.\n\n    inspect_with_mapping(test, 'body_style', body_style_mapping, 'TEST')\n    inspect_with_mapping(test, 'transmission_type', transmission_mapping, 'TEST')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-13T15:42:31.010910Z","iopub.execute_input":"2025-09-13T15:42:31.011225Z","iopub.status.idle":"2025-09-13T15:42:32.687984Z","shell.execute_reply.started":"2025-09-13T15:42:31.011203Z","shell.execute_reply":"2025-09-13T15:42:32.687265Z"}},"outputs":[{"name":"stdout","text":"=== Analyzing the TRAINING set for 'body_style' ===\nTotal rows with missing 'body_style': 43144 (22.88%)\nNumber of missing values that can be filled by the mapping: 7908\nNumber of remaining missing values: 35236\n\n==================================================\n\n=== Analyzing the TRAINING set for 'transmission_type' ===\nTotal rows with missing 'transmission_type': 19782 (10.49%)\nNumber of missing values that can be filled by the mapping: 8034\nNumber of remaining missing values: 11748\n\n==================================================\n\n=== Analyzing the TEST set for 'body_style' ===\nTotal rows with missing 'body_style': 28863 (22.96%)\nNumber of missing values that can be filled by the mapping: 5318\nNumber of remaining missing values: 23545\n\n==================================================\n\n=== Analyzing the TEST set for 'transmission_type' ===\nTotal rows with missing 'transmission_type': 13196 (10.50%)\nNumber of missing values that can be filled by the mapping: 5266\nNumber of remaining missing values: 7930\n\n==================================================\n\n","output_type":"stream"}],"execution_count":21},{"cell_type":"markdown","source":"### 10. Imputation Strategy for Body Style and Transmission Type\nTo address missing values in `body_style` and `transmission_type`, I implemented a multi-step imputation pipeline that combined targeted mapping, brand-level rules, and sensible defaults.\n\n**Step 1. Apply Pre-computed Mapping**\nA helper function `apply_imputation_mapping()` was created to leverage mappings derived from `(brand, base_model, model_year)` combinations. This function:\n* Builds a temporary key for each row.\n* Uses the mapping dictionary to replace missing values wherever a match exists.\n* Removes the temporary key after completion.\n\nOutcome:\n* **8,034** missing `transmission_type` values were filled in the training set.\n* **5,266** missing `transmission_type` values were filled in the test set.\n* This stage alone resolved \\~40% of missingness in transmission type.\n\n**Step 2. Missingness Flags**\nBefore imputation, binary flags were created to capture whether `body_style` or `transmission_type` was missing. These features (`is_body_style_missing`, `is_transmission_type_missing`) were preserved to allow the model to learn from the fact that some entries had incomplete data.\n\n**Step 3. Hierarchical Imputation for Transmission Type**\nFor unresolved missing values after mapping:\n\n1. Filled with the **most common transmission type within each brand**.\n2. Remaining gaps defaulted to `'Automatic'`, since it dominated the dataset (149k+ entries).\n\nThis ensured all transmission records were consistently filled.\n\n**Step 4. Hierarchical Imputation for Body Style**\nFor `body_style`, a similar hierarchical approach was applied:\n\n1. Filled missing values using the **most frequent body style within each (brand, base\\_model)** combination.\n2. If still missing, filled using the **most common style for that brand**.\n3. As a final fallback, missing entries defaulted to `'Sedan'`, reflecting its strong representation in the data (37k entries).\n\n**Step 5. Target Encoding**\nAfter imputation, both `body_style` and `transmission_type` were target-encoded against the log-transformed price. This step converted categorical values into numeric features that captured their statistical relationship with the target variable, enhancing predictive power without introducing leakage.","metadata":{}},{"cell_type":"code","source":"def apply_imputation_mapping(train, test, target_column, mapping):\n    \"\"\"\n    Apply a pre-computed mapping to fill missing values in train and test.\n    Uses (brand, base_model, model_year) as key (to match mapping).\n    \"\"\"\n    if mapping is None:\n        print(f\"⚠️ No mapping for {target_column} — skipping.\")\n        return train, test\n    \n    # Create a temporary key (consistent with mapping)\n    for df in [train, test]:\n        df['_key'] = list(zip(df['brand'], df['base_model'], df['model_year']))\n    \n    # Convert mapping to dict for fast lookup\n    mapping_dict = mapping.to_dict()\n    \n    # Fill missing values in train\n    mask_train = train[target_column].isnull() & train['_key'].isin(mapping_dict.keys())\n    train.loc[mask_train, target_column] = train.loc[mask_train, '_key'].map(mapping_dict)\n    filled_train = mask_train.sum()\n    \n    # Fill missing values in test\n    mask_test = test[target_column].isnull() & test['_key'].isin(mapping_dict.keys())\n    test.loc[mask_test, target_column] = test.loc[mask_test, '_key'].map(mapping_dict)\n    filled_test = mask_test.sum()\n    \n    # Clean up\n    for df in [train, test]:\n        df.drop(columns=['_key'], inplace=True)\n    \n    print(f\"✅ Filled {filled_train} missing '{target_column}' in train using mapping.\")\n    print(f\"✅ Filled {filled_test} missing '{target_column}' in test using mapping.\")\n    \n    return train, test","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-13T15:42:32.688929Z","iopub.execute_input":"2025-09-13T15:42:32.689210Z","iopub.status.idle":"2025-09-13T15:42:32.697517Z","shell.execute_reply.started":"2025-09-13T15:42:32.689182Z","shell.execute_reply":"2025-09-13T15:42:32.696715Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"# Step 1: Create missingness flags (BEFORE imputation)\ntrain['is_transmission_type_missing'] = train['transmission_type'].isnull().astype(int)\ntest['is_transmission_type_missing'] = test['transmission_type'].isnull().astype(int)\n\ntrain['is_body_style_missing'] = train['body_style'].isnull().astype(int)\ntest['is_body_style_missing'] = test['body_style'].isnull().astype(int)\n\n# Step 2: Fill transmission_type with your mapping\ntrain, test = apply_imputation_mapping(train, test, 'transmission_type', transmission_mapping)\n\n# Step 3: Fill remaining transmission_type with mode by brand\nbrand_mode_trans = train.groupby('brand')['transmission_type'].agg(lambda x: x.mode().iloc[0] if not x.mode().empty else 'Automatic')\ntrain['transmission_type'] = train['transmission_type'].fillna(train['brand'].map(brand_mode_trans))\ntest['transmission_type'] = test['transmission_type'].fillna(test['brand'].map(brand_mode_trans))\ntrain['transmission_type'].fillna('Automatic', inplace=True)\ntest['transmission_type'].fillna('Automatic', inplace=True)\n\n# Step 4: Fill body_style with mode by brand + base_model\ntrain['_brand_base'] = train['brand'] + '|' + train['base_model']\ntest['_brand_base'] = test['brand'] + '|' + test['base_model']\n\nbrand_base_mode_body = train.groupby('_brand_base')['body_style'].agg(lambda x: x.mode().iloc[0] if not x.mode().empty else np.nan)\ntrain['body_style'] = train['body_style'].fillna(train['_brand_base'].map(brand_base_mode_body))\ntest['body_style'] = test['body_style'].fillna(test['_brand_base'].map(brand_base_mode_body))\n\nbrand_mode_body = train.groupby('brand')['body_style'].agg(lambda x: x.mode().iloc[0] if not x.mode().empty else 'Sedan')\ntrain['body_style'] = train['body_style'].fillna(train['brand'].map(brand_mode_body))\ntest['body_style'] = test['body_style'].fillna(test['brand'].map(brand_mode_body))\n\ntrain['body_style'].fillna('Sedan', inplace=True)\ntest['body_style'].fillna('Sedan', inplace=True)\n\n# Clean up\nfor df in [train, test]:\n    df.drop(columns=['_brand_base'], inplace=True)\n\n# Step 5: Target encode\ntrain, test = create_target_encodings(\n    train, \n    test, \n    cat_cols=['body_style', 'transmission_type'], \n    target_col='log_price'\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-13T15:42:32.698571Z","iopub.execute_input":"2025-09-13T15:42:32.699277Z","iopub.status.idle":"2025-09-13T15:42:34.168269Z","shell.execute_reply.started":"2025-09-13T15:42:32.699247Z","shell.execute_reply":"2025-09-13T15:42:34.167300Z"}},"outputs":[{"name":"stdout","text":"✅ Filled 8034 missing 'transmission_type' in train using mapping.\n✅ Filled 5266 missing 'transmission_type' in test using mapping.\n","output_type":"stream"}],"execution_count":23},{"cell_type":"markdown","source":"**Final Results**\nPost-imputation checks confirmed:\n\n* **0 missing values** for `body_style` in both train and test.\n* **0 missing values** for `transmission_type` in both train and test.\n\nThis workflow ensured that all rows were preserved, the categorical variables were fully usable for modeling, and the imputation process maintained consistency between training and test sets.","metadata":{}},{"cell_type":"code","source":"print(\"=== AFTER IMPUTATION ===\")\nprint(f\"Train body_style missing: {train['body_style'].isnull().sum()}\")\nprint(f\"Train transmission_type missing: {train['transmission_type'].isnull().sum()}\")\nprint(f\"Test body_style missing: {test['body_style'].isnull().sum()}\")\nprint(f\"Test transmission_type missing: {test['transmission_type'].isnull().sum()}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-13T15:42:34.169146Z","iopub.execute_input":"2025-09-13T15:42:34.169385Z","iopub.status.idle":"2025-09-13T15:42:34.211603Z","shell.execute_reply.started":"2025-09-13T15:42:34.169367Z","shell.execute_reply":"2025-09-13T15:42:34.210587Z"}},"outputs":[{"name":"stdout","text":"=== AFTER IMPUTATION ===\nTrain body_style missing: 0\nTrain transmission_type missing: 0\nTest body_style missing: 0\nTest transmission_type missing: 0\n","output_type":"stream"}],"execution_count":24},{"cell_type":"markdown","source":"### Identifying Luxury Brands\nThis block defined luxury brands as those with a **median price above \\$60,000**. By grouping the training set by `brand` and calculating median prices, a list of brands crossing the threshold was created. A new binary flag `is_luxury_brand` was then added to both the training and test sets to capture whether a vehicle belongs to this luxury segment.\n\n**Result:**\nA total of **8 luxury brands** were identified:\n`['Aston', 'Bentley', 'Bugatti', 'Ferrari', 'Lamborghini', 'McLaren', 'Rivian', 'Rolls-Royce']`.","metadata":{}},{"cell_type":"code","source":"# Define luxury brands as those with median price > $60,000\nluxury_threshold = 60000\nbrand_median_price = train.groupby('brand')['price'].median()\nluxury_brands = brand_median_price[brand_median_price > luxury_threshold].index.tolist()\n\ntrain['is_luxury_brand'] = train['brand'].isin(luxury_brands).astype(int)\ntest['is_luxury_brand'] = test['brand'].isin(luxury_brands).astype(int)\n\nprint(f\"✅ Created is_luxury_brand — {len(luxury_brands)} luxury brands identified:\")\nprint(luxury_brands)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-13T15:42:34.591225Z","iopub.execute_input":"2025-09-13T15:42:34.591509Z","iopub.status.idle":"2025-09-13T15:42:34.632991Z","shell.execute_reply.started":"2025-09-13T15:42:34.591487Z","shell.execute_reply":"2025-09-13T15:42:34.632125Z"}},"outputs":[{"name":"stdout","text":"✅ Created is_luxury_brand — 8 luxury brands identified:\n['Aston', 'Bentley', 'Bugatti', 'Ferrari', 'Lamborghini', 'McLaren', 'Rivian', 'Rolls-Royce']\n","output_type":"stream"}],"execution_count":27},{"cell_type":"markdown","source":"### Numeric Feature Analysis\nHere I performed a deeper diagnostic on numeric features in three ways:\n1. **Correlation with Price** – Each numeric feature was ranked by how strongly it correlated with the target variable.\n   * Strongest negative correlations were found for `log_mileage (-0.31)` and `milage (-0.28)`, confirming that higher mileage lowers price.\n   * Positive correlations included `horsepower (0.25)` and `model_year (0.23)`, reflecting that newer, more powerful cars tend to be more expensive.\n\n2. **Skewness** – Assessed distributional skew for numeric features.\n   * Highly skewed variables included `has_auto_shift (9.86)` and `num_speeds (-2.93)`, suggesting transformation or careful treatment may be required.\n   * Most continuous features such as `horsepower (0.78)` and `engine_liters (0.51)` showed mild skewness.\n\n3. **Outliers (IQR Method)** – Outlier counts and proportions were flagged.\n   * `num_speeds` had the highest outlier rate, with **16,413 cases (8.7%)** outside the IQR range.\n   * Mileage-related variables (`log_mileage: 7,521 outliers, 4.0%`) and temporal features (`model_year` and `car_age`: \\~2.7% each) also exhibited notable outliers.","metadata":{}},{"cell_type":"code","source":"# Identify numeric columns (exclude target encodings and binary flags for now)\nnumeric_cols = train.select_dtypes(include=['int64', 'float64']).columns.tolist()\nexclude_cols = [\n    'id', 'log_price', 'price', \n    'base_model_target_enc', 'brand_target_enc', \n    'body_style_target_enc', 'transmission_type_target_enc',\n    'has_accident_reported', 'is_clean_title', 'is_fuel_unknown',\n    'is_horsepower_missing', 'is_cylinders_missing', 'is_engine_liters_missing',\n    'is_luxury_brand', 'is_transmission_type_missing', 'is_body_style_missing'\n]\n\nnumeric_cols = [col for col in numeric_cols if col not in exclude_cols]\n\n# Calculate correlation with price\ncorr_with_price = train[numeric_cols + ['price']].corr()['price'].drop('price').sort_values(key=abs, ascending=False)\n\nprint(\"📊 === NUMERIC FEATURES — CORRELATION WITH PRICE ===\")\nprint(corr_with_price.to_string())\n\n# Calculate skewness\nfrom scipy.stats import skew\n\nskewness = train[numeric_cols].apply(lambda x: skew(x.dropna())).sort_values(key=abs, ascending=False)\n\nprint(\"\\n📉 === NUMERIC FEATURES — SKEWNESS ===\")\nprint(skewness.to_string())\n\n# Identify outliers (using IQR)\noutlier_info = {}\nfor col in numeric_cols:\n    Q1 = train[col].quantile(0.25)\n    Q3 = train[col].quantile(0.75)\n    IQR = Q3 - Q1\n    lower_bound = Q1 - 1.5 * IQR\n    upper_bound = Q3 + 1.5 * IQR\n    outliers = ((train[col] < lower_bound) | (train[col] > upper_bound)).sum()\n    outlier_pct = outliers / len(train) * 100\n    outlier_info[col] = {'outliers': outliers, 'pct': outlier_pct}\n\nprint(\"\\n🚨 === NUMERIC FEATURES — OUTLIERS (IQR METHOD) ===\")\nfor col, info in sorted(outlier_info.items(), key=lambda x: x[1]['pct'], reverse=True):\n    if info['pct'] > 1:\n        print(f\"{col:<20} : {info['outliers']:>6} outliers ({info['pct']:.1f}%)\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-13T15:42:34.637458Z","iopub.execute_input":"2025-09-13T15:42:34.637725Z","iopub.status.idle":"2025-09-13T15:42:34.952156Z","shell.execute_reply.started":"2025-09-13T15:42:34.637704Z","shell.execute_reply":"2025-09-13T15:42:34.951192Z"}},"outputs":[{"name":"stdout","text":"📊 === NUMERIC FEATURES — CORRELATION WITH PRICE ===\nlog_mileage         -0.31\nmilage              -0.28\nhorsepower           0.25\ncar_age             -0.23\nmodel_year           0.23\nmileage_per_year    -0.19\ncylinders            0.13\nhas_auto_shift       0.10\nengine_liters_bin    0.10\nengine_liters        0.09\nnum_speeds           0.04\n\n📉 === NUMERIC FEATURES — SKEWNESS ===\nhas_auto_shift       9.86\nnum_speeds          -2.93\nmileage_per_year     1.67\nlog_mileage         -1.35\ncar_age              1.04\nmodel_year          -1.04\nmilage               0.90\nhorsepower           0.78\nengine_liters        0.51\nengine_liters_bin    0.42\ncylinders            0.25\n\n🚨 === NUMERIC FEATURES — OUTLIERS (IQR METHOD) ===\nnum_speeds           :  16413 outliers (8.7%)\nlog_mileage          :   7521 outliers (4.0%)\nmodel_year           :   5132 outliers (2.7%)\ncar_age              :   5132 outliers (2.7%)\nmileage_per_year     :   3316 outliers (1.8%)\nhorsepower           :   2832 outliers (1.5%)\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/pandas/core/computation/expressions.py:73: RuntimeWarning: invalid value encountered in less\n  return op(a, b)\n/usr/local/lib/python3.11/dist-packages/pandas/core/computation/expressions.py:73: RuntimeWarning: invalid value encountered in greater\n  return op(a, b)\n","output_type":"stream"}],"execution_count":28},{"cell_type":"markdown","source":"### Categorical Feature Predictive Power\nHere, the predictive strength of categorical features was assessed by computing the **variance of their target encodings** (mean price per category). A higher variance indicates stronger differentiation in price across categories, hence higher predictive power.\n\n**Results (Top Predictors):**\n* `material` stood out as the strongest categorical predictor with a variance of **\\$9.87B across 2 categories**, suggesting a major price split depending on material type.\n* `brand (\\$1.30B variance, 57 categories)` and `transmission_type (\\$1.20B variance, 5 categories)` also ranked highly, confirming their strong role in pricing.\n* `base_model` showed predictive power as well (\\$1.16B variance, 542 categories), though its large number of categories increases complexity.\n* Features like `paint_finish (\\$648M)` and `base_color (\\$245M)` provided moderate predictive signals, while `fuel_type (\\$122M)` and `body_style (\\$133M)` were weaker by comparison.","metadata":{}},{"cell_type":"code","source":"# Categorical columns to analyze\ncat_cols = ['brand', 'base_model', 'fuel_type', 'transmission_type', 'body_style', 'base_color', 'paint_finish', 'material', 'base_int_color']\n\nprint(\"🎯 === CATEGORICAL FEATURES — PREDICTIVE POWER (VARIANCE OF TARGET ENCODING) ===\")\n\ncat_predictive_power = {}\n\nfor col in cat_cols:\n    # Compute target encoding (mean price per category)\n    target_enc = train.groupby(col)['price'].mean()\n    # Compute variance of target encoding → high variance = high predictive power\n    variance = target_enc.var()\n    n_categories = train[col].nunique()\n    cat_predictive_power[col] = {\n        'variance': variance,\n        'n_categories': n_categories\n    }\n\n# Sort by variance (descending)\nsorted_cats = sorted(cat_predictive_power.items(), key=lambda x: x[1]['variance'], reverse=True)\n\nfor col, info in sorted_cats:\n    print(f\"{col:<20} : Variance=${info['variance']:>12,.0f} | Categories={info['n_categories']}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-13T15:42:34.953047Z","iopub.execute_input":"2025-09-13T15:42:34.953352Z","iopub.status.idle":"2025-09-13T15:42:35.171172Z","shell.execute_reply.started":"2025-09-13T15:42:34.953331Z","shell.execute_reply":"2025-09-13T15:42:35.170307Z"}},"outputs":[{"name":"stdout","text":"🎯 === CATEGORICAL FEATURES — PREDICTIVE POWER (VARIANCE OF TARGET ENCODING) ===\nmaterial             : Variance=$9,869,372,095 | Categories=2\nbrand                : Variance=$1,298,212,649 | Categories=57\ntransmission_type    : Variance=$1,202,098,793 | Categories=5\nbase_model           : Variance=$1,158,580,683 | Categories=542\npaint_finish         : Variance=$ 647,939,532 | Categories=6\nbase_color           : Variance=$ 244,808,306 | Categories=14\nbase_int_color       : Variance=$ 160,210,716 | Categories=11\nbody_style           : Variance=$ 132,870,027 | Categories=8\nfuel_type            : Variance=$ 122,176,916 | Categories=6\n","output_type":"stream"}],"execution_count":29},{"cell_type":"markdown","source":"### **Follow-up Decision:**\n\nAlthough `material` ranked highest in variance, the feature itself lacked meaningful variability. Nearly all entries were labeled as `unknown (188,356)` versus just `leather (177)`, making the feature heavily imbalanced and uninformative. For this reason, `material` was dropped from the dataset despite its apparent variance, as the predictive signal was artificial and driven by the dominance of the `unknown` category rather than genuine information.","metadata":{}},{"cell_type":"code","source":"train[\"material\"].value_counts()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-13T15:42:35.172029Z","iopub.execute_input":"2025-09-13T15:42:35.172363Z","iopub.status.idle":"2025-09-13T15:42:35.195374Z","shell.execute_reply.started":"2025-09-13T15:42:35.172341Z","shell.execute_reply":"2025-09-13T15:42:35.194057Z"}},"outputs":[{"execution_count":30,"output_type":"execute_result","data":{"text/plain":"material\nUnknown    188356\nLeather       177\nName: count, dtype: int64"},"metadata":{}}],"execution_count":30},{"cell_type":"code","source":"# Keep ids safe before dropping\ntrain_ids = train['id'].copy()\ntest_ids = test['id'].copy()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-13T15:42:35.238413Z","iopub.execute_input":"2025-09-13T15:42:35.238731Z","iopub.status.idle":"2025-09-13T15:42:35.257385Z","shell.execute_reply.started":"2025-09-13T15:42:35.238706Z","shell.execute_reply":"2025-09-13T15:42:35.255977Z"}},"outputs":[],"execution_count":33},{"cell_type":"markdown","source":"## Pre-Model Feature Engineering and Refinement\nBefore moving to model training, I focused on systematically preparing the dataset to ensure cleaner inputs and more stable learning. The steps taken here were shaped directly by earlier investigations into outliers, skewness, and categorical feature behavior.\n\n### 1. Removing Irrelevant and Redundant Features\nSeveral columns were excluded because they either carried no predictive signal (e.g., identifiers, descriptive text, raw color fields) or risked introducing noise and redundancy. Consolidating these removals into a single pipeline step ensured consistency between train and test sets.","metadata":{}},{"cell_type":"code","source":"to_drop = [\n    'id',\n    'model',\n    'engine',\n    'transmission',\n    'ext_col',\n    'int_col',\n    'accident',\n    'clean_title',\n    'config',\n    'extra_descriptors',\n    'raw_ext_col',\n    'raw_int_col',\n    'special_notes',\n    'engine_liters_bin',\n    'base_model'\n]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-13T15:42:35.258614Z","iopub.execute_input":"2025-09-13T15:42:35.258949Z","iopub.status.idle":"2025-09-13T15:42:35.270386Z","shell.execute_reply.started":"2025-09-13T15:42:35.258922Z","shell.execute_reply":"2025-09-13T15:42:35.268937Z"}},"outputs":[],"execution_count":34},{"cell_type":"markdown","source":"### 2. Handling Skewness with Log Transforms\nFrom the skewness analysis, I identified that both **car age** and **mileage per year** exhibited long-tailed distributions. Applying log transforms reduced their skew, making them more Gaussian-like and better suited for models sensitive to feature distribution. The raw versions of these features were then dropped to avoid duplication.","metadata":{}},{"cell_type":"code","source":"# Log transforms for skewed features\ntrain['log_mileage_per_year'] = np.log1p(train['mileage_per_year'])\ntest['log_mileage_per_year'] = np.log1p(test['mileage_per_year'])\n\ntrain['log_car_age'] = np.log1p(train['car_age'])\ntest['log_car_age'] = np.log1p(test['car_age'])\n\n# Combine all columns to drop\nfinal_drop = to_drop + ['model_year', 'mileage_per_year', 'car_age']\n\n# Drop them in one go\ntrain_df = train.drop(columns=final_drop, errors='ignore')\ntest_df = test.drop(columns=final_drop, errors='ignore')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-13T15:42:35.271392Z","iopub.execute_input":"2025-09-13T15:42:35.271681Z","iopub.status.idle":"2025-09-13T15:42:35.340659Z","shell.execute_reply.started":"2025-09-13T15:42:35.271659Z","shell.execute_reply":"2025-09-13T15:42:35.339660Z"}},"outputs":[],"execution_count":35},{"cell_type":"markdown","source":"### 3. Capping Extreme Outliers\nOur earlier outlier checks revealed heavy right tails in variables like horsepower and log-transformed mileage. Instead of discarding data, I applied percentile-based capping — clipping horsepower at the 99th percentile and log mileage between the 1st and 99th. This approach retained most of the data while reducing the influence of extreme points.","metadata":{}},{"cell_type":"code","source":"# Cap horsepower at 99th percentile\nhp_99 = train_df['horsepower'].quantile(0.99)\ntrain_df['horsepower'] = train_df['horsepower'].clip(upper=hp_99)\ntest_df['horsepower'] = test_df['horsepower'].clip(upper=hp_99)\n\n# Cap log_mileage at 1st and 99th percentiles\nlm_01 = train_df['log_mileage'].quantile(0.01)\nlm_99 = train_df['log_mileage'].quantile(0.99)\ntrain_df['log_mileage'] = train_df['log_mileage'].clip(lower=lm_01, upper=lm_99)\ntest_df['log_mileage'] = test_df['log_mileage'].clip(lower=lm_01, upper=lm_99)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-13T15:42:35.341658Z","iopub.execute_input":"2025-09-13T15:42:35.341929Z","iopub.status.idle":"2025-09-13T15:42:35.378013Z","shell.execute_reply.started":"2025-09-13T15:42:35.341908Z","shell.execute_reply":"2025-09-13T15:42:35.376987Z"}},"outputs":[],"execution_count":36},{"cell_type":"markdown","source":"### 4. Encoding Categorical Variables\nCategorical features required tailored treatment:\n* **Dropped Columns with No Variability:** e.g., *material* showed no variation across rows and was excluded.\n* **Target Encoding for Base Color:** Instead of one-hot encoding all 14 categories, I used mean target encoding, replacing colors with the average log-price of cars in that color. This preserved signal strength without over-expanding dimensions.\n* **One-Hot Encoding for Key Features:** Variables like *transmission type, paint finish, body style, fuel type,* and *base interior color* were one-hot encoded, adding 36 new binary columns.\n\n### 5. Final Dataset Shape\nAfter all refinements, the training set contained **62 columns** (including price-related targets) and the test set contained **60 columns**. This finalized dataset provided a balanced mix of numeric and encoded categorical features, optimized for downstream modeling","metadata":{}},{"cell_type":"code","source":"# 1. DROP material — no variability\ntrain_df = train_df.drop(columns=['material'], errors='ignore')\ntest_df = test_df.drop(columns=['material'], errors='ignore')\n\n# 2. DROP raw brand — use brand_target_enc\ntrain_df = train_df.drop(columns=['brand'], errors='ignore')\ntest_df = test_df.drop(columns=['brand'], errors='ignore')\n\n# 3. TARGET ENCODE base_color (14 categories)\n# Compute mapping from train only\nbase_color_target_map = train_df.groupby('base_color')['log_price'].mean()\nglobal_price_mean = train_df['log_price'].mean()\n\ntrain_df['base_color_target_enc'] = train_df['base_color'].map(base_color_target_map).fillna(global_price_mean)\ntest_df['base_color_target_enc'] = test_df['base_color'].map(base_color_target_map).fillna(global_price_mean)\n\n# Drop raw base_color\ntrain_df = train_df.drop(columns=['base_color'], errors='ignore')\ntest_df = test_df.drop(columns=['base_color'], errors='ignore')\n\n# 4. ONE-HOT ENCODE: transmission_type, paint_finish, body_style, fuel_type\n# Columns to one-hot encode\nohe_cols = ['transmission_type', 'paint_finish', 'body_style', 'fuel_type', 'base_int_color']\n\n# Initialize encoder\nohe = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n\n# Fit on train\nohe.fit(train_df[ohe_cols].astype(str))\n\n# Transform train\nohe_train = ohe.transform(train_df[ohe_cols].astype(str))\nohe_train_df = pd.DataFrame(ohe_train, columns=ohe.get_feature_names_out(ohe_cols), index=train_df.index)\n\n# Transform test\nohe_test = ohe.transform(test_df[ohe_cols].astype(str))\nohe_test_df = pd.DataFrame(ohe_test, columns=ohe.get_feature_names_out(ohe_cols), index=test_df.index)\n\n# Concatenate\ntrain_df = pd.concat([train_df.drop(columns=ohe_cols), ohe_train_df], axis=1)\ntest_df = pd.concat([test_df.drop(columns=ohe_cols), ohe_test_df], axis=1)\n\nprint(f\"✅ One-Hot Encoding complete. Added {ohe_train_df.shape[1]} columns.\")\nprint(f\"   Train shape: {train_df.shape}\")\nprint(f\"   Test shape: {test_df.shape}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-13T15:42:35.379125Z","iopub.execute_input":"2025-09-13T15:42:35.379476Z","iopub.status.idle":"2025-09-13T15:42:36.424416Z","shell.execute_reply.started":"2025-09-13T15:42:35.379445Z","shell.execute_reply":"2025-09-13T15:42:36.422815Z"}},"outputs":[{"name":"stdout","text":"✅ One-Hot Encoding complete. Added 36 columns.\n   Train shape: (188533, 62)\n   Test shape: (125690, 60)\n","output_type":"stream"}],"execution_count":37},{"cell_type":"markdown","source":"## Building the Models\n### First Model: Ridge Regression\nFor my baseline model, I started with **Ridge Regression**. This was a good first step because Ridge is:\n* A linear model that’s easy to interpret\n* Regularized (L2 penalty), which makes it less sensitive to multicollinearity\n* A strong benchmark before moving to more flexible models\n\n#### Setup\n* Target: `log_price`(raw `price` was dropped)\n* Features: 60 engineered predictors\n* Scaling: Applied **StandardScaler** inside each fold (Ridge is scale-sensitive)\n\n#### Cross-Validation Strategy\nI didn’t use plain KFold. Instead, I went with **StratifiedKFold**, but adapted it for regression:\n* Created bins of *log\\_price* (10 quantile-based bins)\n* Ensured each fold had a balanced mix of cheap ↔ expensive cars\n* This was smart because it gave me more stable and representative validation scores, reducing variance across folds and improving generalization to the unseen test set.\n\n#### Results\n* **Fold RMSEs (raw price scale):**\n  * Fold 1: 67,084\n  * Fold 2: 71,486\n  * Fold 3: 76,148\n  * Fold 4: 77,704\n  * Fold 5: 77,291\n* **Final CV RMSE:** 74,055\n* **Mean Fold RMSE:** 73,943 ± 4,082\n\n#### Notes\n* Out-of-fold (OOF) predictions were saved for stacking (`ridge_oof.npy`).\n* This gave me a solid baseline: Ridge isn’t complex, but it proved my features were carrying real signal.","metadata":{}},{"cell_type":"code","source":"# Drop 'price' for training (keep 'log_price' as target)\ntrain_modeling = train_df.drop(columns=['price'], errors='ignore')\ntest_modeling = test_df  # test has no price\n\n# Final feature list (all 60 features except 'price' and 'log_price')\nfeature_cols = [col for col in train_modeling.columns if col not in ['log_price']]\nX = train_modeling[feature_cols]\ny = train_modeling['log_price']  # ← TARGET: log_price\nX_test = test_modeling[feature_cols]\n\n# 2. CREATE PRICE BINS FOR STRATIFIED K-FOLD\ntrain_modeling['price_bin'] = pd.qcut(train_modeling['log_price'], q=10, labels=False)\n\n# 3. STRATIFIED K-FOLD CV (5 FOLDS)\nskf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\noof_preds_log = np.zeros(len(X))\ntest_preds_log = np.zeros(len(X_test))\nfold_scores = []\n\nfor fold, (train_idx, val_idx) in enumerate(skf.split(X, train_modeling['price_bin'])):\n    print(f\"=== FOLD {fold+1} ===\")\n    \n    # Split data\n    X_train_fold = X.iloc[train_idx]\n    y_train_fold = y.iloc[train_idx]\n    X_val_fold = X.iloc[val_idx]\n    y_val_fold = y.iloc[val_idx]\n    \n    # Scale features (Ridge needs scaling)\n    scaler = StandardScaler()\n    X_train_fold_scaled = scaler.fit_transform(X_train_fold)\n    X_val_fold_scaled = scaler.transform(X_val_fold)\n    X_test_scaled = scaler.transform(X_test)\n    \n    # Train Ridge\n    model = Ridge(alpha=1.0, random_state=42)\n    model.fit(X_train_fold_scaled, y_train_fold)\n    \n    # Predict on validation (log scale)\n    val_pred_log = model.predict(X_val_fold_scaled)\n    oof_preds_log[val_idx] = val_pred_log\n    \n    # Convert to raw price for scoring\n    val_pred_price = np.expm1(val_pred_log)\n    y_val_price = np.expm1(y_val_fold)\n    \n    # Score on RAW price (RMSE)\n    fold_rmse = np.sqrt(mean_squared_error(y_val_price, val_pred_price))\n    fold_scores.append(fold_rmse)\n    print(f\"  Fold {fold+1} RMSE (raw price): {fold_rmse:.2f}\")\n    \n    # Predict on test (log scale) → average later\n    test_pred_log = model.predict(X_test_scaled)\n    test_preds_log += test_pred_log / 5\n\n# 4. FINAL CV SCORE\noof_preds_price = np.expm1(oof_preds_log)  # ← RAW PRICE OOF\ny_train_price = np.expm1(y)\ncv_rmse = np.sqrt(mean_squared_error(y_train_price, oof_preds_price))\nprint(f\"\\n✅ FINAL CV RMSE (raw price): {cv_rmse:.2f}\")\nprint(f\"✅ FOLD SCORES: {fold_scores}\")\nprint(f\"✅ MEAN FOLD RMSE: {np.mean(fold_scores):.2f} ± {np.std(fold_scores):.2f}\")\n\n# 5. SAVE OOF PREDICTIONS FOR STACKING (RAW PRICE)\nnp.save('/kaggle/working/ridge_oof.npy', oof_preds_price)  # ← ADDED THIS LINE\nprint(f\"✅ Saved ridge_oof.npy (shape: {oof_preds_price.shape}) for stacking.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-13T20:48:04.499645Z","iopub.execute_input":"2025-09-13T20:48:04.500319Z","iopub.status.idle":"2025-09-13T20:48:13.318980Z","shell.execute_reply.started":"2025-09-13T20:48:04.500288Z","shell.execute_reply":"2025-09-13T20:48:13.317941Z"}},"outputs":[{"name":"stdout","text":"=== FOLD 1 ===\n  Fold 1 RMSE (raw price): 67084.05\n=== FOLD 2 ===\n  Fold 2 RMSE (raw price): 71486.05\n=== FOLD 3 ===\n  Fold 3 RMSE (raw price): 76147.90\n=== FOLD 4 ===\n  Fold 4 RMSE (raw price): 77704.37\n=== FOLD 5 ===\n  Fold 5 RMSE (raw price): 77291.42\n\n✅ FINAL CV RMSE (raw price): 74055.29\n✅ FOLD SCORES: [67084.04908556341, 71486.04717802184, 76147.904179928, 77704.37256796678, 77291.41963145764]\n✅ MEAN FOLD RMSE: 73942.76 ± 4081.75\n✅ Saved ridge_oof.npy (shape: (188533,)) for stacking.\n","output_type":"stream"}],"execution_count":60},{"cell_type":"markdown","source":"### Sanity Check: Beating the Mean Price Baseline\nBefore moving on, I wanted to confirm whether Ridge was actually learning anything useful, or if it was no better than just predicting the **average car price for everyone**.\n\n* The **mean price baseline RMSE** came out to **\\$78,819**.\n* In comparison, Ridge achieved a **CV RMSE of \\~\\$74,055**, comfortably lower than the baseline.\n\nThis gap shows that Ridge was already capturing meaningful relationships between features and price. Even though the improvement isn’t massive, it’s a good sign that the pipeline is on the right track.","metadata":{}},{"cell_type":"code","source":"# Check if Ridge is better than predicting mean price\nmean_price = y_train_price.mean()\nbaseline_preds = np.full(len(y_train_price), mean_price)\nbaseline_rmse = np.sqrt(mean_squared_error(y_train_price, baseline_preds))\nprint(f\"🎯 Mean Price Baseline RMSE: ${baseline_rmse:,.0f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-13T15:42:44.675110Z","iopub.execute_input":"2025-09-13T15:42:44.675392Z","iopub.status.idle":"2025-09-13T15:42:44.684453Z","shell.execute_reply.started":"2025-09-13T15:42:44.675364Z","shell.execute_reply":"2025-09-13T15:42:44.683414Z"}},"outputs":[{"name":"stdout","text":"🎯 Mean Price Baseline RMSE: $78,819\n","output_type":"stream"}],"execution_count":40},{"cell_type":"code","source":"'''\n# 5. FINAL TEST PREDICTIONS (replace price in sub with predictions)\ntest_preds_price = np.expm1(test_preds_log)\n\n# Drop old price column if it exists\nsub = sub.drop(columns=['price'], errors='ignore')\n\n# Insert predictions\nsub['price'] = test_preds_price\n\n# Save submission\nsub.to_csv('submission_ridge_baseline.csv', index=False)\n\nprint(f\"\\n✅ Submission saved: submission_ridge_baseline.csv\")\nprint(f\"📊 Submission shape: {sub.shape}\")\nprint(f\"📈 Sample predictions:\\n{sub.head().to_string(index=False)}\")'''","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-13T15:42:44.685440Z","iopub.execute_input":"2025-09-13T15:42:44.685681Z","iopub.status.idle":"2025-09-13T15:42:44.702025Z","shell.execute_reply.started":"2025-09-13T15:42:44.685663Z","shell.execute_reply":"2025-09-13T15:42:44.701136Z"}},"outputs":[{"execution_count":41,"output_type":"execute_result","data":{"text/plain":"'\\n# 5. FINAL TEST PREDICTIONS (replace price in sub with predictions)\\ntest_preds_price = np.expm1(test_preds_log)\\n\\n# Drop old price column if it exists\\nsub = sub.drop(columns=[\\'price\\'], errors=\\'ignore\\')\\n\\n# Insert predictions\\nsub[\\'price\\'] = test_preds_price\\n\\n# Save submission\\nsub.to_csv(\\'submission_ridge_baseline.csv\\', index=False)\\n\\nprint(f\"\\n✅ Submission saved: submission_ridge_baseline.csv\")\\nprint(f\"📊 Submission shape: {sub.shape}\")\\nprint(f\"📈 Sample predictions:\\n{sub.head().to_string(index=False)}\")'"},"metadata":{}}],"execution_count":41},{"cell_type":"markdown","source":"### LightGBM Model\nAfter testing Ridge as a strong linear baseline, I moved on to **LightGBM (LGBM)** — a gradient boosting model known for handling non-linear relationships and mixed feature types more effectively. I kept the same **StratifiedKFold CV setup** to ensure robust evaluation across different price ranges.\n\n#### How the training worked\n* **Cross-validation setup:** 5-fold StratifiedKFold on price bins (to avoid over-representing certain price ranges).\n* **Training:** Each fold trained up to 10,000 rounds, but with **early stopping (patience = 100)** to prevent overfitting.\n* **Parameters:** Conservative defaults (learning rate = 0.05, num\\_leaves = 31, feature\\_fraction = 0.8, etc.) aimed at stability and generalization.\n* **Evaluation metric:** RMSE on raw price.\n\n#### Results\n* Fold-level RMSEs ranged from **66,389 to 77,005**.\n* **Final CV RMSE:** **73,365**, an improvement over Ridge (\\~74,055).\n* Variance across folds (\\~4.6k) suggests consistent performance.\n\n#### Feature importance (top drivers)\nLightGBM also gave a window into what features mattered most. The top contributors were:\n* **Mileage & mileage-derived features**: `mileage`, `log_mileage_per_year`, `log_mileage`\n* **Vehicle characteristics**: `horsepower`, `cylinders`, `engine_liters`\n* **Encodings**: `base_model_target_enc`, `brand_target_enc`, `base_color_target_enc`, `body_style_target_enc`\n* **Vehicle age**: `log_car_age`\n* **Body style categories**: SUVs and Sedans still held signal after encoding.\n\n#### Interpretation\n* **Why it worked better than Ridge:** Ridge only captures linear trends, while LGBM leveraged the **non-linear effects** of age, mileage, and encodings.\n* **Why StratifiedKFold was smart here:** Price distribution is skewed — stratification ensured that each fold represented both cheaper and high-end cars fairly, reducing the risk of overfitting to one segment.\n* The feature importance ranking also confirmed that the preprocessing steps (e.g., log transforms, encodings) were meaningful — especially since engineered features consistently ranked at the top.\n\n#### Next step\nThe OOF predictions were saved (`lgb_oof.csv`), which I’ll later use for **stacking and blending models** to push performance further.","metadata":{}},{"cell_type":"code","source":"# Prepare data (same as before)\ntrain_modeling = train_df.drop(columns=['price'], errors='ignore')\ntest_modeling = test_df\n\nfeature_cols = [col for col in train_modeling.columns if col not in ['log_price']]\nX = train_modeling[feature_cols]\ny = train_modeling['log_price']\nX_test = test_modeling[feature_cols]\n\n# Create price bins for stratification\ntrain_modeling['price_bin'] = pd.qcut(train_modeling['log_price'], q=10, labels=False)\n\n# CV\nskf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\noof_preds_log = np.zeros(len(X))\ntest_preds_log = np.zeros(len(X_test))\nfold_scores = []\nimportances = []\n\nfor fold, (train_idx, val_idx) in enumerate(skf.split(X, train_modeling['price_bin'])):\n    print(f\"=== FOLD {fold+1} ===\")\n    \n    X_train_fold = X.iloc[train_idx]\n    y_train_fold = y.iloc[train_idx]\n    X_val_fold = X.iloc[val_idx]\n    y_val_fold = y.iloc[val_idx]\n    \n    # LightGBM Dataset\n    train_ds = lgb.Dataset(X_train_fold, label=y_train_fold)\n    val_ds = lgb.Dataset(X_val_fold, label=y_val_fold, reference=train_ds)\n    \n    # Parameters\n    params = {\n        'objective': 'regression',\n        'metric': 'rmse',\n        'boosting_type': 'gbdt',\n        'learning_rate': 0.05,\n        'num_leaves': 31,\n        'max_depth': -1,\n        'min_data_in_leaf': 200,\n        'feature_fraction': 0.8,\n        'bagging_fraction': 0.8,\n        'bagging_freq': 5,\n        'seed': 42,\n        'verbose': -1\n    }\n    \n    # Train\n    model = lgb.train(\n        params,\n        train_ds,\n        valid_sets=[val_ds],\n        num_boost_round=10000,\n        callbacks=[lgb.early_stopping(100), lgb.log_evaluation(200)]\n    )\n    \n    # Predict\n    val_pred_log = model.predict(X_val_fold)\n    oof_preds_log[val_idx] = val_pred_log\n    \n    # Score on raw price\n    val_pred_price = np.expm1(val_pred_log)\n    y_val_price = np.expm1(y_val_fold)\n    fold_rmse = np.sqrt(mean_squared_error(y_val_price, val_pred_price))\n    fold_scores.append(fold_rmse)\n    print(f\"  Fold {fold+1} RMSE: {fold_rmse:.2f}\")\n    \n    # Feature importance\n    importances.append(model.feature_importance())\n    \n    # Predict on test\n    test_pred_log = model.predict(X_test)\n    test_preds_log += test_pred_log / 5\n\n# Final CV score\noof_preds_price = np.expm1(oof_preds_log)\ny_train_price = np.expm1(y)\ncv_rmse = np.sqrt(mean_squared_error(y_train_price, oof_preds_price))\nprint(f\"\\n✅ FINAL CV RMSE: {cv_rmse:.2f}\")\n\n# Feature importance\nimportance_df = pd.DataFrame({\n    'feature': feature_cols,\n    'importance': np.mean(importances, axis=0)\n}).sort_values('importance', ascending=False)\n\nprint(\"\\n🏆 TOP 20 FEATURES:\")\nprint(importance_df.head(20).to_string(index=False))\n\n# Save OOF predictions\nlgb_oof_df = pd.DataFrame({\n    'id': train_ids,          # ← Use original train_df's id\n    'lgb_oof': oof_preds_price,    # ← Raw price OOF predictions\n})\nlgb_oof_df.to_csv('lgb_oof.csv', index=False)\n\nprint(f\"\\n💾 Saved OOF predictions as lgb_oof.csv\")\nprint(lgb_oof_df.head().to_string(index=False)) \n\n# Save test preds\nlgb_test_df = pd.DataFrame({\n    'id': test_ids,\n    'price': np.expm1(test_preds_log)  # convert back from log space\n})\nlgb_test_df.to_csv('lgb_test.csv', index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-13T15:42:44.702994Z","iopub.execute_input":"2025-09-13T15:42:44.703280Z","iopub.status.idle":"2025-09-13T15:44:07.636129Z","shell.execute_reply.started":"2025-09-13T15:42:44.703259Z","shell.execute_reply":"2025-09-13T15:44:07.635310Z"}},"outputs":[{"name":"stdout","text":"=== FOLD 1 ===\nTraining until validation scores don't improve for 100 rounds\n[200]\tvalid_0's rmse: 0.48825\n[400]\tvalid_0's rmse: 0.486786\n[600]\tvalid_0's rmse: 0.486066\n[800]\tvalid_0's rmse: 0.485574\n[1000]\tvalid_0's rmse: 0.485551\nEarly stopping, best iteration is:\n[937]\tvalid_0's rmse: 0.485499\n  Fold 1 RMSE: 66389.05\n=== FOLD 2 ===\nTraining until validation scores don't improve for 100 rounds\n[200]\tvalid_0's rmse: 0.4874\n[400]\tvalid_0's rmse: 0.485644\n[600]\tvalid_0's rmse: 0.485096\n[800]\tvalid_0's rmse: 0.484895\nEarly stopping, best iteration is:\n[750]\tvalid_0's rmse: 0.484856\n  Fold 2 RMSE: 70744.97\n=== FOLD 3 ===\nTraining until validation scores don't improve for 100 rounds\n[200]\tvalid_0's rmse: 0.492061\n[400]\tvalid_0's rmse: 0.490476\n[600]\tvalid_0's rmse: 0.489998\n[800]\tvalid_0's rmse: 0.489869\nEarly stopping, best iteration is:\n[849]\tvalid_0's rmse: 0.489831\n  Fold 3 RMSE: 75424.03\n=== FOLD 4 ===\nTraining until validation scores don't improve for 100 rounds\n[200]\tvalid_0's rmse: 0.493682\n[400]\tvalid_0's rmse: 0.492383\n[600]\tvalid_0's rmse: 0.491759\n[800]\tvalid_0's rmse: 0.491525\nEarly stopping, best iteration is:\n[832]\tvalid_0's rmse: 0.491478\n  Fold 4 RMSE: 77005.13\n=== FOLD 5 ===\nTraining until validation scores don't improve for 100 rounds\n[200]\tvalid_0's rmse: 0.496349\n[400]\tvalid_0's rmse: 0.494723\n[600]\tvalid_0's rmse: 0.493981\n[800]\tvalid_0's rmse: 0.493624\n[1000]\tvalid_0's rmse: 0.493522\nEarly stopping, best iteration is:\n[981]\tvalid_0's rmse: 0.493489\n  Fold 5 RMSE: 76687.06\n\n✅ FINAL CV RMSE: 73364.58\n\n🏆 TOP 20 FEATURES:\n              feature  importance\n               milage     3785.00\n log_mileage_per_year     3356.60\n           horsepower     3289.40\nbase_model_target_enc     2870.80\n          log_car_age     2487.40\n     brand_target_enc     1712.40\n          log_mileage     1527.60\n        engine_liters     1452.00\nbase_color_target_enc     1203.60\nbody_style_target_enc      564.60\nis_horsepower_missing      369.60\n base_int_color_Black      307.60\n            cylinders      289.40\nhas_accident_reported      227.40\nis_body_style_missing      198.60\n       is_clean_title      184.00\n is_cylinders_missing      164.80\n base_int_color_Beige      137.80\n       body_style_SUV      128.60\n     body_style_Sedan      126.20\n\n💾 Saved OOF predictions as lgb_oof.csv\n id  lgb_oof\n  0  6558.02\n  1  7222.26\n  2 10069.83\n  3 55883.86\n  4 69179.09\n","output_type":"stream"}],"execution_count":42},{"cell_type":"code","source":"'''# TEST PREDICTIONS using LGBM (replace price in sub with predictions)\ntest_preds_price = np.expm1(test_preds_log)\n\n# Drop old price column if it exists\nsub = sub.drop(columns=['price'], errors='ignore')\n\n# Insert predictions\nsub['price'] = test_preds_price\n\n# Save submission\nsub.to_csv('submission_lgbm_no_tuning.csv', index=False)\n\nprint(f\"\\n✅ Submission saved: submission_lgbm_no_tuning.csv\")\nprint(f\"📊 Submission shape: {sub.shape}\")\nprint(f\"📈 Sample predictions:\\n{sub.head().to_string(index=False)}\")'''","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-13T15:44:07.637058Z","iopub.execute_input":"2025-09-13T15:44:07.637382Z","iopub.status.idle":"2025-09-13T15:44:07.644056Z","shell.execute_reply.started":"2025-09-13T15:44:07.637360Z","shell.execute_reply":"2025-09-13T15:44:07.642998Z"}},"outputs":[{"execution_count":43,"output_type":"execute_result","data":{"text/plain":"'# TEST PREDICTIONS using LGBM (replace price in sub with predictions)\\ntest_preds_price = np.expm1(test_preds_log)\\n\\n# Drop old price column if it exists\\nsub = sub.drop(columns=[\\'price\\'], errors=\\'ignore\\')\\n\\n# Insert predictions\\nsub[\\'price\\'] = test_preds_price\\n\\n# Save submission\\nsub.to_csv(\\'submission_lgbm_no_tuning.csv\\', index=False)\\n\\nprint(f\"\\n✅ Submission saved: submission_lgbm_no_tuning.csv\")\\nprint(f\"📊 Submission shape: {sub.shape}\")\\nprint(f\"📈 Sample predictions:\\n{sub.head().to_string(index=False)}\")'"},"metadata":{}}],"execution_count":43},{"cell_type":"markdown","source":"### CatBoost Model\nAfter LightGBM, I wanted to test **CatBoost**, another gradient boosting method that tends to handle categorical features more naturally and often requires less tuning. Since my dataset already included several target-encoded and categorical-like features, CatBoost felt like a good candidate.\n\n#### Training setup\n* **Data prep:** Same input features as before, with stratification based on price bins to balance cheap vs high-end cars across folds.\n* **Cross-validation:** 5-fold StratifiedKFold, keeping consistency with earlier models.\n* **Parameters:**\n  * Iterations: up to 10,000 with **early stopping (patience = 100)**\n  * Learning rate: 0.05\n  * Depth: 6\n  * Loss: RMSE\n* **Evaluation:** RMSE on raw price values.\n\n#### Results across folds\n* Fold 1 RMSE: **66,399**\n* Fold 2 RMSE: **70,766**\n* Fold 3 RMSE: **75,363**\n* Fold 4 RMSE: **76,930**\n* Fold 5 RMSE: **76,629**\n* **Final CV RMSE: 73,330**\n\n#### What this means\n* **Performance:** CatBoost landed almost exactly in line with LightGBM (73,365), slightly edging it out by a margin of \\~35 RMSE. This suggests that both boosting approaches captured the same underlying signals.\n* **Stability:** Fold scores were relatively consistent, though folds 3–5 trended a bit higher, showing some sensitivity to different price ranges.\n* **Feature handling:** Even though CatBoost is designed to natively handle categorical variables, the preprocessing I had already done (encodings + engineered features) meant it didn’t have a big advantage over LightGBM.\n\n#### Why keep CatBoost in the mix\n\n* Adds **model diversity** for ensembling: while similar to LightGBM in performance, CatBoost learns slightly different splits and might capture patterns LGBM misses.\n* Works well as a complementary model when stacking, rather than a strict replacement.\n\nLike with Ridge and LightGBM, I saved the **OOF predictions (`cb_oof.csv`)** and **test predictions (`cb_test.csv`)** for use in blending later.","metadata":{}},{"cell_type":"code","source":"# Prepare data (same as before)\ntrain_modeling = train_df.drop(columns=['price'], errors='ignore')\ntest_modeling = test_df\n\nfeature_cols = [col for col in train_modeling.columns if col not in ['log_price']]\nX = train_modeling[feature_cols]\ny = train_modeling['log_price']\nX_test = test_modeling[feature_cols]\n\n# Create price bins for stratification\ntrain_modeling['price_bin'] = pd.qcut(train_modeling['log_price'], q=10, labels=False)\n\n# CV\nskf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\noof_preds_log = np.zeros(len(X))\ntest_preds_log = np.zeros(len(X_test))\nfold_scores = []\n\nfor fold, (train_idx, val_idx) in enumerate(skf.split(X, train_modeling['price_bin'])):\n    print(f\"=== FOLD {fold+1} ===\")\n    \n    X_train_fold = X.iloc[train_idx]\n    y_train_fold = y.iloc[train_idx]\n    X_val_fold = X.iloc[val_idx]\n    y_val_fold = y.iloc[val_idx]\n    \n    # CatBoost\n    model = CatBoostRegressor(\n        iterations=10000,\n        learning_rate=0.05,\n        depth=6,\n        l2_leaf_reg=3,\n        loss_function='RMSE',\n        eval_metric='RMSE',\n        random_seed=42,\n        early_stopping_rounds=100,\n        verbose=200\n    )\n    \n    model.fit(\n        X_train_fold, y_train_fold,\n        eval_set=(X_val_fold, y_val_fold),\n        use_best_model=True\n    )\n    \n    # Predict\n    val_pred_log = model.predict(X_val_fold)\n    oof_preds_log[val_idx] = val_pred_log\n    \n    # Score on raw price\n    val_pred_price = np.expm1(val_pred_log)\n    y_val_price = np.expm1(y_val_fold)\n    fold_rmse = np.sqrt(mean_squared_error(y_val_price, val_pred_price))\n    fold_scores.append(fold_rmse)\n    print(f\"  Fold {fold+1} RMSE: {fold_rmse:.2f}\")\n    \n    # Predict on test\n    test_pred_log = model.predict(X_test)\n    test_preds_log += test_pred_log / 5\n\n# Final CV score\noof_preds_price = np.expm1(oof_preds_log)\ny_train_price = np.expm1(y)\ncv_rmse = np.sqrt(mean_squared_error(y_train_price, oof_preds_price))\nprint(f\"\\n✅ CATBOOST FINAL CV RMSE: {cv_rmse:.2f}\")\n\n# Save CatBoost OOF\ncb_oof_df = pd.DataFrame({\n    'id': train_ids,\n    'cb_oof': oof_preds_price,\n})\ncb_oof_df.to_csv('cb_oof.csv', index=False)\n\nprint(f\"\\n💾 Saved CatBoost OOF predictions as cb_oof.csv\")\nprint(cb_oof_df.head().to_string(index=False))\n\n# Save test preds\ncb_test_df = pd.DataFrame({\n    'id': test_ids,\n    'price': np.expm1(test_preds_log)\n})\ncb_test_df.to_csv('cb_test.csv', index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-13T15:44:07.645636Z","iopub.execute_input":"2025-09-13T15:44:07.645929Z","iopub.status.idle":"2025-09-13T15:46:46.422355Z","shell.execute_reply.started":"2025-09-13T15:44:07.645906Z","shell.execute_reply":"2025-09-13T15:46:46.421321Z"}},"outputs":[{"name":"stdout","text":"=== FOLD 1 ===\n0:\tlearn: 0.8201811\ttest: 0.8195039\tbest: 0.8195039 (0)\ttotal: 74.6ms\tremaining: 12m 26s\n200:\tlearn: 0.4944470\ttest: 0.4927725\tbest: 0.4927725 (200)\ttotal: 3.08s\tremaining: 2m 30s\n400:\tlearn: 0.4883842\ttest: 0.4896233\tbest: 0.4896233 (400)\ttotal: 5.94s\tremaining: 2m 22s\n600:\tlearn: 0.4843131\ttest: 0.4882795\tbest: 0.4882773 (599)\ttotal: 8.8s\tremaining: 2m 17s\n800:\tlearn: 0.4811668\ttest: 0.4875554\tbest: 0.4875514 (799)\ttotal: 11.7s\tremaining: 2m 14s\n1000:\tlearn: 0.4784332\ttest: 0.4870862\tbest: 0.4870814 (999)\ttotal: 14.6s\tremaining: 2m 10s\n1200:\tlearn: 0.4759447\ttest: 0.4868100\tbest: 0.4868100 (1200)\ttotal: 17.4s\tremaining: 2m 7s\n1400:\tlearn: 0.4736979\ttest: 0.4865864\tbest: 0.4865727 (1387)\ttotal: 20.3s\tremaining: 2m 4s\n1600:\tlearn: 0.4716382\ttest: 0.4863734\tbest: 0.4863540 (1569)\ttotal: 23.1s\tremaining: 2m 1s\nStopped by overfitting detector  (100 iterations wait)\n\nbestTest = 0.4863540335\nbestIteration = 1569\n\nShrink model to first 1570 iterations.\n  Fold 1 RMSE: 66399.90\n=== FOLD 2 ===\n0:\tlearn: 0.8202669\ttest: 0.8191121\tbest: 0.8191121 (0)\ttotal: 22.3ms\tremaining: 3m 42s\n200:\tlearn: 0.4943388\ttest: 0.4921316\tbest: 0.4921316 (200)\ttotal: 2.96s\tremaining: 2m 24s\n400:\tlearn: 0.4884741\ttest: 0.4891124\tbest: 0.4891124 (400)\ttotal: 5.86s\tremaining: 2m 20s\n600:\tlearn: 0.4845416\ttest: 0.4877886\tbest: 0.4877886 (600)\ttotal: 9.24s\tremaining: 2m 24s\n800:\tlearn: 0.4813226\ttest: 0.4870086\tbest: 0.4869998 (798)\ttotal: 12.2s\tremaining: 2m 19s\n1000:\tlearn: 0.4786382\ttest: 0.4864892\tbest: 0.4864892 (1000)\ttotal: 15.1s\tremaining: 2m 15s\n1200:\tlearn: 0.4761994\ttest: 0.4861060\tbest: 0.4861048 (1199)\ttotal: 17.9s\tremaining: 2m 11s\n1400:\tlearn: 0.4739135\ttest: 0.4858009\tbest: 0.4858009 (1400)\ttotal: 20.8s\tremaining: 2m 7s\n1600:\tlearn: 0.4717775\ttest: 0.4855999\tbest: 0.4855999 (1600)\ttotal: 23.6s\tremaining: 2m 3s\n1800:\tlearn: 0.4696590\ttest: 0.4854575\tbest: 0.4854545 (1792)\ttotal: 26.5s\tremaining: 2m\n2000:\tlearn: 0.4677871\ttest: 0.4854437\tbest: 0.4854159 (1908)\ttotal: 29.4s\tremaining: 1m 57s\nStopped by overfitting detector  (100 iterations wait)\n\nbestTest = 0.4854158774\nbestIteration = 1908\n\nShrink model to first 1909 iterations.\n  Fold 2 RMSE: 70766.08\n=== FOLD 3 ===\n0:\tlearn: 0.8200807\ttest: 0.8194495\tbest: 0.8194495 (0)\ttotal: 20.9ms\tremaining: 3m 29s\n200:\tlearn: 0.4933521\ttest: 0.4964687\tbest: 0.4964687 (200)\ttotal: 2.98s\tremaining: 2m 25s\n400:\tlearn: 0.4873920\ttest: 0.4933952\tbest: 0.4933952 (400)\ttotal: 5.94s\tremaining: 2m 22s\n600:\tlearn: 0.4834984\ttest: 0.4920701\tbest: 0.4920701 (600)\ttotal: 8.83s\tremaining: 2m 18s\n800:\tlearn: 0.4803991\ttest: 0.4914313\tbest: 0.4914313 (800)\ttotal: 11.7s\tremaining: 2m 14s\n1000:\tlearn: 0.4776396\ttest: 0.4909306\tbest: 0.4909306 (1000)\ttotal: 14.7s\tremaining: 2m 12s\n1200:\tlearn: 0.4751186\ttest: 0.4906124\tbest: 0.4906091 (1198)\ttotal: 17.5s\tremaining: 2m 8s\n1400:\tlearn: 0.4728170\ttest: 0.4904605\tbest: 0.4904481 (1390)\ttotal: 20.4s\tremaining: 2m 5s\n1600:\tlearn: 0.4706667\ttest: 0.4902734\tbest: 0.4902734 (1600)\ttotal: 23.3s\tremaining: 2m 2s\n1800:\tlearn: 0.4686287\ttest: 0.4901458\tbest: 0.4901458 (1800)\ttotal: 26.3s\tremaining: 1m 59s\nStopped by overfitting detector  (100 iterations wait)\n\nbestTest = 0.4900965754\nbestIteration = 1847\n\nShrink model to first 1848 iterations.\n  Fold 3 RMSE: 75363.13\n=== FOLD 4 ===\n0:\tlearn: 0.8196197\ttest: 0.8197756\tbest: 0.8197756 (0)\ttotal: 22ms\tremaining: 3m 39s\n200:\tlearn: 0.4927701\ttest: 0.4980047\tbest: 0.4980047 (200)\ttotal: 2.95s\tremaining: 2m 23s\n400:\tlearn: 0.4869490\ttest: 0.4951941\tbest: 0.4951941 (400)\ttotal: 5.85s\tremaining: 2m 20s\n600:\tlearn: 0.4829564\ttest: 0.4939960\tbest: 0.4939960 (600)\ttotal: 8.68s\tremaining: 2m 15s\n800:\tlearn: 0.4798472\ttest: 0.4934810\tbest: 0.4934778 (794)\ttotal: 11.5s\tremaining: 2m 12s\n1000:\tlearn: 0.4771258\ttest: 0.4930552\tbest: 0.4930543 (999)\ttotal: 14.3s\tremaining: 2m 8s\n1200:\tlearn: 0.4746053\ttest: 0.4926635\tbest: 0.4926606 (1196)\ttotal: 17.2s\tremaining: 2m 6s\n1400:\tlearn: 0.4723139\ttest: 0.4924852\tbest: 0.4924789 (1395)\ttotal: 20.1s\tremaining: 2m 3s\n1600:\tlearn: 0.4701582\ttest: 0.4923104\tbest: 0.4923078 (1572)\ttotal: 22.9s\tremaining: 2m\n1800:\tlearn: 0.4681857\ttest: 0.4922210\tbest: 0.4922157 (1797)\ttotal: 25.8s\tremaining: 1m 57s\n2000:\tlearn: 0.4662534\ttest: 0.4921701\tbest: 0.4921682 (1931)\ttotal: 28.6s\tremaining: 1m 54s\n2200:\tlearn: 0.4644578\ttest: 0.4921935\tbest: 0.4921553 (2148)\ttotal: 31.4s\tremaining: 1m 51s\nStopped by overfitting detector  (100 iterations wait)\n\nbestTest = 0.4921552588\nbestIteration = 2148\n\nShrink model to first 2149 iterations.\n  Fold 4 RMSE: 76929.90\n=== FOLD 5 ===\n0:\tlearn: 0.8191585\ttest: 0.8214842\tbest: 0.8214842 (0)\ttotal: 21.1ms\tremaining: 3m 31s\n200:\tlearn: 0.4920560\ttest: 0.5006645\tbest: 0.5006645 (200)\ttotal: 3.01s\tremaining: 2m 26s\n400:\tlearn: 0.4863969\ttest: 0.4974612\tbest: 0.4974605 (399)\ttotal: 5.83s\tremaining: 2m 19s\n600:\tlearn: 0.4824795\ttest: 0.4960290\tbest: 0.4960290 (600)\ttotal: 8.7s\tremaining: 2m 16s\n800:\tlearn: 0.4794588\ttest: 0.4952953\tbest: 0.4952953 (800)\ttotal: 11.5s\tremaining: 2m 12s\n1000:\tlearn: 0.4766927\ttest: 0.4948380\tbest: 0.4948377 (998)\ttotal: 14.4s\tremaining: 2m 9s\n1200:\tlearn: 0.4741853\ttest: 0.4944351\tbest: 0.4944351 (1200)\ttotal: 17.3s\tremaining: 2m 6s\n1400:\tlearn: 0.4719363\ttest: 0.4941238\tbest: 0.4941223 (1398)\ttotal: 20.1s\tremaining: 2m 3s\n1600:\tlearn: 0.4698568\ttest: 0.4939590\tbest: 0.4939590 (1600)\ttotal: 23s\tremaining: 2m\n1800:\tlearn: 0.4677870\ttest: 0.4938272\tbest: 0.4938272 (1800)\ttotal: 25.9s\tremaining: 1m 57s\n2000:\tlearn: 0.4659422\ttest: 0.4936970\tbest: 0.4936873 (1980)\ttotal: 28.7s\tremaining: 1m 54s\n2200:\tlearn: 0.4641391\ttest: 0.4936390\tbest: 0.4936064 (2154)\ttotal: 31.6s\tremaining: 1m 51s\n2400:\tlearn: 0.4624499\ttest: 0.4935346\tbest: 0.4935293 (2388)\ttotal: 34.5s\tremaining: 1m 49s\n2600:\tlearn: 0.4607389\ttest: 0.4934655\tbest: 0.4934549 (2581)\ttotal: 37.3s\tremaining: 1m 46s\nStopped by overfitting detector  (100 iterations wait)\n\nbestTest = 0.4934549036\nbestIteration = 2581\n\nShrink model to first 2582 iterations.\n  Fold 5 RMSE: 76628.66\n\n✅ CATBOOST FINAL CV RMSE: 73330.10\n\n💾 Saved CatBoost OOF predictions as cb_oof.csv\n id   cb_oof\n  0  5879.96\n  1  7865.67\n  2  9512.51\n  3 53408.88\n  4 65371.11\n","output_type":"stream"}],"execution_count":44},{"cell_type":"code","source":"'''# TEST PREDICTIONS using CatBoost (replace price in sub with predictions)\ntest_preds_price = np.expm1(test_preds_log)\n\n# Drop old price column if it exists\nsub = sub.drop(columns=['price'], errors='ignore')\n\n# Insert predictions\nsub['price'] = test_preds_price\n\n# Save submission\nsub.to_csv('submission_cb_baseline.csv', index=False)\n\nprint(f\"\\n✅ Submission saved: submission_cb_baseline.csv\")\nprint(f\"📊 Submission shape: {sub.shape}\")\nprint(f\"📈 Sample predictions:\\n{sub.head().to_string(index=False)}\")'''","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-13T15:46:46.423416Z","iopub.execute_input":"2025-09-13T15:46:46.423714Z","iopub.status.idle":"2025-09-13T15:46:46.430324Z","shell.execute_reply.started":"2025-09-13T15:46:46.423682Z","shell.execute_reply":"2025-09-13T15:46:46.429125Z"}},"outputs":[{"execution_count":45,"output_type":"execute_result","data":{"text/plain":"'# TEST PREDICTIONS using CatBoost (replace price in sub with predictions)\\ntest_preds_price = np.expm1(test_preds_log)\\n\\n# Drop old price column if it exists\\nsub = sub.drop(columns=[\\'price\\'], errors=\\'ignore\\')\\n\\n# Insert predictions\\nsub[\\'price\\'] = test_preds_price\\n\\n# Save submission\\nsub.to_csv(\\'submission_cb_baseline.csv\\', index=False)\\n\\nprint(f\"\\n✅ Submission saved: submission_cb_baseline.csv\")\\nprint(f\"📊 Submission shape: {sub.shape}\")\\nprint(f\"📈 Sample predictions:\\n{sub.head().to_string(index=False)}\")'"},"metadata":{}}],"execution_count":45},{"cell_type":"code","source":"'''# Load predictions\nlgb_sub = pd.read_csv('/kaggle/working/submission_lgbm_no_tuning.csv')\ncb_sub = pd.read_csv('/kaggle/working/submission_cb_baseline.csv')\n\n# Blend (50-50)\nblended_price = 0.5 * lgb_sub['price'] + 0.5 * cb_sub['price']\n\n# Drop old price column if it exists\nsub = sub.drop(columns=['price'], errors='ignore')\n\n# Insert predictions\nsub['price'] = blended_price\n\nsub.to_csv('submission_blend_lgb_cb.csv', index=False)\nprint(f\"✅ Blended submission saved. Mean price: ${blended_price.mean():,.0f}\")\n\n# Output: Blended submission saved. Mean price: $36,357'''","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-13T15:46:46.431425Z","iopub.execute_input":"2025-09-13T15:46:46.431737Z","iopub.status.idle":"2025-09-13T15:46:46.450021Z","shell.execute_reply.started":"2025-09-13T15:46:46.431700Z","shell.execute_reply":"2025-09-13T15:46:46.448918Z"}},"outputs":[{"execution_count":46,"output_type":"execute_result","data":{"text/plain":"'# Load predictions\\nlgb_sub = pd.read_csv(\\'/kaggle/working/submission_lgbm_no_tuning.csv\\')\\ncb_sub = pd.read_csv(\\'/kaggle/working/submission_cb_baseline.csv\\')\\n\\n# Blend (50-50)\\nblended_price = 0.5 * lgb_sub[\\'price\\'] + 0.5 * cb_sub[\\'price\\']\\n\\n# Drop old price column if it exists\\nsub = sub.drop(columns=[\\'price\\'], errors=\\'ignore\\')\\n\\n# Insert predictions\\nsub[\\'price\\'] = blended_price\\n\\nsub.to_csv(\\'submission_blend_lgb_cb.csv\\', index=False)\\nprint(f\"✅ Blended submission saved. Mean price: ${blended_price.mean():,.0f}\")\\n\\n# Output: Blended submission saved. Mean price: $36,357'"},"metadata":{}}],"execution_count":46},{"cell_type":"markdown","source":"### First Blend (LightGBM + CatBoost)\nBefore moving into hyperparameter tuning, I tried a quick blend of the LightGBM and CatBoost models to see if their predictions complemented each other. The approach was straightforward:\n\n#### How I did it\n* **OOF predictions:** Loaded the saved out-of-fold predictions (`lgb_oof.csv` and `cb_oof.csv`).\n* **Test predictions:** Loaded both model outputs on the test set.\n* **Blending method:**\n\n  * Defined an objective function that blends the two models using a weight `w` (LightGBM gets weight `w`, CatBoost gets `1 - w`).\n  * Optimized `w` using `minimize_scalar` to directly minimize RMSE against the true training prices.\n* **Submission:** Created a blended submission file with the optimized weights.\n\n#### Results\n* **Optimal blend weight for LightGBM:** **0.288**\n* **Corresponding CatBoost weight:** **0.712**\n* **OOF RMSE:** **73,323.34**\n* **Blended test set mean price:** \\~\\$36,356\n\n#### What this means\n* The optimal blend leaned more heavily on **CatBoost (\\~71%)** than LightGBM (\\~29%), suggesting CatBoost’s predictions were slightly more reliable overall.\n* The blended RMSE (73,323) was almost identical to CatBoost’s solo score (73,330), but the fact that blending didn’t hurt performance is a good sign for later ensembling.\n* This quick test shows there’s potential value in combining models, though the real gains may come after hyperparameter tuning and possibly adding more model diversity.","metadata":{}},{"cell_type":"code","source":"lgb_oof = pd.read_csv(\"/kaggle/working/lgb_oof.csv\")['lgb_oof'].values\ncb_oof = pd.read_csv(\"/kaggle/working/cb_oof.csv\")['cb_oof'].values\n\nlgb_test = pd.read_csv(\"/kaggle/working/lgb_test.csv\")['price'].values\ncb_test = pd.read_csv(\"/kaggle/working/cb_test.csv\")['price'].values\n\n# True prices (raw)\ny_true = np.expm1(train_df['log_price'].values)  # ← Use your original train_df\n\n# Define objective\ndef blend_rmse(w):\n    blended = w * lgb_oof + (1 - w) * cb_oof\n    return np.sqrt(mean_squared_error(y_true, blended))\n\n# Optimize\nresult = minimize_scalar(blend_rmse, bounds=(0, 1), method='bounded')\nbest_w = result.x\n\nprint(f\"✅ Optimal blend weight for LightGBM: {best_w:.3f}\")\nprint(f\"✅ OOF RMSE: {result.fun:.2f}\")\n\n# Get test predictions (you have test_preds_log from both runs)\n# Blend test\nblended_test = best_w * lgb_test + (1 - best_w) * cb_test\n\n# Drop old price column if it exists\nsub = sub.drop(columns=['price'], errors='ignore')\n\n# Insert predictions\nsub['price'] = blended_test\n\n# Save submission\nsub.to_csv('submission_blend_optimized.csv', index=False)\nprint(f\"✅ Optimized blend submission saved. Mean price: ${blended_test.mean():,.0f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-13T15:49:10.153540Z","iopub.execute_input":"2025-09-13T15:49:10.153924Z","iopub.status.idle":"2025-09-13T15:49:10.638232Z","shell.execute_reply.started":"2025-09-13T15:49:10.153897Z","shell.execute_reply":"2025-09-13T15:49:10.637367Z"}},"outputs":[{"name":"stdout","text":"✅ Optimal blend weight for LightGBM: 0.288\n✅ OOF RMSE: 73323.34\n✅ Optimized blend submission saved. Mean price: $36,356\n","output_type":"stream"}],"execution_count":48},{"cell_type":"markdown","source":"## Hyperparameter Tuning\n\n\n### Why Optuna for Hyperparameter Tuning\nAfter establishing baseline models, the next step was hyperparameter tuning. Instead of manually testing values or relying on grid/random search, I used **Optuna**, and this turned out to be the smartest choice for several reasons:\n\n* **Efficiency:**\n  Optuna uses a smarter search strategy (Tree-structured Parzen Estimator, TPE) to explore the parameter space. This means it doesn’t waste trials on unlikely combinations the way grid search often does.\n\n* **Automation with Pruning:**\n  It can automatically stop underperforming trials early (via pruning), which saves time and compute while still converging towards good parameter sets.\n\n* **Flexibility:**\n  I could define realistic ranges for parameters like `learning_rate`, `num_leaves`, and `depth` and let Optuna optimize them across multiple folds of cross-validation.\n\n* **Better Exploration vs Exploitation:**\n  Unlike random search, which is blind, Optuna balances exploration of new areas of the search space with exploitation of promising regions based on past results.\n\nThis approach paid off. For instance:\n\n* **LightGBM** → Optuna found that a learning rate of `0.071`, `num_leaves=90`, and `max_depth=9` worked best.\n* **CatBoost** → It identified `learning_rate=0.099`, `depth=9`, and `l2_leaf_reg≈2.64` as optimal.\n\nIn short, Optuna made hyperparameter tuning both **systematic and efficient**, allowing me to focus on interpreting results rather than getting stuck in trial-and-error.","metadata":{}},{"cell_type":"code","source":"!pip install optuna\nimport optuna","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-13T16:08:36.899567Z","iopub.execute_input":"2025-09-13T16:08:36.899925Z","iopub.status.idle":"2025-09-13T16:08:40.838474Z","shell.execute_reply.started":"2025-09-13T16:08:36.899902Z","shell.execute_reply":"2025-09-13T16:08:40.837319Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: optuna in /usr/local/lib/python3.11/dist-packages (4.4.0)\nRequirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from optuna) (1.16.2)\nRequirement already satisfied: colorlog in /usr/local/lib/python3.11/dist-packages (from optuna) (6.9.0)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from optuna) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from optuna) (25.0)\nRequirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.11/dist-packages (from optuna) (2.0.41)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from optuna) (4.67.1)\nRequirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from optuna) (6.0.2)\nRequirement already satisfied: Mako in /usr/local/lib/python3.11/dist-packages (from alembic>=1.5.0->optuna) (1.3.10)\nRequirement already satisfied: typing-extensions>=4.12 in /usr/local/lib/python3.11/dist-packages (from alembic>=1.5.0->optuna) (4.14.0)\nRequirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.2.3)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->optuna) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->optuna) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->optuna) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->optuna) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->optuna) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->optuna) (2.4.1)\nRequirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.11/dist-packages (from Mako->alembic>=1.5.0->optuna) (3.0.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->optuna) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->optuna) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->optuna) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->optuna) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->optuna) (2024.2.0)\n","output_type":"stream"}],"execution_count":49},{"cell_type":"code","source":"def objective_lgb(trial):\n    params = {\n        'objective': 'regression',\n        'metric': 'rmse',\n        'boosting_type': 'gbdt',\n        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.1),\n        'num_leaves': trial.suggest_int('num_leaves', 20, 100),\n        'max_depth': trial.suggest_int('max_depth', 3, 10),\n        'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 100, 500),\n        'feature_fraction': trial.suggest_float('feature_fraction', 0.5, 1.0),\n        'bagging_fraction': trial.suggest_float('bagging_fraction', 0.5, 1.0),\n        'seed': 42,\n        'verbose': -1\n    }\n    \n    cv_scores = []\n    for fold, (train_idx, val_idx) in enumerate(skf.split(X, train_modeling['price_bin'])):\n        X_train_fold = X.iloc[train_idx]\n        y_train_fold = y.iloc[train_idx]\n        X_val_fold = X.iloc[val_idx]\n        y_val_fold = y.iloc[val_idx]\n        \n        train_ds = lgb.Dataset(X_train_fold, label=y_train_fold)\n        val_ds = lgb.Dataset(X_val_fold, label=y_val_fold, reference=train_ds)\n        \n        model = lgb.train(\n            params,\n            train_ds,\n            valid_sets=[val_ds],\n            num_boost_round=1000,\n            callbacks=[lgb.early_stopping(50), lgb.log_evaluation(0)]\n        )\n        \n        val_pred_log = model.predict(X_val_fold)\n        val_pred_price = np.expm1(val_pred_log)\n        y_val_price = np.expm1(y_val_fold)\n        rmse = np.sqrt(mean_squared_error(y_val_price, val_pred_price))\n        cv_scores.append(rmse)\n    \n    return np.mean(cv_scores)\n\nstudy_lgb = optuna.create_study(direction='minimize')\nstudy_lgb.optimize(objective_lgb, n_trials=50)\n\nprint(f\"✅ Best LightGBM params: {study_lgb.best_params}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-13T16:09:16.445114Z","iopub.execute_input":"2025-09-13T16:09:16.445909Z","iopub.status.idle":"2025-09-13T16:47:00.562459Z","shell.execute_reply.started":"2025-09-13T16:09:16.445870Z","shell.execute_reply":"2025-09-13T16:47:00.561450Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stderr","text":"[I 2025-09-13 16:09:16,453] A new study created in memory with name: no-name-85f44c00-1af6-4d0e-967b-775b0095c55e\n","output_type":"stream"},{"name":"stdout","text":"Training until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[621]\tvalid_0's rmse: 0.485649\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[748]\tvalid_0's rmse: 0.485217\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[917]\tvalid_0's rmse: 0.489369\nTraining until validation scores don't improve for 50 rounds\nDid not meet early stopping. Best iteration is:\n[994]\tvalid_0's rmse: 0.490933\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[783]\tvalid_0's rmse: 0.492947\n","output_type":"stream"},{"name":"stderr","text":"[I 2025-09-13 16:09:58,698] Trial 0 finished with value: 73257.58501759262 and parameters: {'learning_rate': 0.061804031750318345, 'num_leaves': 39, 'max_depth': 7, 'min_data_in_leaf': 338, 'feature_fraction': 0.638550311547829, 'bagging_fraction': 0.9057500495838889}. Best is trial 0 with value: 73257.58501759262.\n","output_type":"stream"},{"name":"stdout","text":"Training until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[658]\tvalid_0's rmse: 0.485444\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[681]\tvalid_0's rmse: 0.485104\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[568]\tvalid_0's rmse: 0.489889\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[787]\tvalid_0's rmse: 0.491274\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[562]\tvalid_0's rmse: 0.493274\n","output_type":"stream"},{"name":"stderr","text":"[I 2025-09-13 16:10:34,430] Trial 1 finished with value: 73256.71123359448 and parameters: {'learning_rate': 0.08223984389779822, 'num_leaves': 66, 'max_depth': 7, 'min_data_in_leaf': 321, 'feature_fraction': 0.6270055702941251, 'bagging_fraction': 0.5428809732125282}. Best is trial 1 with value: 73256.71123359448.\n","output_type":"stream"},{"name":"stdout","text":"Training until validation scores don't improve for 50 rounds\nDid not meet early stopping. Best iteration is:\n[999]\tvalid_0's rmse: 0.485596\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[829]\tvalid_0's rmse: 0.485398\nTraining until validation scores don't improve for 50 rounds\nDid not meet early stopping. Best iteration is:\n[994]\tvalid_0's rmse: 0.4896\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[877]\tvalid_0's rmse: 0.491473\nTraining until validation scores don't improve for 50 rounds\nDid not meet early stopping. Best iteration is:\n[997]\tvalid_0's rmse: 0.493147\n","output_type":"stream"},{"name":"stderr","text":"[I 2025-09-13 16:11:18,530] Trial 2 finished with value: 73254.56031509044 and parameters: {'learning_rate': 0.05170590761476232, 'num_leaves': 76, 'max_depth': 6, 'min_data_in_leaf': 216, 'feature_fraction': 0.7700944865068644, 'bagging_fraction': 0.5802221727045807}. Best is trial 2 with value: 73254.56031509044.\n","output_type":"stream"},{"name":"stdout","text":"Training until validation scores don't improve for 50 rounds\nDid not meet early stopping. Best iteration is:\n[1000]\tvalid_0's rmse: 0.486284\nTraining until validation scores don't improve for 50 rounds\nDid not meet early stopping. Best iteration is:\n[1000]\tvalid_0's rmse: 0.485607\nTraining until validation scores don't improve for 50 rounds\nDid not meet early stopping. Best iteration is:\n[1000]\tvalid_0's rmse: 0.490252\nTraining until validation scores don't improve for 50 rounds\nDid not meet early stopping. Best iteration is:\n[1000]\tvalid_0's rmse: 0.491681\nTraining until validation scores don't improve for 50 rounds\nDid not meet early stopping. Best iteration is:\n[1000]\tvalid_0's rmse: 0.493661\n","output_type":"stream"},{"name":"stderr","text":"[I 2025-09-13 16:12:13,156] Trial 3 finished with value: 73269.37760232514 and parameters: {'learning_rate': 0.02281971948272444, 'num_leaves': 78, 'max_depth': 7, 'min_data_in_leaf': 201, 'feature_fraction': 0.8956943742396762, 'bagging_fraction': 0.861006465013408}. Best is trial 2 with value: 73254.56031509044.\n","output_type":"stream"},{"name":"stdout","text":"Training until validation scores don't improve for 50 rounds\nDid not meet early stopping. Best iteration is:\n[999]\tvalid_0's rmse: 0.491992\nTraining until validation scores don't improve for 50 rounds\nDid not meet early stopping. Best iteration is:\n[1000]\tvalid_0's rmse: 0.491106\nTraining until validation scores don't improve for 50 rounds\nDid not meet early stopping. Best iteration is:\n[1000]\tvalid_0's rmse: 0.495525\nTraining until validation scores don't improve for 50 rounds\nDid not meet early stopping. Best iteration is:\n[999]\tvalid_0's rmse: 0.497215\nTraining until validation scores don't improve for 50 rounds\nDid not meet early stopping. Best iteration is:\n[1000]\tvalid_0's rmse: 0.499767\n","output_type":"stream"},{"name":"stderr","text":"[I 2025-09-13 16:12:44,467] Trial 4 finished with value: 73500.14559153732 and parameters: {'learning_rate': 0.019333655162813164, 'num_leaves': 59, 'max_depth': 3, 'min_data_in_leaf': 322, 'feature_fraction': 0.7371236533354212, 'bagging_fraction': 0.7149054629546198}. Best is trial 2 with value: 73254.56031509044.\n","output_type":"stream"},{"name":"stdout","text":"Training until validation scores don't improve for 50 rounds\nDid not meet early stopping. Best iteration is:\n[999]\tvalid_0's rmse: 0.486295\nTraining until validation scores don't improve for 50 rounds\nDid not meet early stopping. Best iteration is:\n[999]\tvalid_0's rmse: 0.485756\nTraining until validation scores don't improve for 50 rounds\nDid not meet early stopping. Best iteration is:\n[1000]\tvalid_0's rmse: 0.49058\nTraining until validation scores don't improve for 50 rounds\nDid not meet early stopping. Best iteration is:\n[1000]\tvalid_0's rmse: 0.492048\nTraining until validation scores don't improve for 50 rounds\nDid not meet early stopping. Best iteration is:\n[994]\tvalid_0's rmse: 0.493914\n","output_type":"stream"},{"name":"stderr","text":"[I 2025-09-13 16:13:21,594] Trial 5 finished with value: 73294.55234299257 and parameters: {'learning_rate': 0.0968773567284108, 'num_leaves': 37, 'max_depth': 4, 'min_data_in_leaf': 374, 'feature_fraction': 0.5963825679222141, 'bagging_fraction': 0.6830023774825045}. Best is trial 2 with value: 73254.56031509044.\n","output_type":"stream"},{"name":"stdout","text":"Training until validation scores don't improve for 50 rounds\nDid not meet early stopping. Best iteration is:\n[1000]\tvalid_0's rmse: 0.488047\nTraining until validation scores don't improve for 50 rounds\nDid not meet early stopping. Best iteration is:\n[1000]\tvalid_0's rmse: 0.487116\nTraining until validation scores don't improve for 50 rounds\nDid not meet early stopping. Best iteration is:\n[1000]\tvalid_0's rmse: 0.491282\nTraining until validation scores don't improve for 50 rounds\nDid not meet early stopping. Best iteration is:\n[999]\tvalid_0's rmse: 0.49366\nTraining until validation scores don't improve for 50 rounds\nDid not meet early stopping. Best iteration is:\n[997]\tvalid_0's rmse: 0.495687\n","output_type":"stream"},{"name":"stderr","text":"[I 2025-09-13 16:13:51,924] Trial 6 finished with value: 73344.57616986765 and parameters: {'learning_rate': 0.07467840633732203, 'num_leaves': 73, 'max_depth': 3, 'min_data_in_leaf': 153, 'feature_fraction': 0.979387597070885, 'bagging_fraction': 0.7845485767800662}. Best is trial 2 with value: 73254.56031509044.\n","output_type":"stream"},{"name":"stdout","text":"Training until validation scores don't improve for 50 rounds\nDid not meet early stopping. Best iteration is:\n[992]\tvalid_0's rmse: 0.485691\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[945]\tvalid_0's rmse: 0.485171\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[767]\tvalid_0's rmse: 0.490018\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[859]\tvalid_0's rmse: 0.491427\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[843]\tvalid_0's rmse: 0.493515\n","output_type":"stream"},{"name":"stderr","text":"[I 2025-09-13 16:14:34,845] Trial 7 finished with value: 73262.53122030121 and parameters: {'learning_rate': 0.06120419963517732, 'num_leaves': 95, 'max_depth': 7, 'min_data_in_leaf': 489, 'feature_fraction': 0.7627474381299206, 'bagging_fraction': 0.8412297771613213}. Best is trial 2 with value: 73254.56031509044.\n","output_type":"stream"},{"name":"stdout","text":"Training until validation scores don't improve for 50 rounds\nDid not meet early stopping. Best iteration is:\n[1000]\tvalid_0's rmse: 0.486928\nTraining until validation scores don't improve for 50 rounds\nDid not meet early stopping. Best iteration is:\n[1000]\tvalid_0's rmse: 0.485945\nTraining until validation scores don't improve for 50 rounds\nDid not meet early stopping. Best iteration is:\n[999]\tvalid_0's rmse: 0.49086\nTraining until validation scores don't improve for 50 rounds\nDid not meet early stopping. Best iteration is:\n[974]\tvalid_0's rmse: 0.492649\nTraining until validation scores don't improve for 50 rounds\nDid not meet early stopping. Best iteration is:\n[1000]\tvalid_0's rmse: 0.494633\n","output_type":"stream"},{"name":"stderr","text":"[I 2025-09-13 16:15:19,078] Trial 8 finished with value: 73315.56285846992 and parameters: {'learning_rate': 0.028008952247833764, 'num_leaves': 22, 'max_depth': 8, 'min_data_in_leaf': 378, 'feature_fraction': 0.9500272919118333, 'bagging_fraction': 0.833297459654443}. Best is trial 2 with value: 73254.56031509044.\n","output_type":"stream"},{"name":"stdout","text":"Training until validation scores don't improve for 50 rounds\nDid not meet early stopping. Best iteration is:\n[1000]\tvalid_0's rmse: 0.485517\nTraining until validation scores don't improve for 50 rounds\nDid not meet early stopping. Best iteration is:\n[1000]\tvalid_0's rmse: 0.485212\nTraining until validation scores don't improve for 50 rounds\nDid not meet early stopping. Best iteration is:\n[1000]\tvalid_0's rmse: 0.489648\nTraining until validation scores don't improve for 50 rounds\nDid not meet early stopping. Best iteration is:\n[1000]\tvalid_0's rmse: 0.491448\nTraining until validation scores don't improve for 50 rounds\nDid not meet early stopping. Best iteration is:\n[1000]\tvalid_0's rmse: 0.493097\n","output_type":"stream"},{"name":"stderr","text":"[I 2025-09-13 16:16:26,266] Trial 9 finished with value: 73252.08458933093 and parameters: {'learning_rate': 0.018284816016376104, 'num_leaves': 66, 'max_depth': 9, 'min_data_in_leaf': 124, 'feature_fraction': 0.7696619422714005, 'bagging_fraction': 0.6146313256992928}. Best is trial 9 with value: 73252.08458933093.\n","output_type":"stream"},{"name":"stdout","text":"Training until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[603]\tvalid_0's rmse: 0.484703\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[472]\tvalid_0's rmse: 0.484823\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[586]\tvalid_0's rmse: 0.48893\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[435]\tvalid_0's rmse: 0.490508\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[513]\tvalid_0's rmse: 0.492321\n","output_type":"stream"},{"name":"stderr","text":"[I 2025-09-13 16:17:11,928] Trial 10 finished with value: 73228.5959738309 and parameters: {'learning_rate': 0.040051578647902986, 'num_leaves': 100, 'max_depth': 10, 'min_data_in_leaf': 105, 'feature_fraction': 0.5031945942918024, 'bagging_fraction': 0.9983245770793863}. Best is trial 10 with value: 73228.5959738309.\n","output_type":"stream"},{"name":"stdout","text":"Training until validation scores don't improve for 50 rounds\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[533]\tvalid_0's rmse: 0.484816\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[731]\tvalid_0's rmse: 0.488993\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[437]\tvalid_0's rmse: 0.49065\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[553]\tvalid_0's rmse: 0.492392\n","output_type":"stream"},{"name":"stderr","text":"[I 2025-09-13 16:18:02,083] Trial 11 finished with value: 73228.08828886102 and parameters: {'learning_rate': 0.03966809709908576, 'num_leaves': 94, 'max_depth': 10, 'min_data_in_leaf': 102, 'feature_fraction': 0.5109724715068077, 'bagging_fraction': 0.9953941643052572}. Best is trial 11 with value: 73228.08828886102.\n","output_type":"stream"},{"name":"stdout","text":"Training until validation scores don't improve for 50 rounds\nDid not meet early stopping. Best iteration is:\n[956]\tvalid_0's rmse: 0.484669\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[680]\tvalid_0's rmse: 0.484611\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[825]\tvalid_0's rmse: 0.488916\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[736]\tvalid_0's rmse: 0.490686\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[781]\tvalid_0's rmse: 0.492573\n","output_type":"stream"},{"name":"stderr","text":"[I 2025-09-13 16:19:03,243] Trial 12 finished with value: 73235.65035664459 and parameters: {'learning_rate': 0.03845715631118373, 'num_leaves': 98, 'max_depth': 10, 'min_data_in_leaf': 228, 'feature_fraction': 0.5002698348246012, 'bagging_fraction': 0.9981501710351915}. Best is trial 11 with value: 73228.08828886102.\n","output_type":"stream"},{"name":"stdout","text":"Training until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[554]\tvalid_0's rmse: 0.484845\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[561]\tvalid_0's rmse: 0.484746\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[624]\tvalid_0's rmse: 0.488745\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[438]\tvalid_0's rmse: 0.49091\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[668]\tvalid_0's rmse: 0.492467\n","output_type":"stream"},{"name":"stderr","text":"[I 2025-09-13 16:19:50,571] Trial 13 finished with value: 73229.47500741768 and parameters: {'learning_rate': 0.042058872289917, 'num_leaves': 90, 'max_depth': 10, 'min_data_in_leaf': 119, 'feature_fraction': 0.501245055868804, 'bagging_fraction': 0.977328784585337}. Best is trial 11 with value: 73228.08828886102.\n","output_type":"stream"},{"name":"stdout","text":"Training until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[899]\tvalid_0's rmse: 0.484668\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[668]\tvalid_0's rmse: 0.484689\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[807]\tvalid_0's rmse: 0.488827\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[906]\tvalid_0's rmse: 0.490744\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[764]\tvalid_0's rmse: 0.49256\n","output_type":"stream"},{"name":"stderr","text":"[I 2025-09-13 16:20:47,380] Trial 14 finished with value: 73226.0874561334 and parameters: {'learning_rate': 0.03820051777756713, 'num_leaves': 87, 'max_depth': 9, 'min_data_in_leaf': 171, 'feature_fraction': 0.5744877767773984, 'bagging_fraction': 0.9337161943172059}. Best is trial 14 with value: 73226.0874561334.\n","output_type":"stream"},{"name":"stdout","text":"Training until validation scores don't improve for 50 rounds\nDid not meet early stopping. Best iteration is:\n[999]\tvalid_0's rmse: 0.486577\nTraining until validation scores don't improve for 50 rounds\nDid not meet early stopping. Best iteration is:\n[1000]\tvalid_0's rmse: 0.485932\nTraining until validation scores don't improve for 50 rounds\nDid not meet early stopping. Best iteration is:\n[1000]\tvalid_0's rmse: 0.490309\nTraining until validation scores don't improve for 50 rounds\nDid not meet early stopping. Best iteration is:\n[1000]\tvalid_0's rmse: 0.491779\nTraining until validation scores don't improve for 50 rounds\nDid not meet early stopping. Best iteration is:\n[1000]\tvalid_0's rmse: 0.494012\n","output_type":"stream"},{"name":"stderr","text":"[I 2025-09-13 16:22:03,285] Trial 15 finished with value: 73315.0309996092 and parameters: {'learning_rate': 0.011022660097919863, 'num_leaves': 85, 'max_depth': 9, 'min_data_in_leaf': 256, 'feature_fraction': 0.579631466463722, 'bagging_fraction': 0.9242495910122859}. Best is trial 14 with value: 73226.0874561334.\n","output_type":"stream"},{"name":"stdout","text":"Training until validation scores don't improve for 50 rounds\nDid not meet early stopping. Best iteration is:\n[997]\tvalid_0's rmse: 0.485034\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[878]\tvalid_0's rmse: 0.485182\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[803]\tvalid_0's rmse: 0.489303\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[723]\tvalid_0's rmse: 0.491073\nTraining until validation scores don't improve for 50 rounds\nDid not meet early stopping. Best iteration is:\n[962]\tvalid_0's rmse: 0.492687\n","output_type":"stream"},{"name":"stderr","text":"[I 2025-09-13 16:23:00,909] Trial 16 finished with value: 73233.15510674063 and parameters: {'learning_rate': 0.03216652231770547, 'num_leaves': 84, 'max_depth': 9, 'min_data_in_leaf': 171, 'feature_fraction': 0.7012986103473592, 'bagging_fraction': 0.9226943154832151}. Best is trial 14 with value: 73226.0874561334.\n","output_type":"stream"},{"name":"stdout","text":"Training until validation scores don't improve for 50 rounds\nDid not meet early stopping. Best iteration is:\n[998]\tvalid_0's rmse: 0.486009\nTraining until validation scores don't improve for 50 rounds\nDid not meet early stopping. Best iteration is:\n[998]\tvalid_0's rmse: 0.485515\nTraining until validation scores don't improve for 50 rounds\nDid not meet early stopping. Best iteration is:\n[1000]\tvalid_0's rmse: 0.489892\nTraining until validation scores don't improve for 50 rounds\nDid not meet early stopping. Best iteration is:\n[999]\tvalid_0's rmse: 0.491888\nTraining until validation scores don't improve for 50 rounds\nDid not meet early stopping. Best iteration is:\n[996]\tvalid_0's rmse: 0.493662\n","output_type":"stream"},{"name":"stderr","text":"[I 2025-09-13 16:23:46,463] Trial 17 finished with value: 73287.9272649138 and parameters: {'learning_rate': 0.05085452834300799, 'num_leaves': 51, 'max_depth': 5, 'min_data_in_leaf': 265, 'feature_fraction': 0.5625548783147043, 'bagging_fraction': 0.7849601291083526}. Best is trial 14 with value: 73226.0874561334.\n","output_type":"stream"},{"name":"stdout","text":"Training until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[457]\tvalid_0's rmse: 0.485099\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[508]\tvalid_0's rmse: 0.484817\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[511]\tvalid_0's rmse: 0.489401\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[344]\tvalid_0's rmse: 0.49134\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[557]\tvalid_0's rmse: 0.492745\n","output_type":"stream"},{"name":"stderr","text":"[I 2025-09-13 16:24:16,978] Trial 18 finished with value: 73234.2156097976 and parameters: {'learning_rate': 0.06928008025137676, 'num_leaves': 88, 'max_depth': 8, 'min_data_in_leaf': 171, 'feature_fraction': 0.6656433127753325, 'bagging_fraction': 0.9461830845396695}. Best is trial 14 with value: 73226.0874561334.\n","output_type":"stream"},{"name":"stdout","text":"Training until validation scores don't improve for 50 rounds\nDid not meet early stopping. Best iteration is:\n[1000]\tvalid_0's rmse: 0.485262\nTraining until validation scores don't improve for 50 rounds\nDid not meet early stopping. Best iteration is:\n[999]\tvalid_0's rmse: 0.485161\nTraining until validation scores don't improve for 50 rounds\nDid not meet early stopping. Best iteration is:\n[988]\tvalid_0's rmse: 0.489546\nTraining until validation scores don't improve for 50 rounds\nDid not meet early stopping. Best iteration is:\n[1000]\tvalid_0's rmse: 0.490969\nTraining until validation scores don't improve for 50 rounds\nDid not meet early stopping. Best iteration is:\n[1000]\tvalid_0's rmse: 0.49317\n","output_type":"stream"},{"name":"stderr","text":"[I 2025-09-13 16:25:13,397] Trial 19 finished with value: 73259.51965189324 and parameters: {'learning_rate': 0.04611485524532306, 'num_leaves': 79, 'max_depth': 8, 'min_data_in_leaf': 476, 'feature_fraction': 0.5713852402406377, 'bagging_fraction': 0.8800721194333545}. Best is trial 14 with value: 73226.0874561334.\n","output_type":"stream"},{"name":"stdout","text":"Training until validation scores don't improve for 50 rounds\nDid not meet early stopping. Best iteration is:\n[967]\tvalid_0's rmse: 0.485422\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[797]\tvalid_0's rmse: 0.485016\nTraining until validation scores don't improve for 50 rounds\nDid not meet early stopping. Best iteration is:\n[996]\tvalid_0's rmse: 0.489469\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[697]\tvalid_0's rmse: 0.491512\nTraining until validation scores don't improve for 50 rounds\nDid not meet early stopping. Best iteration is:\n[1000]\tvalid_0's rmse: 0.493158\n","output_type":"stream"},{"name":"stderr","text":"[I 2025-09-13 16:26:05,030] Trial 20 finished with value: 73246.55838186954 and parameters: {'learning_rate': 0.03237221788531628, 'num_leaves': 51, 'max_depth': 9, 'min_data_in_leaf': 147, 'feature_fraction': 0.8338462764548993, 'bagging_fraction': 0.6626734618997395}. Best is trial 14 with value: 73226.0874561334.\n","output_type":"stream"},{"name":"stdout","text":"Training until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[549]\tvalid_0's rmse: 0.484944\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[602]\tvalid_0's rmse: 0.484833\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[741]\tvalid_0's rmse: 0.4887\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[484]\tvalid_0's rmse: 0.490509\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[597]\tvalid_0's rmse: 0.492377\n","output_type":"stream"},{"name":"stderr","text":"[I 2025-09-13 16:26:55,036] Trial 21 finished with value: 73220.53001053685 and parameters: {'learning_rate': 0.03752700102510505, 'num_leaves': 100, 'max_depth': 10, 'min_data_in_leaf': 101, 'feature_fraction': 0.5303153417893274, 'bagging_fraction': 0.9970211800044321}. Best is trial 21 with value: 73220.53001053685.\n","output_type":"stream"},{"name":"stdout","text":"Training until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[729]\tvalid_0's rmse: 0.484709\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[657]\tvalid_0's rmse: 0.484971\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[608]\tvalid_0's rmse: 0.48903\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[549]\tvalid_0's rmse: 0.490714\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[656]\tvalid_0's rmse: 0.492577\n","output_type":"stream"},{"name":"stderr","text":"[I 2025-09-13 16:27:47,560] Trial 22 finished with value: 73231.93550301711 and parameters: {'learning_rate': 0.03598099547899555, 'num_leaves': 93, 'max_depth': 10, 'min_data_in_leaf': 103, 'feature_fraction': 0.5413649895541042, 'bagging_fraction': 0.9476100137322002}. Best is trial 21 with value: 73220.53001053685.\n","output_type":"stream"},{"name":"stdout","text":"Training until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[907]\tvalid_0's rmse: 0.484816\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[589]\tvalid_0's rmse: 0.484529\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[695]\tvalid_0's rmse: 0.488937\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[648]\tvalid_0's rmse: 0.490802\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[615]\tvalid_0's rmse: 0.492554\n","output_type":"stream"},{"name":"stderr","text":"[I 2025-09-13 16:28:40,764] Trial 23 finished with value: 73220.07660656604 and parameters: {'learning_rate': 0.04568941174611209, 'num_leaves': 100, 'max_depth': 10, 'min_data_in_leaf': 193, 'feature_fraction': 0.5392164524685872, 'bagging_fraction': 0.9530198255416439}. Best is trial 23 with value: 73220.07660656604.\n","output_type":"stream"},{"name":"stdout","text":"Training until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[570]\tvalid_0's rmse: 0.48482\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[549]\tvalid_0's rmse: 0.484722\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[566]\tvalid_0's rmse: 0.488951\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[524]\tvalid_0's rmse: 0.49095\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[480]\tvalid_0's rmse: 0.49291\n","output_type":"stream"},{"name":"stderr","text":"[I 2025-09-13 16:29:19,177] Trial 24 finished with value: 73235.19679528836 and parameters: {'learning_rate': 0.059208213688091815, 'num_leaves': 100, 'max_depth': 9, 'min_data_in_leaf': 193, 'feature_fraction': 0.6160050957559502, 'bagging_fraction': 0.8815859461899773}. Best is trial 23 with value: 73220.07660656604.\n","output_type":"stream"},{"name":"stdout","text":"Training until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[794]\tvalid_0's rmse: 0.485389\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[795]\tvalid_0's rmse: 0.484934\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[844]\tvalid_0's rmse: 0.489449\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[717]\tvalid_0's rmse: 0.491013\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[799]\tvalid_0's rmse: 0.493108\n","output_type":"stream"},{"name":"stderr","text":"[I 2025-09-13 16:30:05,780] Trial 25 finished with value: 73251.67650044238 and parameters: {'learning_rate': 0.045517967132993435, 'num_leaves': 85, 'max_depth': 8, 'min_data_in_leaf': 267, 'feature_fraction': 0.6798299368160355, 'bagging_fraction': 0.8055968859565021}. Best is trial 23 with value: 73220.07660656604.\n","output_type":"stream"},{"name":"stdout","text":"Training until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[892]\tvalid_0's rmse: 0.484763\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[911]\tvalid_0's rmse: 0.484768\nTraining until validation scores don't improve for 50 rounds\nDid not meet early stopping. Best iteration is:\n[973]\tvalid_0's rmse: 0.488538\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[786]\tvalid_0's rmse: 0.490858\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[883]\tvalid_0's rmse: 0.492423\n","output_type":"stream"},{"name":"stderr","text":"[I 2025-09-13 16:31:13,956] Trial 26 finished with value: 73225.68938507515 and parameters: {'learning_rate': 0.025444639530813835, 'num_leaves': 91, 'max_depth': 10, 'min_data_in_leaf': 147, 'feature_fraction': 0.5560874351315069, 'bagging_fraction': 0.961693342865702}. Best is trial 23 with value: 73220.07660656604.\n","output_type":"stream"},{"name":"stdout","text":"Training until validation scores don't improve for 50 rounds\nDid not meet early stopping. Best iteration is:\n[984]\tvalid_0's rmse: 0.484732\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[789]\tvalid_0's rmse: 0.484563\nTraining until validation scores don't improve for 50 rounds\nDid not meet early stopping. Best iteration is:\n[994]\tvalid_0's rmse: 0.488478\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[851]\tvalid_0's rmse: 0.490647\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[747]\tvalid_0's rmse: 0.492529\n","output_type":"stream"},{"name":"stderr","text":"[I 2025-09-13 16:32:21,000] Trial 27 finished with value: 73230.30468114864 and parameters: {'learning_rate': 0.026874133407206326, 'num_leaves': 93, 'max_depth': 10, 'min_data_in_leaf': 141, 'feature_fraction': 0.5479324455440545, 'bagging_fraction': 0.9711144728430562}. Best is trial 23 with value: 73220.07660656604.\n","output_type":"stream"},{"name":"stdout","text":"Training until validation scores don't improve for 50 rounds\nDid not meet early stopping. Best iteration is:\n[1000]\tvalid_0's rmse: 0.48753\nTraining until validation scores don't improve for 50 rounds\nDid not meet early stopping. Best iteration is:\n[1000]\tvalid_0's rmse: 0.487\nTraining until validation scores don't improve for 50 rounds\nDid not meet early stopping. Best iteration is:\n[1000]\tvalid_0's rmse: 0.491426\nTraining until validation scores don't improve for 50 rounds\nDid not meet early stopping. Best iteration is:\n[1000]\tvalid_0's rmse: 0.493007\nTraining until validation scores don't improve for 50 rounds\nDid not meet early stopping. Best iteration is:\n[1000]\tvalid_0's rmse: 0.49544\n","output_type":"stream"},{"name":"stderr","text":"[I 2025-09-13 16:33:19,900] Trial 28 finished with value: 73362.13410247216 and parameters: {'learning_rate': 0.013458448699661892, 'num_leaves': 100, 'max_depth': 6, 'min_data_in_leaf': 231, 'feature_fraction': 0.5442287623382822, 'bagging_fraction': 0.9009090570064853}. Best is trial 23 with value: 73220.07660656604.\n","output_type":"stream"},{"name":"stdout","text":"Training until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[496]\tvalid_0's rmse: 0.48511\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[529]\tvalid_0's rmse: 0.484849\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[498]\tvalid_0's rmse: 0.48911\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[407]\tvalid_0's rmse: 0.490945\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[586]\tvalid_0's rmse: 0.492417\n","output_type":"stream"},{"name":"stderr","text":"[I 2025-09-13 16:33:57,356] Trial 29 finished with value: 73235.0737147856 and parameters: {'learning_rate': 0.05201394036642818, 'num_leaves': 81, 'max_depth': 10, 'min_data_in_leaf': 188, 'feature_fraction': 0.6353337338748678, 'bagging_fraction': 0.894819304625744}. Best is trial 23 with value: 73220.07660656604.\n","output_type":"stream"},{"name":"stdout","text":"Training until validation scores don't improve for 50 rounds\nDid not meet early stopping. Best iteration is:\n[986]\tvalid_0's rmse: 0.485159\nTraining until validation scores don't improve for 50 rounds\nDid not meet early stopping. Best iteration is:\n[1000]\tvalid_0's rmse: 0.484783\nTraining until validation scores don't improve for 50 rounds\nDid not meet early stopping. Best iteration is:\n[1000]\tvalid_0's rmse: 0.489024\nTraining until validation scores don't improve for 50 rounds\nDid not meet early stopping. Best iteration is:\n[965]\tvalid_0's rmse: 0.491112\nTraining until validation scores don't improve for 50 rounds\nDid not meet early stopping. Best iteration is:\n[977]\tvalid_0's rmse: 0.492512\n","output_type":"stream"},{"name":"stderr","text":"[I 2025-09-13 16:34:57,321] Trial 30 finished with value: 73235.72012452755 and parameters: {'learning_rate': 0.028213745175487553, 'num_leaves': 70, 'max_depth': 8, 'min_data_in_leaf': 137, 'feature_fraction': 0.6590077052333123, 'bagging_fraction': 0.7504010002338702}. Best is trial 23 with value: 73220.07660656604.\n","output_type":"stream"},{"name":"stdout","text":"Training until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[768]\tvalid_0's rmse: 0.484769\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[755]\tvalid_0's rmse: 0.484666\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[624]\tvalid_0's rmse: 0.489177\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[579]\tvalid_0's rmse: 0.491057\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[661]\tvalid_0's rmse: 0.492484\n","output_type":"stream"},{"name":"stderr","text":"[I 2025-09-13 16:35:44,703] Trial 31 finished with value: 73221.03953342988 and parameters: {'learning_rate': 0.04692253884556351, 'num_leaves': 88, 'max_depth': 9, 'min_data_in_leaf': 169, 'feature_fraction': 0.6009653787694381, 'bagging_fraction': 0.9332785474557186}. Best is trial 23 with value: 73220.07660656604.\n","output_type":"stream"},{"name":"stdout","text":"Training until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[576]\tvalid_0's rmse: 0.485659\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[758]\tvalid_0's rmse: 0.484547\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[689]\tvalid_0's rmse: 0.4894\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[910]\tvalid_0's rmse: 0.490858\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[745]\tvalid_0's rmse: 0.492832\n","output_type":"stream"},{"name":"stderr","text":"[I 2025-09-13 16:36:33,805] Trial 32 finished with value: 73244.15794595359 and parameters: {'learning_rate': 0.046394505419587215, 'num_leaves': 92, 'max_depth': 9, 'min_data_in_leaf': 289, 'feature_fraction': 0.6083175646768513, 'bagging_fraction': 0.9594128030972982}. Best is trial 23 with value: 73220.07660656604.\n","output_type":"stream"},{"name":"stdout","text":"Training until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[451]\tvalid_0's rmse: 0.485039\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[353]\tvalid_0's rmse: 0.484941\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[369]\tvalid_0's rmse: 0.489086\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[301]\tvalid_0's rmse: 0.490681\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[414]\tvalid_0's rmse: 0.49239\n","output_type":"stream"},{"name":"stderr","text":"[I 2025-09-13 16:37:05,161] Trial 33 finished with value: 73222.75748867227 and parameters: {'learning_rate': 0.06811062429443786, 'num_leaves': 96, 'max_depth': 10, 'min_data_in_leaf': 161, 'feature_fraction': 0.5310729018534577, 'bagging_fraction': 0.510138619944901}. Best is trial 23 with value: 73220.07660656604.\n","output_type":"stream"},{"name":"stdout","text":"Training until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[624]\tvalid_0's rmse: 0.484931\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[441]\tvalid_0's rmse: 0.48489\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[410]\tvalid_0's rmse: 0.488867\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[394]\tvalid_0's rmse: 0.491107\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[414]\tvalid_0's rmse: 0.492571\n","output_type":"stream"},{"name":"stderr","text":"[I 2025-09-13 16:37:40,914] Trial 34 finished with value: 73231.97713812735 and parameters: {'learning_rate': 0.06788528415119158, 'num_leaves': 97, 'max_depth': 10, 'min_data_in_leaf': 207, 'feature_fraction': 0.541055358744051, 'bagging_fraction': 0.5215840491566751}. Best is trial 23 with value: 73220.07660656604.\n","output_type":"stream"},{"name":"stdout","text":"Training until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[688]\tvalid_0's rmse: 0.485251\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[417]\tvalid_0's rmse: 0.48519\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[515]\tvalid_0's rmse: 0.489463\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[447]\tvalid_0's rmse: 0.491185\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[460]\tvalid_0's rmse: 0.492664\n","output_type":"stream"},{"name":"stderr","text":"[I 2025-09-13 16:38:16,047] Trial 35 finished with value: 73220.63607968068 and parameters: {'learning_rate': 0.05701866884725364, 'num_leaves': 82, 'max_depth': 9, 'min_data_in_leaf': 173, 'feature_fraction': 0.7153179659341345, 'bagging_fraction': 0.5531259238934643}. Best is trial 23 with value: 73220.07660656604.\n","output_type":"stream"},{"name":"stdout","text":"Training until validation scores don't improve for 50 rounds\nDid not meet early stopping. Best iteration is:\n[1000]\tvalid_0's rmse: 0.485409\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[739]\tvalid_0's rmse: 0.485042\nTraining until validation scores don't improve for 50 rounds\nDid not meet early stopping. Best iteration is:\n[1000]\tvalid_0's rmse: 0.489672\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[823]\tvalid_0's rmse: 0.491322\nTraining until validation scores don't improve for 50 rounds\nDid not meet early stopping. Best iteration is:\n[993]\tvalid_0's rmse: 0.493268\n","output_type":"stream"},{"name":"stderr","text":"[I 2025-09-13 16:38:57,049] Trial 36 finished with value: 73248.90563169634 and parameters: {'learning_rate': 0.05656120160830032, 'num_leaves': 74, 'max_depth': 6, 'min_data_in_leaf': 243, 'feature_fraction': 0.7225622950090224, 'bagging_fraction': 0.5921274191978139}. Best is trial 23 with value: 73220.07660656604.\n","output_type":"stream"},{"name":"stdout","text":"Training until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[662]\tvalid_0's rmse: 0.485263\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[423]\tvalid_0's rmse: 0.485524\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[457]\tvalid_0's rmse: 0.490149\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[467]\tvalid_0's rmse: 0.491676\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[408]\tvalid_0's rmse: 0.493075\n","output_type":"stream"},{"name":"stderr","text":"[I 2025-09-13 16:39:26,968] Trial 37 finished with value: 73249.55814050601 and parameters: {'learning_rate': 0.08835156471577253, 'num_leaves': 81, 'max_depth': 9, 'min_data_in_leaf': 354, 'feature_fraction': 0.8350895178128164, 'bagging_fraction': 0.6409444779425636}. Best is trial 23 with value: 73220.07660656604.\n","output_type":"stream"},{"name":"stdout","text":"Training until validation scores don't improve for 50 rounds\nDid not meet early stopping. Best iteration is:\n[1000]\tvalid_0's rmse: 0.484955\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[535]\tvalid_0's rmse: 0.485187\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[719]\tvalid_0's rmse: 0.489185\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[760]\tvalid_0's rmse: 0.491143\nTraining until validation scores don't improve for 50 rounds\nDid not meet early stopping. Best iteration is:\n[981]\tvalid_0's rmse: 0.492716\n","output_type":"stream"},{"name":"stderr","text":"[I 2025-09-13 16:40:10,959] Trial 38 finished with value: 73238.55621415963 and parameters: {'learning_rate': 0.053097486233304715, 'num_leaves': 63, 'max_depth': 7, 'min_data_in_leaf': 192, 'feature_fraction': 0.6434736320908951, 'bagging_fraction': 0.7219346919513288}. Best is trial 23 with value: 73220.07660656604.\n","output_type":"stream"},{"name":"stdout","text":"Training until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[655]\tvalid_0's rmse: 0.485878\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[566]\tvalid_0's rmse: 0.485357\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[460]\tvalid_0's rmse: 0.48992\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[840]\tvalid_0's rmse: 0.491153\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[815]\tvalid_0's rmse: 0.493095\n","output_type":"stream"},{"name":"stderr","text":"[I 2025-09-13 16:40:46,766] Trial 39 finished with value: 73255.73307911304 and parameters: {'learning_rate': 0.07630779950732902, 'num_leaves': 56, 'max_depth': 8, 'min_data_in_leaf': 449, 'feature_fraction': 0.8246194210145504, 'bagging_fraction': 0.5488358077490271}. Best is trial 23 with value: 73220.07660656604.\n","output_type":"stream"},{"name":"stdout","text":"Training until validation scores don't improve for 50 rounds\nDid not meet early stopping. Best iteration is:\n[997]\tvalid_0's rmse: 0.48502\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[656]\tvalid_0's rmse: 0.485104\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[846]\tvalid_0's rmse: 0.489322\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[877]\tvalid_0's rmse: 0.49129\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[893]\tvalid_0's rmse: 0.492701\n","output_type":"stream"},{"name":"stderr","text":"[I 2025-09-13 16:41:31,581] Trial 40 finished with value: 73251.06247473738 and parameters: {'learning_rate': 0.06395477893670105, 'num_leaves': 35, 'max_depth': 7, 'min_data_in_leaf': 292, 'feature_fraction': 0.5959982053364554, 'bagging_fraction': 0.8487358913037993}. Best is trial 23 with value: 73220.07660656604.\n","output_type":"stream"},{"name":"stdout","text":"Training until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[649]\tvalid_0's rmse: 0.485076\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[417]\tvalid_0's rmse: 0.485091\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[523]\tvalid_0's rmse: 0.489066\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[412]\tvalid_0's rmse: 0.490761\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[479]\tvalid_0's rmse: 0.4929\n","output_type":"stream"},{"name":"stderr","text":"[I 2025-09-13 16:42:09,208] Trial 41 finished with value: 73229.6128956045 and parameters: {'learning_rate': 0.06409068887054951, 'num_leaves': 96, 'max_depth': 9, 'min_data_in_leaf': 165, 'feature_fraction': 0.5259505171921558, 'bagging_fraction': 0.5056687399020322}. Best is trial 23 with value: 73220.07660656604.\n","output_type":"stream"},{"name":"stdout","text":"Training until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[507]\tvalid_0's rmse: 0.485547\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[539]\tvalid_0's rmse: 0.485249\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[376]\tvalid_0's rmse: 0.489689\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[371]\tvalid_0's rmse: 0.491542\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[391]\tvalid_0's rmse: 0.49278\n","output_type":"stream"},{"name":"stderr","text":"[I 2025-09-13 16:42:43,553] Trial 42 finished with value: 73224.90240168755 and parameters: {'learning_rate': 0.05690781633372191, 'num_leaves': 88, 'max_depth': 10, 'min_data_in_leaf': 130, 'feature_fraction': 0.7897081159225623, 'bagging_fraction': 0.5541306282195093}. Best is trial 23 with value: 73220.07660656604.\n","output_type":"stream"},{"name":"stdout","text":"Training until validation scores don't improve for 50 rounds\nDid not meet early stopping. Best iteration is:\n[997]\tvalid_0's rmse: 0.486332\nTraining until validation scores don't improve for 50 rounds\nDid not meet early stopping. Best iteration is:\n[998]\tvalid_0's rmse: 0.485686\nTraining until validation scores don't improve for 50 rounds\nDid not meet early stopping. Best iteration is:\n[999]\tvalid_0's rmse: 0.490025\nTraining until validation scores don't improve for 50 rounds\nDid not meet early stopping. Best iteration is:\n[996]\tvalid_0's rmse: 0.491801\nTraining until validation scores don't improve for 50 rounds\nDid not meet early stopping. Best iteration is:\n[999]\tvalid_0's rmse: 0.493751\n","output_type":"stream"},{"name":"stderr","text":"[I 2025-09-13 16:43:27,511] Trial 43 finished with value: 73287.65049578722 and parameters: {'learning_rate': 0.0483299451201603, 'num_leaves': 95, 'max_depth': 5, 'min_data_in_leaf': 216, 'feature_fraction': 0.595803434618973, 'bagging_fraction': 0.5890519388816112}. Best is trial 23 with value: 73220.07660656604.\n","output_type":"stream"},{"name":"stdout","text":"Training until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[487]\tvalid_0's rmse: 0.48527\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[314]\tvalid_0's rmse: 0.485094\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[327]\tvalid_0's rmse: 0.489211\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[326]\tvalid_0's rmse: 0.490733\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[370]\tvalid_0's rmse: 0.492653\n","output_type":"stream"},{"name":"stderr","text":"[I 2025-09-13 16:43:57,374] Trial 44 finished with value: 73226.8436110443 and parameters: {'learning_rate': 0.07754403569970861, 'num_leaves': 82, 'max_depth': 10, 'min_data_in_leaf': 158, 'feature_fraction': 0.527658961853677, 'bagging_fraction': 0.5195363514647867}. Best is trial 23 with value: 73220.07660656604.\n","output_type":"stream"},{"name":"stdout","text":"Training until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[396]\tvalid_0's rmse: 0.485227\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[355]\tvalid_0's rmse: 0.485008\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[340]\tvalid_0's rmse: 0.489056\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[348]\tvalid_0's rmse: 0.490993\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[449]\tvalid_0's rmse: 0.492728\n","output_type":"stream"},{"name":"stderr","text":"[I 2025-09-13 16:44:25,897] Trial 45 finished with value: 73212.29145898709 and parameters: {'learning_rate': 0.07127099115234865, 'num_leaves': 90, 'max_depth': 9, 'min_data_in_leaf': 117, 'feature_fraction': 0.6285407177416754, 'bagging_fraction': 0.5666057071254738}. Best is trial 45 with value: 73212.29145898709.\n","output_type":"stream"},{"name":"stdout","text":"Training until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[390]\tvalid_0's rmse: 0.48538\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[312]\tvalid_0's rmse: 0.485255\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[330]\tvalid_0's rmse: 0.48934\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[304]\tvalid_0's rmse: 0.490876\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[257]\tvalid_0's rmse: 0.49318\n","output_type":"stream"},{"name":"stderr","text":"[I 2025-09-13 16:44:50,113] Trial 46 finished with value: 73222.91531297647 and parameters: {'learning_rate': 0.0894604238850957, 'num_leaves': 77, 'max_depth': 9, 'min_data_in_leaf': 127, 'feature_fraction': 0.6238819482008813, 'bagging_fraction': 0.6320484009020572}. Best is trial 45 with value: 73212.29145898709.\n","output_type":"stream"},{"name":"stdout","text":"Training until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[552]\tvalid_0's rmse: 0.485576\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[511]\tvalid_0's rmse: 0.485274\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[416]\tvalid_0's rmse: 0.489209\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[311]\tvalid_0's rmse: 0.491385\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[347]\tvalid_0's rmse: 0.493087\n","output_type":"stream"},{"name":"stderr","text":"[I 2025-09-13 16:45:17,872] Trial 47 finished with value: 73227.27669425457 and parameters: {'learning_rate': 0.08132865352212962, 'num_leaves': 71, 'max_depth': 8, 'min_data_in_leaf': 117, 'feature_fraction': 0.6896942712466644, 'bagging_fraction': 0.822563862345576}. Best is trial 45 with value: 73212.29145898709.\n","output_type":"stream"},{"name":"stdout","text":"Training until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[792]\tvalid_0's rmse: 0.485176\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[732]\tvalid_0's rmse: 0.484966\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[781]\tvalid_0's rmse: 0.48889\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[574]\tvalid_0's rmse: 0.491097\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[789]\tvalid_0's rmse: 0.492806\n","output_type":"stream"},{"name":"stderr","text":"[I 2025-09-13 16:46:06,781] Trial 48 finished with value: 73232.40239315027 and parameters: {'learning_rate': 0.04176464683736722, 'num_leaves': 89, 'max_depth': 9, 'min_data_in_leaf': 183, 'feature_fraction': 0.703806367147398, 'bagging_fraction': 0.7119050067554809}. Best is trial 45 with value: 73212.29145898709.\n","output_type":"stream"},{"name":"stdout","text":"Training until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[767]\tvalid_0's rmse: 0.485039\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[734]\tvalid_0's rmse: 0.484981\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[874]\tvalid_0's rmse: 0.489124\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[536]\tvalid_0's rmse: 0.491172\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[760]\tvalid_0's rmse: 0.49265\n","output_type":"stream"},{"name":"stderr","text":"[I 2025-09-13 16:47:00,557] Trial 49 finished with value: 73225.85228686182 and parameters: {'learning_rate': 0.03534956798432024, 'num_leaves': 100, 'max_depth': 9, 'min_data_in_leaf': 114, 'feature_fraction': 0.7377978801752029, 'bagging_fraction': 0.6105903034532857}. Best is trial 45 with value: 73212.29145898709.\n","output_type":"stream"},{"name":"stdout","text":"✅ Best LightGBM params: {'learning_rate': 0.07127099115234865, 'num_leaves': 90, 'max_depth': 9, 'min_data_in_leaf': 117, 'feature_fraction': 0.6285407177416754, 'bagging_fraction': 0.5666057071254738}\n","output_type":"stream"}],"execution_count":50},{"cell_type":"code","source":"def objective_cb(trial):\n    params = {\n        'iterations': 1000,\n        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.1),\n        'depth': trial.suggest_int('depth', 4, 10),\n        'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 1, 10),\n        'random_seed': 42,\n        'verbose': False\n    }\n    \n    cv_scores = []\n    for fold, (train_idx, val_idx) in enumerate(skf.split(X, train_modeling['price_bin'])):\n        X_train_fold = X.iloc[train_idx]\n        y_train_fold = y.iloc[train_idx]\n        X_val_fold = X.iloc[val_idx]\n        y_val_fold = y.iloc[val_idx]\n        \n        model = CatBoostRegressor(**params)\n        model.fit(X_train_fold, y_train_fold, eval_set=(X_val_fold, y_val_fold), early_stopping_rounds=50)\n        \n        val_pred_log = model.predict(X_val_fold)\n        val_pred_price = np.expm1(val_pred_log)\n        y_val_price = np.expm1(y_val_fold)\n        rmse = np.sqrt(mean_squared_error(y_val_price, val_pred_price))\n        cv_scores.append(rmse)\n    \n    return np.mean(cv_scores)\n\nstudy_cb = optuna.create_study(direction='minimize')\nstudy_cb.optimize(objective_cb, n_trials=50)\n\nprint(f\"✅ Best CatBoost params: {study_cb.best_params}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-13T16:48:07.643321Z","iopub.execute_input":"2025-09-13T16:48:07.643651Z","iopub.status.idle":"2025-09-13T18:11:04.840936Z","shell.execute_reply.started":"2025-09-13T16:48:07.643623Z","shell.execute_reply":"2025-09-13T18:11:04.840040Z"}},"outputs":[{"name":"stderr","text":"[I 2025-09-13 16:48:07,648] A new study created in memory with name: no-name-4562044d-2b8e-4ffd-b14a-101be0341c4e\n[I 2025-09-13 16:49:43,454] Trial 0 finished with value: 73223.98690837805 and parameters: {'learning_rate': 0.0566960725807962, 'depth': 8, 'l2_leaf_reg': 1.176455524156788}. Best is trial 0 with value: 73223.98690837805.\n[I 2025-09-13 16:50:42,454] Trial 1 finished with value: 73477.75544432146 and parameters: {'learning_rate': 0.025595917290683973, 'depth': 4, 'l2_leaf_reg': 9.535474930608792}. Best is trial 0 with value: 73223.98690837805.\n[I 2025-09-13 16:51:57,816] Trial 2 finished with value: 73309.95881717488 and parameters: {'learning_rate': 0.04046652570508154, 'depth': 6, 'l2_leaf_reg': 3.1883433447273957}. Best is trial 0 with value: 73223.98690837805.\n[I 2025-09-13 16:53:03,752] Trial 3 finished with value: 73266.84676456783 and parameters: {'learning_rate': 0.08929933359759118, 'depth': 5, 'l2_leaf_reg': 3.523881437266364}. Best is trial 0 with value: 73223.98690837805.\n[I 2025-09-13 16:54:30,558] Trial 4 finished with value: 73243.84134215173 and parameters: {'learning_rate': 0.07581344060106512, 'depth': 8, 'l2_leaf_reg': 5.918505941260382}. Best is trial 0 with value: 73223.98690837805.\n[I 2025-09-13 16:55:36,589] Trial 5 finished with value: 73286.33431104888 and parameters: {'learning_rate': 0.07418165870299871, 'depth': 5, 'l2_leaf_reg': 5.996365638254075}. Best is trial 0 with value: 73223.98690837805.\n[I 2025-09-13 16:57:16,659] Trial 6 finished with value: 73242.02332554547 and parameters: {'learning_rate': 0.05761475600587502, 'depth': 8, 'l2_leaf_reg': 6.306776048720164}. Best is trial 0 with value: 73223.98690837805.\n[I 2025-09-13 16:58:22,468] Trial 7 finished with value: 73300.39951941663 and parameters: {'learning_rate': 0.06814688978272498, 'depth': 5, 'l2_leaf_reg': 9.12985075937879}. Best is trial 0 with value: 73223.98690837805.\n[I 2025-09-13 17:02:46,220] Trial 8 finished with value: 73241.19069394346 and parameters: {'learning_rate': 0.03056379213362182, 'depth': 10, 'l2_leaf_reg': 4.716233232794595}. Best is trial 0 with value: 73223.98690837805.\n[I 2025-09-13 17:04:01,210] Trial 9 finished with value: 73289.31664958457 and parameters: {'learning_rate': 0.04860596332475041, 'depth': 6, 'l2_leaf_reg': 3.4847298187431663}. Best is trial 0 with value: 73223.98690837805.\n[I 2025-09-13 17:08:25,212] Trial 10 finished with value: 73327.96742439266 and parameters: {'learning_rate': 0.014034237684847871, 'depth': 10, 'l2_leaf_reg': 1.853479409774263}. Best is trial 0 with value: 73223.98690837805.\n[I 2025-09-13 17:12:46,809] Trial 11 finished with value: 73226.72806398555 and parameters: {'learning_rate': 0.03321509974612519, 'depth': 10, 'l2_leaf_reg': 1.1362626345776663}. Best is trial 0 with value: 73223.98690837805.\n[I 2025-09-13 17:15:00,366] Trial 12 finished with value: 73241.53105034158 and parameters: {'learning_rate': 0.040621985412487224, 'depth': 9, 'l2_leaf_reg': 1.066134976431114}. Best is trial 0 with value: 73223.98690837805.\n[I 2025-09-13 17:16:36,153] Trial 13 finished with value: 73241.55648612534 and parameters: {'learning_rate': 0.058087890667026146, 'depth': 8, 'l2_leaf_reg': 2.0113818225489966}. Best is trial 0 with value: 73223.98690837805.\n[I 2025-09-13 17:17:40,975] Trial 14 finished with value: 73214.93647539394 and parameters: {'learning_rate': 0.09952215053641802, 'depth': 9, 'l2_leaf_reg': 1.105855975393112}. Best is trial 14 with value: 73214.93647539394.\n[I 2025-09-13 17:18:51,447] Trial 15 finished with value: 73232.89030126054 and parameters: {'learning_rate': 0.09082146726363283, 'depth': 7, 'l2_leaf_reg': 7.433238865670909}. Best is trial 14 with value: 73214.93647539394.\n[I 2025-09-13 17:20:00,771] Trial 16 finished with value: 73217.21548542292 and parameters: {'learning_rate': 0.09863815918241457, 'depth': 9, 'l2_leaf_reg': 2.6898734097807875}. Best is trial 14 with value: 73214.93647539394.\n[I 2025-09-13 17:21:16,997] Trial 17 finished with value: 73203.11392617604 and parameters: {'learning_rate': 0.09879665865373456, 'depth': 9, 'l2_leaf_reg': 2.6384854494843424}. Best is trial 17 with value: 73203.11392617604.\n[I 2025-09-13 17:22:35,070] Trial 18 finished with value: 73222.03093418747 and parameters: {'learning_rate': 0.09752552413307303, 'depth': 9, 'l2_leaf_reg': 4.447465527486957}. Best is trial 17 with value: 73203.11392617604.\n[I 2025-09-13 17:23:48,474] Trial 19 finished with value: 73227.71531475062 and parameters: {'learning_rate': 0.08339880787659916, 'depth': 7, 'l2_leaf_reg': 2.3379565938188116}. Best is trial 17 with value: 73203.11392617604.\n[I 2025-09-13 17:25:23,343] Trial 20 finished with value: 73223.88894584944 and parameters: {'learning_rate': 0.08142854373072807, 'depth': 9, 'l2_leaf_reg': 4.7864950906165245}. Best is trial 17 with value: 73203.11392617604.\n[I 2025-09-13 17:26:35,116] Trial 21 finished with value: 73220.84130563306 and parameters: {'learning_rate': 0.09965016545176035, 'depth': 9, 'l2_leaf_reg': 2.666223034632649}. Best is trial 17 with value: 73203.11392617604.\n[I 2025-09-13 17:27:43,724] Trial 22 finished with value: 73234.17385755558 and parameters: {'learning_rate': 0.09143739114983895, 'depth': 9, 'l2_leaf_reg': 3.8059639648978756}. Best is trial 17 with value: 73203.11392617604.\n[I 2025-09-13 17:29:32,180] Trial 23 finished with value: 73220.85501818827 and parameters: {'learning_rate': 0.09907782268759528, 'depth': 10, 'l2_leaf_reg': 2.6253101568136903}. Best is trial 17 with value: 73203.11392617604.\n[I 2025-09-13 17:30:57,401] Trial 24 finished with value: 73210.55408780924 and parameters: {'learning_rate': 0.06747702217330591, 'depth': 8, 'l2_leaf_reg': 1.8950856314304665}. Best is trial 17 with value: 73203.11392617604.\n[I 2025-09-13 17:32:27,171] Trial 25 finished with value: 73225.70476317173 and parameters: {'learning_rate': 0.06642420729252208, 'depth': 8, 'l2_leaf_reg': 1.838698201419412}. Best is trial 17 with value: 73203.11392617604.\n[I 2025-09-13 17:33:45,089] Trial 26 finished with value: 73222.72549899385 and parameters: {'learning_rate': 0.08370606361489695, 'depth': 7, 'l2_leaf_reg': 1.6414137934176574}. Best is trial 17 with value: 73203.11392617604.\n[I 2025-09-13 17:35:16,490] Trial 27 finished with value: 73232.55019874315 and parameters: {'learning_rate': 0.06995004058231634, 'depth': 8, 'l2_leaf_reg': 4.278667093419756}. Best is trial 17 with value: 73203.11392617604.\n[I 2025-09-13 17:36:32,396] Trial 28 finished with value: 73247.58229312433 and parameters: {'learning_rate': 0.08984715427003714, 'depth': 9, 'l2_leaf_reg': 7.92172987474598}. Best is trial 17 with value: 73203.11392617604.\n[I 2025-09-13 17:37:53,530] Trial 29 finished with value: 73220.17905624014 and parameters: {'learning_rate': 0.07814934654767439, 'depth': 8, 'l2_leaf_reg': 1.4467099314050573}. Best is trial 17 with value: 73203.11392617604.\n[I 2025-09-13 17:40:47,009] Trial 30 finished with value: 73215.16560161122 and parameters: {'learning_rate': 0.06366324627707694, 'depth': 10, 'l2_leaf_reg': 3.099067170580683}. Best is trial 17 with value: 73203.11392617604.\n[I 2025-09-13 17:43:33,638] Trial 31 finished with value: 73226.13391805932 and parameters: {'learning_rate': 0.06032545467085444, 'depth': 10, 'l2_leaf_reg': 2.998892437081476}. Best is trial 17 with value: 73203.11392617604.\n[I 2025-09-13 17:46:46,625] Trial 32 finished with value: 73227.73959259647 and parameters: {'learning_rate': 0.049882924721557295, 'depth': 10, 'l2_leaf_reg': 2.277671378384342}. Best is trial 17 with value: 73203.11392617604.\n[I 2025-09-13 17:48:23,060] Trial 33 finished with value: 73211.9506083492 and parameters: {'learning_rate': 0.0630935631810933, 'depth': 9, 'l2_leaf_reg': 1.0644474212484312}. Best is trial 17 with value: 73203.11392617604.\n[I 2025-09-13 17:50:17,138] Trial 34 finished with value: 73234.7083399413 and parameters: {'learning_rate': 0.048814497614182994, 'depth': 9, 'l2_leaf_reg': 1.1435122021929929}. Best is trial 17 with value: 73203.11392617604.\n[I 2025-09-13 17:51:41,121] Trial 35 finished with value: 73213.56659917797 and parameters: {'learning_rate': 0.07270616668167038, 'depth': 8, 'l2_leaf_reg': 1.655889579608626}. Best is trial 17 with value: 73203.11392617604.\n[I 2025-09-13 17:53:00,187] Trial 36 finished with value: 73238.12796118198 and parameters: {'learning_rate': 0.07173913951387717, 'depth': 8, 'l2_leaf_reg': 2.1230240252350043}. Best is trial 17 with value: 73203.11392617604.\n[I 2025-09-13 17:54:15,001] Trial 37 finished with value: 73279.22832040346 and parameters: {'learning_rate': 0.05283031442409841, 'depth': 6, 'l2_leaf_reg': 3.9255489137109443}. Best is trial 17 with value: 73203.11392617604.\n[I 2025-09-13 17:55:38,794] Trial 38 finished with value: 73241.28127778646 and parameters: {'learning_rate': 0.06471936912360904, 'depth': 7, 'l2_leaf_reg': 1.5876689713421408}. Best is trial 17 with value: 73203.11392617604.\n[I 2025-09-13 17:56:37,611] Trial 39 finished with value: 73392.20967339705 and parameters: {'learning_rate': 0.04343292389116438, 'depth': 4, 'l2_leaf_reg': 5.403906010001687}. Best is trial 17 with value: 73203.11392617604.\n[I 2025-09-13 17:58:13,877] Trial 40 finished with value: 73219.20415491119 and parameters: {'learning_rate': 0.07611016644065977, 'depth': 8, 'l2_leaf_reg': 9.865167787785182}. Best is trial 17 with value: 73203.11392617604.\n[I 2025-09-13 17:59:23,067] Trial 41 finished with value: 73225.07048899308 and parameters: {'learning_rate': 0.09459240007458433, 'depth': 8, 'l2_leaf_reg': 1.5020620532345836}. Best is trial 17 with value: 73203.11392617604.\n[I 2025-09-13 18:00:32,750] Trial 42 finished with value: 73214.00533179111 and parameters: {'learning_rate': 0.08700410000140787, 'depth': 9, 'l2_leaf_reg': 1.015186487736053}. Best is trial 17 with value: 73203.11392617604.\n[I 2025-09-13 18:01:43,335] Trial 43 finished with value: 73222.3800119517 and parameters: {'learning_rate': 0.0870170323318043, 'depth': 8, 'l2_leaf_reg': 2.261059871709576}. Best is trial 17 with value: 73203.11392617604.\n[I 2025-09-13 18:03:11,785] Trial 44 finished with value: 73208.35892745519 and parameters: {'learning_rate': 0.07868079566496222, 'depth': 9, 'l2_leaf_reg': 3.4314608983902257}. Best is trial 17 with value: 73203.11392617604.\n[I 2025-09-13 18:04:35,326] Trial 45 finished with value: 73231.65630490957 and parameters: {'learning_rate': 0.07194229939079498, 'depth': 7, 'l2_leaf_reg': 3.3979895553922086}. Best is trial 17 with value: 73203.11392617604.\n[I 2025-09-13 18:05:50,388] Trial 46 finished with value: 73244.06315104291 and parameters: {'learning_rate': 0.0779667225679527, 'depth': 8, 'l2_leaf_reg': 3.0910167593001137}. Best is trial 17 with value: 73203.11392617604.\n[I 2025-09-13 18:07:33,559] Trial 47 finished with value: 73234.17587120536 and parameters: {'learning_rate': 0.06161838526448516, 'depth': 9, 'l2_leaf_reg': 2.7602118073329396}. Best is trial 17 with value: 73203.11392617604.\n[I 2025-09-13 18:09:00,071] Trial 48 finished with value: 73232.94468806633 and parameters: {'learning_rate': 0.06800130162787184, 'depth': 8, 'l2_leaf_reg': 1.9096594455695186}. Best is trial 17 with value: 73203.11392617604.\n[I 2025-09-13 18:11:04,835] Trial 49 finished with value: 73232.11888558062 and parameters: {'learning_rate': 0.05598185285168097, 'depth': 9, 'l2_leaf_reg': 3.842316436943795}. Best is trial 17 with value: 73203.11392617604.\n","output_type":"stream"},{"name":"stdout","text":"✅ Best CatBoost params: {'learning_rate': 0.09879665865373456, 'depth': 9, 'l2_leaf_reg': 2.6384854494843424}\n","output_type":"stream"}],"execution_count":51},{"cell_type":"markdown","source":"### Tuned LightGBM Model\nWith the best hyperparameters from Optuna, I retrained LightGBM using 5-fold cross-validation. This gave me both **out-of-fold (OOF)** predictions for training data and final predictions for the test set.\n\n**Why this matters:**\n* Cross-validation ensures the model generalizes well by testing on unseen folds.\n* OOF predictions are stored for stacking in case I combine multiple models later.\n* The final test predictions were transformed back from log space to actual dollar values for submission.\n\n**Fold-by-Fold Results (Validation RMSE):**\n* **Fold 1:** 0.4852\n* **Fold 2:** 0.4850\n* **Fold 3:** 0.4890\n* **Fold 4:** 0.4910\n* **Fold 5:** 0.4927\n\n**Takeaways:**\n* The model was consistent across folds, with RMSE values in a tight range (\\~0.485–0.493).\n* Early stopping (around 350–450 rounds) prevented overfitting while still allowing the model to learn deeply.\n* The tuned model produced a mean predicted price of $36,289, which became the submission output.","metadata":{}},{"cell_type":"code","source":"# Best params from Optuna\nbest_lgb_params = {\n    'objective': 'regression',\n    'metric': 'rmse',\n    'boosting_type': 'gbdt',\n    'learning_rate': 0.07127099115234865,\n    'num_leaves': 90,\n    'max_depth': 9,\n    'min_data_in_leaf': 117,\n    'feature_fraction': 0.6285407177416754,\n    'bagging_fraction': 0.5666057071254738,\n    'seed': 42,\n    'verbose': -1\n}\n\n# Prepare data\ntrain_modeling = train_df.drop(columns=['price'], errors='ignore')\ntest_modeling = test_df\n\nfeature_cols = [col for col in train_modeling.columns if col not in ['log_price']]\nX = train_modeling[feature_cols]\ny = train_modeling['log_price']\nX_test = test_modeling[feature_cols]\n\n# Create price bins for stratification\ntrain_modeling['price_bin'] = pd.qcut(train_modeling['log_price'], q=10, labels=False)\n\n# CV setup\nskf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\noof_preds_lgb = np.zeros(len(X))\ntest_preds_lgb = np.zeros(len(X_test))\n\nfor fold, (train_idx, val_idx) in enumerate(skf.split(X, train_modeling['price_bin'])):\n    print(f\"=== FOLD {fold+1} ===\")\n    \n    X_train_fold = X.iloc[train_idx]\n    y_train_fold = y.iloc[train_idx]\n    X_val_fold = X.iloc[val_idx]\n    y_val_fold = y.iloc[val_idx]\n    \n    train_ds = lgb.Dataset(X_train_fold, label=y_train_fold)\n    val_ds = lgb.Dataset(X_val_fold, label=y_val_fold, reference=train_ds)\n    \n    model = lgb.train(\n        best_lgb_params,\n        train_ds,\n        valid_sets=[val_ds],\n        num_boost_round=10000,\n        callbacks=[lgb.early_stopping(100), lgb.log_evaluation(200)]\n    )\n    \n    # OOF predictions\n    oof_preds_lgb[val_idx] = model.predict(X_val_fold)\n    \n    # Test predictions (accumulate)\n    test_preds_lgb += model.predict(X_test) / 5\n\n# Save OOF predictions (for stacking)\nnp.save('/kaggle/working/lgb_oof_tuned.npy', np.expm1(oof_preds_lgb))\n\n# Convert test predictions to raw price\ntest_preds_lgb_raw = np.expm1(test_preds_lgb)\n\n# Submission\n# Drop old price column if it exists\nsub = sub.drop(columns=['price'], errors='ignore')\n\n# Insert predictions\nsub['price'] = test_preds_lgb_raw","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-13T19:33:12.577585Z","iopub.execute_input":"2025-09-13T19:33:12.577881Z","iopub.status.idle":"2025-09-13T19:33:58.027371Z","shell.execute_reply.started":"2025-09-13T19:33:12.577861Z","shell.execute_reply":"2025-09-13T19:33:58.026035Z"}},"outputs":[{"name":"stdout","text":"=== FOLD 1 ===\nTraining until validation scores don't improve for 100 rounds\n[200]\tvalid_0's rmse: 0.486043\n[400]\tvalid_0's rmse: 0.485245\nEarly stopping, best iteration is:\n[396]\tvalid_0's rmse: 0.485227\n=== FOLD 2 ===\nTraining until validation scores don't improve for 100 rounds\n[200]\tvalid_0's rmse: 0.485638\n[400]\tvalid_0's rmse: 0.485096\nEarly stopping, best iteration is:\n[355]\tvalid_0's rmse: 0.485008\n=== FOLD 3 ===\nTraining until validation scores don't improve for 100 rounds\n[200]\tvalid_0's rmse: 0.48965\n[400]\tvalid_0's rmse: 0.489043\nEarly stopping, best iteration is:\n[417]\tvalid_0's rmse: 0.488981\n=== FOLD 4 ===\nTraining until validation scores don't improve for 100 rounds\n[200]\tvalid_0's rmse: 0.491505\n[400]\tvalid_0's rmse: 0.491065\nEarly stopping, best iteration is:\n[348]\tvalid_0's rmse: 0.490993\n=== FOLD 5 ===\nTraining until validation scores don't improve for 100 rounds\n[200]\tvalid_0's rmse: 0.493538\n[400]\tvalid_0's rmse: 0.49275\nEarly stopping, best iteration is:\n[449]\tvalid_0's rmse: 0.492728\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_101/1329932815.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0msub\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'price'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_preds_lgb_raw\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m \u001b[0msub_lgb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/kaggle/working/submission_lgb_tuned.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"✅ LightGBM tuned submission saved. Mean price: ${test_preds_lgb_raw.mean():,.0f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'sub_lgb' is not defined"],"ename":"NameError","evalue":"name 'sub_lgb' is not defined","output_type":"error"}],"execution_count":53},{"cell_type":"code","source":"sub.to_csv('/kaggle/working/submission_lgb_tuned.csv', index=False)\nprint(f\"✅ LightGBM tuned submission saved. Mean price: ${test_preds_lgb_raw.mean():,.0f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-13T19:40:18.143625Z","iopub.execute_input":"2025-09-13T19:40:18.143967Z","iopub.status.idle":"2025-09-13T19:40:18.474751Z","shell.execute_reply.started":"2025-09-13T19:40:18.143944Z","shell.execute_reply":"2025-09-13T19:40:18.472196Z"}},"outputs":[{"name":"stdout","text":"✅ LightGBM tuned submission saved. Mean price: $36,289\n","output_type":"stream"}],"execution_count":55},{"cell_type":"markdown","source":"### Tuned CatBoost Model\nUsing the best parameters from Optuna, I trained CatBoost with 5-fold cross-validation. Like with LightGBM, this setup provided both **OOF predictions** and **final test predictions**, later converted back to raw prices for submission.\n\n**Why CatBoost stood out here:**\n* Handles categorical features natively (though I worked with transformed features, this is still a strength).\n* Robust regularization (`l2_leaf_reg`) helps balance bias and variance.\n* The model automatically detects overfitting and shrinks to the best iteration.\n\n**Fold-by-Fold Results (Validation RMSE):**\n* **Fold 1:** 0.4870 (stopped at \\~479 rounds)\n* **Fold 2:** 0.4860 (stopped at \\~577 rounds)\n* **Fold 3:** 0.4901 (stopped at \\~526 rounds)\n* **Fold 4:** 0.4928 (stopped at \\~470 rounds)\n* **Fold 5:** 0.4939 (stopped at \\~631 rounds)\n\n**Takeaways:**\n* CatBoost reached early stopping between **470–630 rounds**, consistently below the max 10,000 set.\n* Performance across folds was very stable, with RMSE values in the **0.486–0.494** range.\n* The tuned model produced a mean predicted price of **\\$36,388**, slightly higher than the tuned LightGBM output.","metadata":{}},{"cell_type":"code","source":"# Best params from Optuna\nbest_cb_params = {\n    'iterations': 10000,\n    'learning_rate': 0.09879665865373456,\n    'depth': 9,\n    'l2_leaf_reg': 2.6384854494843424,\n    'loss_function': 'RMSE',\n    'eval_metric': 'RMSE',\n    'random_seed': 42,\n    'early_stopping_rounds': 100,\n    'verbose': 200\n}\n\n# CV setup (same as above)\noof_preds_cb = np.zeros(len(X))\ntest_preds_cb = np.zeros(len(X_test))\n\nfor fold, (train_idx, val_idx) in enumerate(skf.split(X, train_modeling['price_bin'])):\n    print(f\"=== FOLD {fold+1} ===\")\n    \n    X_train_fold = X.iloc[train_idx]\n    y_train_fold = y.iloc[train_idx]\n    X_val_fold = X.iloc[val_idx]\n    y_val_fold = y.iloc[val_idx]\n    \n    model = CatBoostRegressor(**best_cb_params)\n    model.fit(\n        X_train_fold, y_train_fold,\n        eval_set=(X_val_fold, y_val_fold),\n        use_best_model=True\n    )\n    \n    # OOF predictions\n    oof_preds_cb[val_idx] = model.predict(X_val_fold)\n    \n    # Test predictions (accumulate)\n    test_preds_cb += model.predict(X_test) / 5\n\n# Save OOF predictions (for stacking)\nnp.save('/kaggle/working/cb_oof_tuned.npy', np.expm1(oof_preds_cb))\n\n# Convert test predictions to raw price\ntest_preds_cb_raw = np.expm1(test_preds_cb)\n\n# Submission\n# Drop old price column if it exists\nsub_cb = sub.drop(columns=['price'], errors='ignore').copy()\n\n# Insert predictions\nsub_cb['price'] = test_preds_cb_raw\n\nsub_cb.to_csv('/kaggle/working/submission_cb_tuned.csv', index=False)\nprint(f\"✅ CatBoost tuned submission saved. Mean price: ${test_preds_cb_raw.mean():,.0f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-13T19:40:50.153377Z","iopub.execute_input":"2025-09-13T19:40:50.153712Z","iopub.status.idle":"2025-09-13T19:42:22.663322Z","shell.execute_reply.started":"2025-09-13T19:40:50.153681Z","shell.execute_reply":"2025-09-13T19:42:22.662197Z"}},"outputs":[{"name":"stdout","text":"=== FOLD 1 ===\n0:\tlearn: 0.7945089\ttest: 0.7937058\tbest: 0.7937058 (0)\ttotal: 38ms\tremaining: 6m 20s\n200:\tlearn: 0.4758731\ttest: 0.4881964\tbest: 0.4881964 (200)\ttotal: 5.58s\tremaining: 4m 31s\n400:\tlearn: 0.4616462\ttest: 0.4870471\tbest: 0.4870471 (400)\ttotal: 11.2s\tremaining: 4m 27s\nStopped by overfitting detector  (100 iterations wait)\n\nbestTest = 0.4870077851\nbestIteration = 478\n\nShrink model to first 479 iterations.\n=== FOLD 2 ===\n0:\tlearn: 0.7944696\ttest: 0.7931206\tbest: 0.7931206 (0)\ttotal: 32.7ms\tremaining: 5m 26s\n200:\tlearn: 0.4764481\ttest: 0.4873379\tbest: 0.4873356 (199)\ttotal: 5.58s\tremaining: 4m 32s\n400:\tlearn: 0.4629198\ttest: 0.4863006\tbest: 0.4862901 (383)\ttotal: 11.1s\tremaining: 4m 25s\n600:\tlearn: 0.4516253\ttest: 0.4860818\tbest: 0.4859917 (576)\ttotal: 16.6s\tremaining: 4m 19s\nStopped by overfitting detector  (100 iterations wait)\n\nbestTest = 0.4859916789\nbestIteration = 576\n\nShrink model to first 577 iterations.\n=== FOLD 3 ===\n0:\tlearn: 0.7944849\ttest: 0.7939018\tbest: 0.7939018 (0)\ttotal: 33.4ms\tremaining: 5m 33s\n200:\tlearn: 0.4753899\ttest: 0.4915538\tbest: 0.4915538 (200)\ttotal: 5.61s\tremaining: 4m 33s\n400:\tlearn: 0.4612853\ttest: 0.4903295\tbest: 0.4903279 (399)\ttotal: 11.1s\tremaining: 4m 26s\n600:\tlearn: 0.4500651\ttest: 0.4901865\tbest: 0.4900949 (525)\ttotal: 16.7s\tremaining: 4m 21s\nStopped by overfitting detector  (100 iterations wait)\n\nbestTest = 0.4900948502\nbestIteration = 525\n\nShrink model to first 526 iterations.\n=== FOLD 4 ===\n0:\tlearn: 0.7946100\ttest: 0.7949179\tbest: 0.7949179 (0)\ttotal: 30.2ms\tremaining: 5m 2s\n200:\tlearn: 0.4746689\ttest: 0.4938777\tbest: 0.4938777 (200)\ttotal: 5.65s\tremaining: 4m 35s\n400:\tlearn: 0.4603454\ttest: 0.4929808\tbest: 0.4929521 (373)\ttotal: 11.1s\tremaining: 4m 26s\nStopped by overfitting detector  (100 iterations wait)\n\nbestTest = 0.4928157636\nbestIteration = 469\n\nShrink model to first 470 iterations.\n=== FOLD 5 ===\n0:\tlearn: 0.7935257\ttest: 0.7960574\tbest: 0.7960574 (0)\ttotal: 30ms\tremaining: 5m\n200:\tlearn: 0.4743172\ttest: 0.4955325\tbest: 0.4955325 (200)\ttotal: 5.57s\tremaining: 4m 31s\n400:\tlearn: 0.4604558\ttest: 0.4944641\tbest: 0.4944600 (399)\ttotal: 11.1s\tremaining: 4m 25s\n600:\tlearn: 0.4493884\ttest: 0.4939957\tbest: 0.4939779 (594)\ttotal: 16.6s\tremaining: 4m 20s\nStopped by overfitting detector  (100 iterations wait)\n\nbestTest = 0.4939269011\nbestIteration = 630\n\nShrink model to first 631 iterations.\n✅ CatBoost tuned submission saved. Mean price: $36,388\n","output_type":"stream"}],"execution_count":56},{"cell_type":"markdown","source":"### Stacked Model with Ridge Regression\nAfter training tuned LightGBM and CatBoost models, I combined their strengths using a **meta-model**. Ridge regression was chosen as the stacking model because it’s:\n* **Simple and robust** — it avoids overfitting while handling correlated predictors.\n* **Interpretable** — the learned coefficients directly show how much weight is assigned to each base model.\n\n**Meta-Model Results (on OOF predictions):**\n* **LightGBM weight:** 0.541\n* **CatBoost weight:** 0.712\n* **Intercept:** –2,207\n\nThis indicates that the meta-model leaned more heavily on **CatBoost predictions**, but still gave substantial weight to LightGBM. The negative intercept acts as a small correction factor.\n\n**Final Submission:**\n* Mean predicted price: **\\$43,349**\n* Stacked predictions were generated by applying the Ridge meta-model to the test set outputs from both tuned LightGBM and CatBoost.\n\n**Takeaway:**\nStacking improved robustness by balancing the two models — LightGBM’s generalization with CatBoost’s stability — while avoiding over-reliance on just one learner. The result is a more balanced prediction that better captures signal from both models.","metadata":{}},{"cell_type":"code","source":"# Load OOF predictions (raw price)\nlgb_oof = np.load('/kaggle/working/lgb_oof_tuned.npy')  # shape: (188533,)\ncb_oof = np.load('/kaggle/working/cb_oof_tuned.npy')    # shape: (188533,)\n\n# True prices (raw)\ny_true = np.expm1(train['log_price'].values)  # assuming you have train with log_price\n\n# Stack OOF predictions\nX_meta = np.column_stack([lgb_oof, cb_oof])  # shape: (188533, 2)\n\n# Train meta-model (Ridge — robust, no overfit)\nmeta_model = Ridge(alpha=1.0)\nmeta_model.fit(X_meta, y_true)\n\n# Get weights\nweights = meta_model.coef_\nintercept = meta_model.intercept_\nprint(f\"✅ Meta-model weights: LightGBM={weights[0]:.3f}, CatBoost={weights[1]:.3f}, Intercept={intercept:.3f}\")\n\n# Load test predictions\nlgb_test = pd.read_csv('/kaggle/working/submission_lgb_tuned.csv')['price'].values\ncb_test = pd.read_csv('/kaggle/working/submission_cb_tuned.csv')['price'].values\n\n# Stack test predictions\nX_meta_test = np.column_stack([lgb_test, cb_test])\nfinal_pred = meta_model.predict(X_meta_test)\n\n# Submission\n# Drop old price column if it exists\nsub_stack = sub.drop(columns=['price'], errors='ignore').copy()\n\n# Insert predictions\nsub_stack['price'] = final_pred\n\nsub_stack.to_csv('/kaggle/working/submission_stacked_tuned.csv', index=False)\nprint(f\"✅ Stacked submission saved. Mean price: ${final_pred.mean():,.0f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-13T19:51:05.916946Z","iopub.execute_input":"2025-09-13T19:51:05.917292Z","iopub.status.idle":"2025-09-13T19:51:06.314780Z","shell.execute_reply.started":"2025-09-13T19:51:05.917270Z","shell.execute_reply":"2025-09-13T19:51:06.313901Z"}},"outputs":[{"name":"stdout","text":"✅ Meta-model weights: LightGBM=0.541, CatBoost=0.712, Intercept=-2207.117\n✅ Stacked submission saved. Mean price: $43,349\n","output_type":"stream"}],"execution_count":57},{"cell_type":"code","source":"'''# Load tuned predictions\nlgb_test = pd.read_csv('/kaggle/working/submission_lgb_tuned.csv')['price'].values\ncb_test = pd.read_csv('/kaggle/working/submission_cb_tuned.csv')['price'].values\n\n# Weighted blend (LightGBM 0.7, CatBoost 0.3 — based on LB scores)\nweighted_blend = 0.7 * lgb_test + 0.3 * cb_test\n\nsub_weighted = pd.DataFrame({'id': sub['id'], 'price': weighted_blend})\nsub_weighted.to_csv('/kaggle/working/submission_weighted_blend.csv', index=False)'''","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-13T20:26:43.073655Z","iopub.execute_input":"2025-09-13T20:26:43.073969Z","iopub.status.idle":"2025-09-13T20:26:43.441757Z","shell.execute_reply.started":"2025-09-13T20:26:43.073947Z","shell.execute_reply":"2025-09-13T20:26:43.440415Z"}},"outputs":[],"execution_count":58},{"cell_type":"markdown","source":"### Extending the Stack: Adding Ridge to the Ensemble\n\nAfter stacking tuned **LightGBM** and **CatBoost**, I extended the ensemble by also including the **Ridge baseline model** as a third input to the meta-learner. The idea was to see whether Ridge, though weaker on its own, could still add complementary signal and help the meta-model refine predictions.\n\n**Meta-Model Setup (3 features):**\n* Base inputs: LightGBM (tuned), CatBoost (tuned), Ridge (baseline).\n* Meta-model: Ridge regression (α = 1.0).\n* Training data: OOF predictions stacked into a 3-column matrix.\n\n**Learned Weights:**\n* LightGBM: **0.590**\n* CatBoost: **0.759**\n* Ridge: **–0.104**\n* Intercept: **–1921.533**\n\n**Interpretation:**\n* The model leaned even more on **CatBoost and LightGBM**, confirming their predictive strength.\n* The **negative weight on Ridge** suggests that it did not contribute useful signal; instead, the meta-model treated it almost like a correction term.\n* Ridge’s inclusion didn’t meaningfully shift the ensemble balance, but it didn’t hurt either.\n\n**Final Ensemble Submission:**\n* Mean predicted price: **\\$43,401**\n* Submission combined the three sets of predictions with the learned weights applied.\n\n**Takeaway:**\nBringing in Ridge confirmed that simpler baselines don’t always help when stronger models dominate. Still, testing it validated that the ensemble was already near optimal with LightGBM + CatBoost.","metadata":{}},{"cell_type":"code","source":"# Load all OOF predictions\nlgb_oof = np.load('/kaggle/working/lgb_oof_tuned.npy')\ncb_oof = np.load('/kaggle/working/cb_oof_tuned.npy')\nridge_oof = np.load('/kaggle/working/ridge_oof.npy')  # ← NEW\n\n# Stack OOF (3 features)\nX_meta = np.column_stack([lgb_oof, cb_oof, ridge_oof])  # ← 3 columns\ny_meta = np.expm1(train['log_price'].values)\n\n# Retrain meta-model\nmeta_model_3 = Ridge(alpha=1.0)\nmeta_model_3.fit(X_meta, y_meta)\n\n# Get weights\nweights = meta_model_3.coef_\nintercept = meta_model_3.intercept_\nprint(f\"✅ Meta-model weights: LightGBM={weights[0]:.3f}, CatBoost={weights[1]:.3f}, Ridge={weights[2]:.3f}, Intercept={intercept:.3f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-13T20:48:58.661257Z","iopub.execute_input":"2025-09-13T20:48:58.661575Z","iopub.status.idle":"2025-09-13T20:48:58.689271Z","shell.execute_reply.started":"2025-09-13T20:48:58.661552Z","shell.execute_reply":"2025-09-13T20:48:58.688122Z"}},"outputs":[{"name":"stdout","text":"✅ Meta-model weights: LightGBM=0.590, CatBoost=0.759, Ridge=-0.104, Intercept=-1921.533\n","output_type":"stream"}],"execution_count":61},{"cell_type":"code","source":"# Load test predictions\nlgb_test = pd.read_csv('/kaggle/working/submission_lgb_tuned.csv')['price'].values\ncb_test = pd.read_csv('/kaggle/working/submission_cb_tuned.csv')['price'].values\nridge_test = pd.read_csv('/kaggle/input/dsn-qual-submissions/submission_ridge_baseline.csv')['price'].values\n\n# Stack test (3 features)\nX_meta_test = np.column_stack([lgb_test, cb_test, ridge_test])\nfinal_pred = meta_model_3.predict(X_meta_test) # retrain meta-model with 3 features\n\n# Submission\nsub_ensemble = pd.DataFrame({'id': sub['id'], 'price': final_pred})\nsub_ensemble.to_csv('/kaggle/working/submission_ensemble.csv', index=False)\nprint(f\"✅ Ensemble submission saved. Mean price: ${final_pred.mean():,.0f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-13T20:49:04.013197Z","iopub.execute_input":"2025-09-13T20:49:04.013525Z","iopub.status.idle":"2025-09-13T20:49:04.422702Z","shell.execute_reply.started":"2025-09-13T20:49:04.013497Z","shell.execute_reply":"2025-09-13T20:49:04.421857Z"}},"outputs":[{"name":"stdout","text":"✅ Ensemble submission saved. Mean price: $43,401\n","output_type":"stream"}],"execution_count":62},{"cell_type":"code","source":"'''sub_ensemble.shape'''","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-13T21:24:10.606843Z","iopub.execute_input":"2025-09-13T21:24:10.607202Z","iopub.status.idle":"2025-09-13T21:24:10.613740Z","shell.execute_reply.started":"2025-09-13T21:24:10.607176Z","shell.execute_reply":"2025-09-13T21:24:10.612873Z"}},"outputs":[{"execution_count":64,"output_type":"execute_result","data":{"text/plain":"(125690, 2)"},"metadata":{}}],"execution_count":64},{"cell_type":"code","source":"'''# Weighted blend: 0.6 LGBM + 0.3 CatBoost + 0.1 Ridge\nweighted_blend = 0.6 * lgb_test + 0.3 * cb_test + 0.1 * ridge_test\n\nsub_en_weighted = pd.DataFrame({'id': sub['id'], 'price': weighted_blend})\nsub_en_weighted.to_csv('/kaggle/working/submission_weighted_ensemble.csv', index=False)'''","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-13T21:00:34.468005Z","iopub.execute_input":"2025-09-13T21:00:34.468372Z","iopub.status.idle":"2025-09-13T21:00:34.754905Z","shell.execute_reply.started":"2025-09-13T21:00:34.468350Z","shell.execute_reply":"2025-09-13T21:00:34.753910Z"}},"outputs":[],"execution_count":63}]}